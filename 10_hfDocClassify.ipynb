{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# default_exp docClassify\n",
        "from nbdev import *"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " # HuggingFace Document Classification Utils \n",
        "\n",
        "> Classes to build and run document classification pipelines using baseline HuggingFace functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " \n",
        "\n",
        "This library contains a single utility class and several functions to make it easy to run a simple document classifier for scientific papers. \n",
        "\n",
        "An example code run through is as follows: \n",
        "\n",
        "\n",
        "```\n",
        "# DRSM BASIC TRAINING ANALYSIS  \n",
        "import datasets\n",
        "\n",
        "column_names =['ID_PAPER', 'Labeling_State', 'Comments', 'Explanation', 'Correct_Label', 'Agreement', 'TRIMMED_TEXT']\n",
        "text_columns = ['TRIMMED_TEXT']\n",
        "label_column = 'Correct_Label'\n",
        "drsm_categories = ['clinical characteristics or disease pathology',\n",
        "              'therapeutics in the clinic', \n",
        "              'disease mechanism', \n",
        "              'patient-based therapeutics', \n",
        "              'other',\n",
        "              'irrelevant']\n",
        "\n",
        "ds_temp = datasets.load_dataset('csv', delimiter=\"\\t\", data_files='/dbfs/FileStore/user/gully/drsm_curated_data/labeled_data_2022_01_03.tsv')\n",
        "train_test_valid = ds_temp['train'].train_test_split(0.1)\n",
        "test_valid = train_test_valid['test'].train_test_split(0.5)\n",
        "drsm_ds = datasets.DatasetDict({\n",
        "    'train': train_test_valid['train'],\n",
        "    'test': test_valid['test'],\n",
        "    'valid': test_valid['train']})"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# export \n",
        "\n",
        "from functools import partial \n",
        "from tqdm import tqdm \n",
        "tqdm = partial(tqdm, position=0, leave=True)\n",
        "\n",
        "import transformers\n",
        "import datasets\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datasets import list_datasets, load_dataset, load_metric\n",
        "from transformers import (AutoTokenizer, \n",
        "                          AutoModelForSequenceClassification, AutoConfig, \n",
        "                          TrainingArguments, Trainer)\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
        "\n",
        "import pickle\n",
        "\n",
        "print(f\"Running on transformers v{transformers.__version__} and datasets v{datasets.__version__}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#export\n",
        "\n",
        "class HF_trainer_wrapper():\n",
        "  '''\n",
        "  Class to provide support training and experimenting with simple document classification tools under either a multi-label or multi-class classification paradigm.\n",
        "\n",
        "  Attributes:\n",
        "  * run_name:  \n",
        "  * model_ckpt:  \n",
        "  * output_dir:  \n",
        "  * logging_dir: \n",
        "  * epochs: \n",
        "  * max_length: \n",
        "  * problem_type: \n",
        "  '''\n",
        "  tokenizer = None\n",
        "  name = ''\n",
        "  text_columns = []\n",
        "  ds = None\n",
        "  \n",
        "  def __init__(self, run_name, model_ckpt, output_dir, logging_dir, epochs, max_length=512, problem_type=\"multi_label_classification\"):\n",
        "    self.run_name = run_name\n",
        "    self.model_ckpt = model_ckpt\n",
        "    self.output_dir = output_dir\n",
        "    self.logging_dir = logging_dir\n",
        "    self.epochs = epochs\n",
        "    self.max_length = max_length\n",
        "    self.problem_type = problem_type\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_ckpt, problem_type=self.problem_type)\n",
        "    \n",
        "  def prepare_dataset(self, ds, text_columns, label_column, categories, problem_type=\"multi_label_classification\"):\n",
        "    self.ds = ds \n",
        "    self.text_columns = text_columns\n",
        "    self.label_column = label_column\n",
        "    self.categories = categories\n",
        "    \n",
        "    def concat_text_fields(row):\n",
        "      t = ''\n",
        "      for f in self.text_columns:\n",
        "        if row[f] is not None:\n",
        "          t += row[f]\n",
        "          if t[-1:] != '.':\n",
        "            t += '.\\n'\n",
        "          else: \n",
        "            t += '\\n'\n",
        "      return {\"text\": '%s'%(t)}     \n",
        "\n",
        "    def tokenize_and_encode(row):\n",
        "      return self.tokenizer(row[\"text\"], \n",
        "                     padding='max_length', \n",
        "                     truncation=True, \n",
        "                     max_length=self.max_length)\n",
        "    \n",
        "    print('Stripping Null Data from datasets')\n",
        "    if problem_type==\"multi_label_classification\": \n",
        "      self.ds = self.ds.map(lambda x : {\"labels\": [1 if x[self.label_column] is not None and c in x[self.label_column] else 0 for c in self.categories] })\n",
        "    else: \n",
        "      self.ds = self.ds.map(lambda x : {\"labels\": [self.categories.index(x[self.label_column])]})\n",
        "    \n",
        "    # NOTE THIS USES THE CONTEXTUALLY DEFINED 'field_list' variable \n",
        "    # implicitly in the concat_text_fields function above \n",
        "    # (not all that great, but not sure how better to do this)\n",
        "    print('Concatonating text fields')\n",
        "    self.ds = self.ds.map(concat_text_fields)\n",
        "    \n",
        "    cols = self.ds[\"train\"].column_names\n",
        "    cols.remove(\"labels\")\n",
        "    print('Tokenizing and encoding')\n",
        "    self.ds_enc = self.ds.map(tokenize_and_encode, \n",
        "                    batched=True, \n",
        "                    remove_columns=cols)\n",
        "\n",
        "    # cast label IDs to floats\n",
        "    self.ds_enc.set_format(\"torch\")\n",
        "    \n",
        "    if problem_type==\"multi_label_classification\": \n",
        "      print('Converting label ints to floats')\n",
        "      self.ds_enc = (self.ds_enc.map(lambda x : \n",
        "                           {\"float_labels\": x[\"labels\"].to(torch.float)},\n",
        "                           remove_columns=[\"labels\"])\n",
        "                .rename_column(\"float_labels\", \"labels\"))\n",
        "    print('Done')\n",
        "  \n",
        "  def build_model(self, loc=None):\n",
        "    if loc is None:\n",
        "      loc = self.model_ckpt\n",
        "     \n",
        "    if self.problem_type == 'multi_label_classification':\n",
        "      num_labels = len( self.ds_enc['train']['labels'][0] )\n",
        "    else:\n",
        "      num_labels = len(set([i[0] for i in self.ds['train']['labels']]))\n",
        "\n",
        "    if os.path.exists(loc):\n",
        "      self.model = AutoModelForSequenceClassification.from_pretrained(loc, \n",
        "                                                                      num_labels=num_labels, \n",
        "                                                                      ignore_mismatched_sizes=True, \n",
        "                                                                      problem_type=self.problem_type).to('cuda')\n",
        "    else: \n",
        "      self.model = AutoModelForSequenceClassification.from_pretrained(loc, \n",
        "                                                                      num_labels=num_labels, \n",
        "                                                                      problem_type=self.problem_type).to('cuda')\n",
        "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "    self.model = self.model.to(device)\n",
        "\n",
        "  def build_trainer(self, warmup_prop = 0.1, batch_size = 8, gradient_accumulation_steps = 2):\n",
        "    \n",
        "    def compute_metrics(pred):\n",
        "      #print(pred.label_ids)\n",
        "      #print(pred.predictions)\n",
        "      if self.problem_type==\"multi_label_classification\": \n",
        "        labels = pred.label_ids\n",
        "        preds = torch.sigmoid(torch.FloatTensor(pred.predictions)).round().long().cpu().detach().numpy()\n",
        "        #preds = [pl>0 for pl in pred.predictions] \n",
        "        #preds = pred.predictions.argmax(-1)\n",
        "      else: \n",
        "        preds = np.argmax(pred.predictions, axis=1)\n",
        "        labels = pred.label_ids\n",
        "      \n",
        "      precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
        "      acc = accuracy_score(labels, preds)\n",
        "\n",
        "      return {\n",
        "          'accuracy': acc,\n",
        "          'f1': f1,\n",
        "          'precision': precision,\n",
        "          'recall': recall\n",
        "      }\n",
        "    \n",
        "    num_train_optimization_steps = int(len(self.ds['train']) / batch_size / gradient_accumulation_steps) * self.epochs\n",
        "    warmup_steps = int(warmup_prop * num_train_optimization_steps)\n",
        "    \n",
        "    self.args = TrainingArguments(learning_rate=2e-5, \n",
        "                                  output_dir=self.output_dir, \n",
        "                                  num_train_epochs=self.epochs,\n",
        "                                  per_device_train_batch_size=batch_size, \n",
        "                                  gradient_accumulation_steps=2,\n",
        "                                  per_device_eval_batch_size=batch_size, \n",
        "                                  evaluation_strategy=\"epoch\",\n",
        "                                  disable_tqdm=False, \n",
        "                                  warmup_steps=warmup_steps, \n",
        "                                  logging_dir=self.logging_dir,\n",
        "                                  run_name=self.run_name)\n",
        "    \n",
        "    self.trainer = Trainer(model = self.model, \n",
        "                           args = self.args,\n",
        "                           train_dataset = self.ds_enc[\"train\"], \n",
        "                           eval_dataset = self.ds_enc[\"valid\"], \n",
        "                           tokenizer = self.tokenizer, \n",
        "                           compute_metrics = compute_metrics)\n",
        "\n",
        "  def train(self, checkpoint=None):\n",
        "    if checkpoint:\n",
        "      self.trainer.train(checkpoint)\n",
        "    else: \n",
        "      self.trainer.train()\n",
        "    tmp_model_path = self.output_dir+'/final_model/'\n",
        "    self.trainer.save_model(tmp_model_path)\n",
        "\n",
        "  def evaluate(self):\n",
        "    self.trainer.evaluate()\n",
        "   \n",
        "  def test(self):\n",
        "    return self.trainer.predict(self.ds_enc['test'])\n",
        "\n",
        "  def save(self):\n",
        "    with open(output_dir+'/hft.pickle', 'wb') as f:\n",
        "      pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#export \n",
        "\n",
        "import mlflow\n",
        "import pickle\n",
        "\n",
        "def run_HF_trainer_expt(ds, text_columns, label_column, categories, run_name, \n",
        "                                   model_input, model_path, log_path, epochs, \n",
        "                                   batch_size=8,\n",
        "                                   transfer_model=None,\n",
        "                                   problem_type=\"multi_label_classification\",\n",
        "                                   run_training=True,\n",
        "                                   freeze_layers=False):\n",
        "  \n",
        "  hft = HF_trainer_wrapper(run_name, model_input, model_path, log_path, epochs, problem_type=problem_type)\n",
        "  hft.prepare_dataset(ds, text_columns, label_column, categories, problem_type=problem_type)\n",
        "  if transfer_model is None:\n",
        "    hft.build_model()\n",
        "  else:\n",
        "    hft.build_model(loc=transfer_model)\n",
        "  \n",
        "  if freeze_layers:\n",
        "    for param in hft.model.bert.parameters():\n",
        "      param.requires_grad = False\n",
        "  \n",
        "  hft.build_trainer(batch_size=batch_size)\n",
        "  if run_training:\n",
        "    hft.train()\n",
        "  pdat = hft.test()\n",
        "  with open(log_path+'/pdat.pkl', 'wb') as f:\n",
        "    # Pickle the 'data' dictionary using the highest protocol available.\n",
        "    pickle.dump(pdat, f, pickle.HIGHEST_PROTOCOL)\n",
        "  \n",
        "  mlflow.log_param('model_input', model_input)\n",
        "  mlflow.log_param('model_path', model_path)\n",
        "  mlflow.log_param('log_path', log_path)\n",
        "  if transfer_model: \n",
        "    mlflow.log_param('transfer_model', transfer_model )\n",
        "  mlflow.log_param('epochs', model_input)\n",
        "  mlflow.log_metric('test_accuracy', pdat.metrics['test_accuracy'])\n",
        "  mlflow.log_metric('test_f1', pdat.metrics['test_f1'])\n",
        "  mlflow.log_metric('test_precision', pdat.metrics['test_precision'])\n",
        "  mlflow.log_metric('test_recall', pdat.metrics['test_recall'])\n",
        "  mlflow.end_run()\n",
        "\n",
        "  return hft"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#export \n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from datasets import Dataset\n",
        "\n",
        "def get_folds_from_dataframe(df, id_col, category_col, n_splits):\n",
        "  folded_ds = []\n",
        "  X = np.array(df[id_col].to_list())\n",
        "  y = np.array(df[category_col].to_list())\n",
        "  skf = StratifiedKFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
        "  skf.get_n_splits(X, y)\n",
        "  for train_index, test_index in skf.split(X, y):\n",
        "    train_valid_df = df.iloc[train_index]\n",
        "    train_df, valid_df = train_test_split(train_valid_df, train_size=.9)\n",
        "    test_df = df.iloc[test_index]\n",
        "    \n",
        "    #checks\n",
        "    for i in test_index:\n",
        "      if i in train_index:\n",
        "        raise ValueError('TEST DATA FOUND IN TRAINING DATA.')\n",
        "    \n",
        "    train = Dataset.from_pandas(train_df)\n",
        "    valid = Dataset.from_pandas(valid_df)\n",
        "    test = Dataset.from_pandas(test_df)\n",
        "    drsm_ds = datasets.DatasetDict({'train': train, 'test': test, 'valid': valid})\n",
        "    folded_ds.append(drsm_ds)\n",
        "  \n",
        "  return folded_ds"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#export \n",
        "\n",
        "import mlflow\n",
        "from datasets import concatenate_datasets\n",
        "\n",
        "def run_HF_trainer_kfold_crossvalidation(folds, text_columns, label_column, categories, run_name, \n",
        "                                                    model_input, model_path, log_path, epochs, \n",
        "                                                    batch_size=8, \n",
        "                                                    problem_type=\"multi_label_classification\",\n",
        "                                                    transfer_model=None, \n",
        "                                                    run_training=True,\n",
        "                                                    freeze_layers=False):\n",
        "  metrics_list = []\n",
        "  for i, fold_ds in enumerate(folds):\n",
        "    \n",
        "    hft = run_HF_trainer_expt(fold_ds, text_columns, label_column, categories, run_name, \n",
        "                                         model_input, model_path+'/fold'+str(i), log_path+'/fold'+str(i), \n",
        "                                         epochs, \n",
        "                                         batch_size=batch_size, \n",
        "                                         problem_type=problem_type, \n",
        "                                         transfer_model=transfer_model, \n",
        "                                         run_training=run_training,\n",
        "                                         freeze_layers=freeze_layers)\n",
        "    \n",
        "    with open(log_path+'/fold'+str(i)+'/pdat.pkl', 'rb') as f:\n",
        "      pdat = pickle.load(f)\n",
        "    tuple = (pdat.metrics['test_accuracy'], pdat.metrics['test_f1'], pdat.metrics['test_precision'], pdat.metrics['test_recall'])\n",
        "    metrics_list.append(tuple)\n",
        "  \n",
        "  df = pd.DataFrame(metrics_list, columns=['test_accuracy','test_f1','test_precision','test_recall'])\n",
        "\n",
        "  return df\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}