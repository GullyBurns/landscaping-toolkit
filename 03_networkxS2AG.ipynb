{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# default_exp networkXS2AG\n",
        "from nbdev import *"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " # NetworkX S2AG Tools \n",
        "\n",
        "> Tools to create and analyze Semantic Scholar Academic Graph (S2AG) data. The initial focus is on analysis of the authors in order to key opinion leaders / prominent researchers based on centrality measures within a network of papers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#export\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import requests\n",
        "import json\n",
        "import os.path\n",
        "from urllib.parse import quote_plus\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import networkx as nx\n",
        "from networkx.algorithms import bipartite\n",
        "import pickle \n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "from scipy.sparse import dok_matrix\n",
        "from scipy import linalg\n",
        "\n",
        "class NetworkxS2AG:\n",
        "  \"\"\"This class permits the construction of a local NetworkX graph that copies the basic organization of S2AG data.<BR>\n",
        "  Functionality includes:\n",
        "    * query all papers, references, and cited papers of a single individual\n",
        "    * build an author-to-author citation graph\n",
        "    * run eigenfactor analysis over the author graph\n",
        "  \n",
        "  Attributes:\n",
        "    * x_api_key: an API key obtained from Semantic Scholar (https://www.semanticscholar.org/product/api)\n",
        "    * author_stem_url, paper_stem_url: urls for API endpoints in S2AG\n",
        "    * features: a regular expression expressed as a string to provide a simple filter for papers in the graph [preliminary]\n",
        "    * g: the networkx graph representing the citation / authorship network \n",
        "    * added_papers: papers that have had all citations and references added to graph\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, x_api_key):\n",
        "    \"\"\" Initialize the interface with an API key. \n",
        "    \"\"\"\n",
        "    self.author_search_url = 'https://api.semanticscholar.org/graph/v1/author/search'\n",
        "    self.author_stem_url = 'https://api.semanticscholar.org/graph/v1/author/'\n",
        "    self.paper_stem_url = 'https://api.semanticscholar.org/graph/v1/paper/'\n",
        "    self.x_api_key = x_api_key\n",
        "    self.features = None\n",
        "    self.added_papers = set()\n",
        "    self.g = nx.DiGraph()\n",
        "\n",
        "  def print_basic_stats(self):\n",
        "    \"\"\" Prints out basic stats about the current graph stored in memory.\n",
        "    \"\"\"\n",
        "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "    print(\"# Papers: %d\"%(len(self.search_nodes('paper'))))\n",
        "    print(\"# Authors: %d\"%(len(self.search_nodes('author'))))\n",
        "    print(\"# Authorship: %d\"%(len(self.search_edges('wrote'))))\n",
        "    print(\"# Citations: %d\"%(len(self.search_edges('cites'))))\n",
        "    infCites = [(e1, e2) for e1,e2,attrs in self.search_edges('cites') if attrs.get('isInfluential')]\n",
        "    print(\"# Influential Citations: %d\"%(len(infCites)))\n",
        "    feature_papers = [n1 for n1,attrs in nxS2.search_nodes('paper') if attrs.get('features')]\n",
        "    print(\"# Papers with feature: %d\"%(len(feature_papers)))\n",
        "    print(\"SCC: %d\"%(nx.number_strongly_connected_components(self.g)))\n",
        "    print(\"WCC: %d\"%(nx.number_weakly_connected_components(self.g)))\n",
        "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "\n",
        "  # ~~~~~~~~~~ HIGH-LEVEL API ~~~~~~~~~~ \n",
        "  def search_for_disambiguated_author(self, author_name):\n",
        "    \"\"\" Searches for an author and returns 10 most influential papers for each disambiguated example. \n",
        "    \"\"\"\n",
        "    search_url = self.author_search_url+'?query='+author_name+'&fields=name,paperCount,hIndex,papers.paperId,papers.title,papers.influentialCitationCount'\n",
        "    r = requests.get(search_url, headers={\"x-api-key\":self.x_api_key})   \n",
        "    author_data = json.loads(r.content.decode(r.encoding))\n",
        "    n_found = author_data.get('total')\n",
        "    df2 = pd.DataFrame(author_data.get('data'))\n",
        "    #df2 = df2[df2.paperCount > n_papers_threshold]\n",
        "    paper_titles = []\n",
        "    for row2 in df2.itertuples():\n",
        "      p_text_list = '     |     '.join([p.get('title') for p in sorted(row2.papers, key=lambda d: d['influentialCitationCount'], reverse=True)[:10]])\n",
        "      paper_titles.append(p_text_list)\n",
        "      #print(json.dumps(p, indent=4, sort_keys=True))  \n",
        "      #  paperIds.add(p.get('paperId'))\n",
        "    #df2 = df2.drop(columns=['papers'])\n",
        "    df2['Top 10 Pubs'] = paper_titles\n",
        "    return df2\n",
        "\n",
        "  def build_author_citation_graph(self, authorId, influentialOnly=True, pkl_file=None):\n",
        "    \"\"\"This builds a complete graph for a given individual based on \n",
        "    their papers, and 'highly influential' references + citations of those papers.\n",
        "    See https://www.nature.com/articles/547032a. The system will then fill in \n",
        "    all cross-references of papers in this graph that cite each other. \n",
        "    \"\"\"\n",
        "    # Build the S2 author / paper graph\n",
        "    self.addKeyOpinionLeader(authorId)\n",
        "    if influentialOnly:\n",
        "      inf_edges = [(n1,n2) for n1,n2,attrs in self.search_edges('cites') if attrs.get('isInfluential')]\n",
        "      papers_to_add = set([n1 for n1,n2 in inf_edges]).union(set([n2 for n1,n2 in inf_edges]))\n",
        "      self.g = self.get_influential_graph()\n",
        "    else:\n",
        "      papers_to_add = [n1 for n1,attrs in self.search_nodes('paper')]\n",
        "    self.addCitationsOrReferencesToGraph(papers_to_add, 'references', True, pkl_file)\n",
        "    for p in papers_to_add:\n",
        "      self.added_papers.add(p)\n",
        "  \n",
        "  def run_thresholded_centrality_analysis(self, min_pub_count=3, top_n=100 ):\n",
        "    \"\"\"The system will analyse all authors within the graph that have total \n",
        "    number of publications above `min_pub_count` by peforming an author-based\n",
        "    eigenfactor calculation (see [West et al 2013](https://jevinwest.org/papers/West2013JASIST.pdf)) \n",
        "    and then return a pandas data fram of the `top_n` most central authors \n",
        "    in the graph.\n",
        "    \"\"\"\n",
        "    thresholded_authors, counts  = self.threshold_authors_by_pubcount(min_pub_count)\n",
        "    author_eigfacs_df = self.compute_author_eigenfactors(thresholded_authors, verbose=True)\n",
        "    top_n_df = author_eigfacs_df.sort_values('eigenfactor',ascending=False)[0:top_n]\n",
        "    authorIds = [row.id for row in top_n_df.itertuples()]\n",
        "    topn_author_metadata_df, errors = self.query_authors_metadata(authorIds)\n",
        "    return topn_author_metadata_df.set_index('authorId').join(top_n_df.set_index('id'))\n",
        "    \n",
        "  # ~~~~~~~~~~ BUILDING THE GRAPH FROM S2AG ~~~~~~~~~~    \n",
        "  def executeSemScholAuthorPapersQueryWithOffset(self, authorId, offset, verbose=True):\n",
        "    fields = [\n",
        "        'paperId',\n",
        "        'authors',\n",
        "        'title',\n",
        "        'abstract',\n",
        "        'referenceCount',\n",
        "        'year'\n",
        "      ]\n",
        "    url = '%s%d/papers?fields=%s&limit=1000&offset=%d'%(self.author_stem_url,authorId,','.join(fields),offset)\n",
        "    \n",
        "    if verbose:\n",
        "      print('AUTHOR_ID: %d'%(authorId))\n",
        "      print(url)\n",
        "    \n",
        "    r = requests.get(url, headers={\"x-api-key\":self.x_api_key}, timeout=20)   \n",
        "    author_response = json.loads(r.content.decode(r.encoding))\n",
        "    rdata = author_response.get('data')\n",
        "    \n",
        "    if rdata is None:\n",
        "      return []\n",
        "    \n",
        "    #print(json.dumps(rdata, indent=4, sort_keys=True))\n",
        "\n",
        "    if self.features is not None:\n",
        "      paperTuples = list(set([\n",
        "          (\n",
        "             p_hash.get('paperId'), \n",
        "             len(p_hash.get('authors')), \n",
        "             p_hash.get('referenceCount'),\n",
        "             p_hash.get('year'),\n",
        "             re.search(self.features, '%s. %s'%(p_hash.get('title',''),p_hash.get('abstract',''))) is not None\n",
        "          )\n",
        "          for p_hash in rdata \n",
        "          if p_hash.get('referenceCount') is not None\n",
        "        ]))\n",
        "    else:\n",
        "      paperTuples = list(set([\n",
        "          (\n",
        "             p_hash.get('paperId'), \n",
        "             len(p_hash.get('authors')), \n",
        "             p_hash.get('referenceCount'),\n",
        "             p_hash.get('year'),\n",
        "          )\n",
        "          for p_hash in rdata \n",
        "          if p_hash.get('referenceCount') is not None\n",
        "        ]))\n",
        "    \n",
        "    if verbose:\n",
        "      print('Adding papers:'+str(len(paperTuples)))\n",
        "\n",
        "    authorIds = list(set([a_hash.get('authorId') \n",
        "                          for p_hash in rdata \n",
        "                          for a_hash in p_hash.get('authors')\n",
        "                          if a_hash.get('authorId') is not None]))\n",
        "    if verbose:\n",
        "      print('Adding authors:'+str(len(authorIds)))\n",
        "    \n",
        "    authorEdges1 = list(set([(a_hash.get('authorId'),p_hash.get('paperId')) \n",
        "                             for p_hash in rdata \n",
        "                             for a_hash in p_hash.get('authors')\n",
        "                             if a_hash.get('authorId') is not None]))\n",
        "    authorEdges2 = [(e2,e1) for (e1,e2) in authorEdges1]\n",
        "    if verbose:\n",
        "      print('Adding author edges:'+str(len(authorEdges1)))\n",
        "\n",
        "    for tup in paperTuples:\n",
        "      if self.features is not None:\n",
        "        self.g.add_node(tup[0], label='paper', nAuthors=int(tup[1]), nRefs=int(tup[2]), year=tup[3], features=bool(tup[4]))\n",
        "      else:\n",
        "        self.g.add_node(tup[0], label='paper', nAuthors=int(tup[1]), nRefs=int(tup[2]), year=tup[3])\n",
        "        \n",
        "    self.g.add_nodes_from(authorIds, label='author' )\n",
        "    self.g.add_edges_from(authorEdges1, label='wrote')\n",
        "    self.g.add_edges_from(authorEdges2, label='was_written_by')\n",
        "    \n",
        "    return rdata\n",
        "\n",
        "  # structure of data \n",
        "  # kolId = [{paperId,\n",
        "  #                authors:[{authorId,name}], \n",
        "  #                citations:[{paperId,authors:[{authorId,name}]}], \n",
        "  #                references:[{paperId,authors:[{authorId,name}]}]}]\n",
        "  def runSemScholAuthorPapersQuery(self, authorId, verbose=False):\n",
        "    offset = 0\n",
        "    rdata = []\n",
        "    while len(rdata)%1000 == 0: \n",
        "      rdata = self.executeSemScholAuthorPapersQueryWithOffset(authorId, offset, verbose)\n",
        "      offset += 1000\n",
        "  \n",
        "  def addKeyOpinionLeader(self, kolId, pkl_file=None, verbose=False):\n",
        "    '''Given an author `kol`, add all papers published by `kol` to `g`. \n",
        "    Then, add all citations and references of those papers to `g`, \n",
        "    and add them to `added papers`. \n",
        "    Note - if the self.features attribute is set, only papers that conform\n",
        "    to that regular expression will be included in the graph.\n",
        "    '''\n",
        "    self.runSemScholAuthorPapersQuery(kolId, verbose=verbose)\n",
        "    if self.features is not None:\n",
        "      self.g = self.get_featured_graph()\n",
        "    kol_papers = [e2 for e1, e2, attrs in self.g.out_edges(str(kolId), data=True) if attrs.get('label') == 'wrote']  \n",
        "    self.addCitationsOrReferencesToGraph(kol_papers, 'citations', False, pkl_file)\n",
        "    self.addCitationsOrReferencesToGraph(kol_papers, 'references', False, pkl_file)\n",
        "    if self.features is not None:\n",
        "      self.g = self.get_featured_graph()\n",
        "    for p in kol_papers:\n",
        "      self.added_papers.add(p)\n",
        "    \n",
        "  def addCitationsOrReferencesWithOffset(self, paperId, citref, offset, isClosed, verbose=False):\n",
        "    if citref == 'citations':\n",
        "      citing_cited = 'citingPaper'\n",
        "    elif citref == 'references':\n",
        "      citing_cited = 'citedPaper' \n",
        "    else:\n",
        "      raise Exception('error with citref: '+citref)\n",
        "\n",
        "    paper_stem_url = 'https://api.semanticscholar.org/graph/v1/paper/'\n",
        "    fields = [\n",
        "        'paperId',\n",
        "        'authors',\n",
        "        'isInfluential',\n",
        "        'referenceCount',\n",
        "        'title',\n",
        "        'abstract',\n",
        "        'year'\n",
        "      ]\n",
        "    url = '%s%s/%s?fields=%s&limit=1000&offset=%d'%(paper_stem_url, paperId, citref, ','.join(fields), offset)\n",
        "    r = requests.get(url, headers={\"x-api-key\":self.x_api_key}, timeout=20)   \n",
        "    paper_response = json.loads(r.content.decode(r.encoding))\n",
        "    rdata = paper_response.get('data')\n",
        "    \n",
        "    if verbose:\n",
        "      print('\\n'+str(paperId))\n",
        "      print(url)\n",
        "    #print(json.dumps(rdata, indent=4, sort_keys=True))\n",
        "  \n",
        "    try:\n",
        "      \n",
        "      if self.features is not None:\n",
        "        paperTuples = list(set([(p_hash.get(citing_cited).get('paperId'), \n",
        "                                len(p_hash.get(citing_cited).get('authors')), \n",
        "                                p_hash.get(citing_cited).get('referenceCount'),\n",
        "                                p_hash.get(citing_cited).get('year'),\n",
        "                                re.search(self.features, '%s. %s'%(p_hash.get(citing_cited).get('title'),p_hash.get(citing_cited).get('abstract'))) is not None \n",
        "                                )\n",
        "                           for p_hash in rdata\n",
        "                           if p_hash.get(citing_cited).get('paperId') is not None \n",
        "                              and p_hash.get(citing_cited).get('referenceCount') is not None]))\n",
        "      else:\n",
        "        paperTuples = list(set([(p_hash.get(citing_cited).get('paperId'), \n",
        "                                len(p_hash.get(citing_cited).get('authors')), \n",
        "                                p_hash.get(citing_cited).get('referenceCount'),\n",
        "                                p_hash.get(citing_cited).get('year'))\n",
        "                           for p_hash in rdata\n",
        "                           if p_hash.get(citing_cited).get('paperId') is not None \n",
        "                              and p_hash.get(citing_cited).get('referenceCount') is not None]))\n",
        "      if verbose:\n",
        "        print('Adding papers:'+str(len(paperTuples)))\n",
        "            \n",
        "      authorIds = list(set([a_hash.get('authorId') \n",
        "                            for p_hash in rdata \n",
        "                            for a_hash in p_hash.get(citing_cited).get('authors')\n",
        "                            if a_hash.get('authorId') is not None]))\n",
        "      if verbose:\n",
        "        print('Adding authors:'+str(len(authorIds)))\n",
        "      \n",
        "      authorEdges1 = list(set([(a_hash.get('authorId'), p_hash.get(citing_cited).get('paperId')) \n",
        "                               for p_hash in rdata \n",
        "                               for a_hash in p_hash.get(citing_cited).get('authors')\n",
        "                               if a_hash.get('authorId') is not None]))\n",
        "      authorEdges2 = [(e2,e1) for (e1,e2) in authorEdges1]\n",
        "      if verbose:\n",
        "        print('Adding author edges:'+str(len(authorEdges1)))\n",
        "            \n",
        "      if citref == 'citations':\n",
        "        citEdges = list(set([(p_hash.get(citing_cited).get('paperId'), paperId, p_hash.get('isInfluential')) \n",
        "                                    for p_hash in rdata \n",
        "                                    if p_hash.get(citing_cited).get('paperId') is not None]))\n",
        "      else: \n",
        "        citEdges = list(set([(paperId, p_hash.get(citing_cited).get('paperId'), p_hash.get('isInfluential')) \n",
        "                                    for p_hash in rdata \n",
        "                                    if p_hash.get(citing_cited).get('paperId') is not None]))\n",
        "        \n",
        "    except:\n",
        "      print('Error in adding citations from paper: '+paperId)\n",
        "      return []\n",
        "\n",
        "    if verbose:\n",
        "      print('Adding citations:'+str(len(citEdges))+'\\n')\n",
        "    \n",
        "    if isClosed:\n",
        "      checked_edge_list = [(e1,e2,sig) for e1,e2,sig in citEdges if e1 in self.g.nodes and e2 in self.g.nodes]\n",
        "      for e1, e2, isInf in checked_edge_list:\n",
        "        self.g.add_edge(e1, e2, label='cites', isInfluential=isInf)        \n",
        "    else:\n",
        "      for tup in paperTuples:\n",
        "        if self.features is not None:\n",
        "          self.g.add_node(tup[0], label='paper', nAuthors=int(tup[1]), nRefs=int(tup[2]), year=tup[3], features=tup[4])\n",
        "        else: \n",
        "          self.g.add_node(tup[0], label='paper', nAuthors=int(tup[1]), nRefs=int(tup[2]), year=tup[3])\n",
        "        #print(tup)\n",
        "      self.g.add_nodes_from(authorIds, label='author' )\n",
        "      self.g.add_edges_from(authorEdges1, label='wrote')\n",
        "      self.g.add_edges_from(authorEdges2, label='was_written_by')\n",
        "      for e1, e2, isInf in citEdges:\n",
        "        self.g.add_edge(e1, e2, label='cites', isInfluential=isInf)        \n",
        "    return rdata\n",
        "  \n",
        "  def addCitationsOrReferencesToGraph(self, paperIds, citref, isClosed, pklpath=None):\n",
        "    \n",
        "    for i, paperId in tqdm(enumerate(paperIds), total=len(paperIds)):\n",
        "      \n",
        "      if pklpath and i%1000==0: # checkpoint save\n",
        "        with open(pklpath, 'wb') as f:\n",
        "          pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)    \n",
        "      \n",
        "      if paperId in self.added_papers:\n",
        "        continue\n",
        "      \n",
        "      offset = 0\n",
        "      rdata = []\n",
        "      while len(rdata)%1000 == 0: \n",
        "        rdata = self.addCitationsOrReferencesWithOffset(paperId, citref, offset, isClosed)\n",
        "        if len(rdata) == 0:\n",
        "          break      \n",
        "        offset += 1000 \n",
        "        \n",
        "    # Final save\n",
        "    if pklpath:\n",
        "      with open(pklpath, 'wb') as f:\n",
        "        pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
        "        \n",
        "  # ~~~~~~~~~~ S2 METADATA QUERIES ~~~~~~~~~~  \n",
        "        \n",
        "  def executeSemScholAuthorQuery(self, authorId):\n",
        "    fields = [\n",
        "      'authorId',\n",
        "      'name',\n",
        "      'paperCount',\n",
        "      'citationCount',\n",
        "      'hIndex',\n",
        "      'papers.paperId',\n",
        "      'papers.title',\n",
        "      'papers.influentialCitationCount'\n",
        "      ]\n",
        "    url = '%s%d?fields=%s'%(self.author_stem_url, authorId,','.join(fields))\n",
        "    #print(url)\n",
        "    r = requests.get(url, headers={\"x-api-key\":self.x_api_key}, timeout=20)   \n",
        "    author_response = json.loads(r.content.decode(r.encoding))\n",
        "\n",
        "    p_text_list = '     |     '.join([p.get('title') for p in sorted(author_response['papers'], key=lambda d: d['influentialCitationCount'], reverse=True)[:10]])\n",
        "    author_response['Top_10_Pubs'] = p_text_list\n",
        "    author_response['authorId'] = int(author_response['authorId'])\n",
        "    author_response.pop('papers')\n",
        "    #print(json.dumps(author_response, indent=4, sort_keys=True))\n",
        "\n",
        "    return author_response\n",
        "\n",
        "  def query_authors_metadata(self, authorIds):\n",
        "    extras = []\n",
        "    errors = []\n",
        "    for authorId in tqdm(authorIds):\n",
        "      try:\n",
        "        rdata = self.executeSemScholAuthorQuery(int(authorId))\n",
        "        extras.append(rdata)\n",
        "      except ReadTimeoutError as e:\n",
        "        print('Error in '+authorId)\n",
        "        errors.append(authorId)\n",
        "    extras_df = pd.DataFrame(extras)\n",
        "    extras_df.set_index('authorId')\n",
        "    return extras_df, errors\n",
        "  \n",
        "  # ~~~~~~~~~~ INFLUENTIAL GRAPH FUNCTIONS ~~~~~~~~~~  \n",
        "  def get_featured_graph(self):\n",
        "    featured_papers = [n1 for n1,attrs in self.search_nodes('paper') if attrs.get('features')]\n",
        "    return self.build_new_graph(featured_papers)\n",
        "\n",
        "  def get_influential_graph(self):\n",
        "    inf_edges = [(n1,n2) for n1,n2,attrs in self.search_edges('cites') if attrs.get('isInfluential')]\n",
        "    inf_papers =  set([n1 for n1,n2 in inf_edges]).union(set([n2 for n1,n2 in inf_edges]))\n",
        "    return self.build_new_graph(inf_papers)\n",
        "  \n",
        "  def build_new_graph(self, paperIds):\n",
        "    g = self.g\n",
        "    g2 = nx.DiGraph()\n",
        "    for paperId in sorted(list(paperIds)):\n",
        "      nAuthors = g.nodes[paperId].get('nAuthors')\n",
        "      nRefs = g.nodes[paperId].get('nRefs')\n",
        "      year = g.nodes[paperId].get('year')\n",
        "      features = g.nodes[paperId].get('features')\n",
        "      if features is not None:\n",
        "        g2.add_node(paperId, label='paper', nAuthors=nAuthors, nRefs=nRefs, year=year, features=features)\n",
        "      else:\n",
        "        g2.add_node(paperId, label='paper', nAuthors=nAuthors, nRefs=nRefs, year=year)\n",
        "      for e1, e2, attrs in g.out_edges(paperId, data=True): \n",
        "        if attrs.get('label') == 'was_written_by': \n",
        "          g2.add_node(e2, label='author')\n",
        "          g2.add_edge(e1, e2, label='was_written_by')\n",
        "          g2.add_edge(e2, e1, label='wrote')\n",
        "        elif attrs.get('label') == 'cites' and e2 in paperIds: \n",
        "          if e2 not in g2.nodes: \n",
        "            nAuthors = g.nodes[e2].get('nAuthors')\n",
        "            nRefs = g.nodes[e2].get('nRefs')\n",
        "            year = g.nodes[e2].get('year')\n",
        "            features = g.nodes[e2].get('features')\n",
        "            if features is not None:\n",
        "              g2.add_node(e2, label='paper', nAuthors=nAuthors, nRefs=nRefs, year=year, features=features)\n",
        "            else:\n",
        "              g2.add_node(e2, label='paper', nAuthors=nAuthors, nRefs=nRefs, year=year)\n",
        "          isInf = attrs.get('isInfluential')\n",
        "          g2.add_edge(e1, e2, label='cites', isInfluential=isInf)\n",
        "    return g2\n",
        "\n",
        "  # ~~~~~~~~~~ AUTHOR-INFLUENCE-GRAPH FUNCTIONS ~~~~~~~~~~  \n",
        "  def threshold_authors_by_pubcount(self, min_pub_count):\n",
        "    authors = sorted([int(a) for a,attrs in self.g.nodes.data() if attrs.get('label')=='author'])\n",
        "    authors_to_id = {a:i for i,a in enumerate(authors)}\n",
        "    n_authors = len(authors)\n",
        "    thresholded_authors = []\n",
        "    counts = []\n",
        "    for i,a in tqdm(enumerate(sorted(authors)), total=n_authors):\n",
        "      n_authors_articles = len([(e1,e2) for e1,e2 in self.g.out_edges(str(a)) if self.g.edges[e1,e2].get('label')=='wrote'])\n",
        "      counts.append(n_authors_articles)\n",
        "      if n_authors_articles > min_pub_count:\n",
        "        thresholded_authors.append(a)\n",
        "    return thresholded_authors, counts\n",
        "  \n",
        "  def compute_author_eigenfactors(self, thresholded_authors, alpha=0.99, verbose=True):\n",
        "    thresholded_authors_df = pd.DataFrame(thresholded_authors, columns=['id'])\n",
        "    if self.features is not None:\n",
        "      feature_positive_authors = set([n1 for n1,n2,attrs in self.search_edges('wrote') if self.g.nodes[n2]['features']])\n",
        "      thresholded_authors_df['features'] = [row.id in feature_positive_authors for row in thresholded_authors_df.itertuples()]\n",
        "    n_thresholded_authors = len(thresholded_authors)\n",
        "    thresholded_authors_to_id = {a:i for i,a in enumerate(thresholded_authors)}\n",
        "    \n",
        "    if verbose:\n",
        "      print(\"Computing Z for %d authors\"%(n_thresholded_authors))\n",
        "    Z = dok_matrix((n_thresholded_authors, n_thresholded_authors), dtype=np.float64)\n",
        "    for i, authorId in tqdm(enumerate(thresholded_authors), total=len(thresholded_authors)):\n",
        "      edgeMap = self.compute_edges_for_author(authorId)\n",
        "      for t in edgeMap.keys():  \n",
        "        if int(t) in thresholded_authors:\n",
        "          j = thresholded_authors_to_id[int(t)]\n",
        "          Z[i,j] = edgeMap[t]\n",
        "    \n",
        "    if verbose:\n",
        "      print(\"Computing M = Z / Z_colsum\")\n",
        "    Z_colsum = np.sum(Z, axis=0) \n",
        "    M = Z / Z_colsum  \n",
        "    \n",
        "    if verbose:\n",
        "      print(\"Computing A = teleport probability\")\n",
        "    v = []\n",
        "    n_all_articles = len([nid for nid, attrs in self.g.nodes.data() if attrs.get('label')=='paper'])\n",
        "    for i,a in tqdm(enumerate(thresholded_authors), total=n_thresholded_authors):\n",
        "      n_authors_articles = len([(e1,e2) for e1,e2 in self.g.out_edges(str(a)) if self.g.edges[e1,e2].get('label')=='wrote'])\n",
        "      v.append(n_authors_articles / n_all_articles)\n",
        "    A = np.array([v]).T @ np.ones((1,n_thresholded_authors))\n",
        "    \n",
        "    if verbose:\n",
        "      print(\"Computing P = alpha * M + (1-alpha) * A (alpha=%f)\"%(alpha))\n",
        "    P = alpha * M + (1-alpha) * A\n",
        "    \n",
        "    if verbose:\n",
        "      print(\"Computing Eigenfactors + Eigenvectors\")\n",
        "    PP = P + P.T\n",
        "    print(PP)\n",
        "    eigf, eigv = linalg.eig(np.nan_to_num(PP))\n",
        "\n",
        "    if verbose:\n",
        "      print(\"Done\")\n",
        "    \n",
        "    leading_eigenvector_index = np.argmax(eigf)\n",
        "    f = eigv[:,leading_eigenvector_index].real#.reshape(P.shape[0],1)\n",
        "    thresholded_authors_df['eigenfactor'] = f \n",
        "    \n",
        "    return thresholded_authors_df\n",
        "  \n",
        "  # modified from ./networkx/algorithms/traversal/breadth_first_search.py\n",
        "  def search_for_reference_author_pathways(self, source_author):\n",
        "    depth_limit = 3\n",
        "    out = []\n",
        "    queue = deque([('', source_author, depth_limit, self.g.successors(source_author))])\n",
        "    while queue:\n",
        "      route, parent, depth_now, children = queue[0]\n",
        "      for child in children:\n",
        "        if (self.g.nodes[child].get('label')=='paper' and depth_now>1) or (self.g.nodes[child].get('label')=='author' and depth_now==1):    \n",
        "          out.append(route+'|'+parent+'|'+child)\n",
        "          if depth_now > 1:\n",
        "            queue.append((route+'|'+parent, child, depth_now - 1, self.g.successors(child)))\n",
        "      queue.popleft()\n",
        "    return out\n",
        "\n",
        "\n",
        "  # modified from ./networkx/algorithms/traversal/breadth_first_search.py\n",
        "  def search_for_citation_author_pathways(self, source_author):\n",
        "    depth_limit = 3\n",
        "    out = []\n",
        "    queue = deque([('', source_author, depth_limit, self.g.predecessors(source_author))])\n",
        "    while queue:\n",
        "      route, parent, depth_now, children = queue[0]\n",
        "      for child in children:\n",
        "        if (self.g.nodes[child].get('label')=='paper' and depth_now>1) or (self.g.nodes[child].get('label')=='author' and depth_now==1):    \n",
        "          out.append(route+'|'+parent+'|'+child)\n",
        "          if depth_now > 1:\n",
        "            queue.append((route+'|'+parent, child, depth_now - 1, self.g.predecessors(child)))\n",
        "      queue.popleft()\n",
        "    return out\n",
        "\n",
        "  def compute_edges_for_author(self, a, forward=True):\n",
        "    weights = {}\n",
        "    if forward:\n",
        "      traversals = self.search_for_citation_author_pathways(str(a))\n",
        "    else:\n",
        "      traversals = self.search_for_reference_author_pathways(str(a))\n",
        "    for routes_string in traversals:\n",
        "      #routes_string = r+'|'+p+'|'+c\n",
        "      l = re.split('\\|', routes_string)\n",
        "      if len(l) == 5:\n",
        "        xxx, a1, p1, p2, a2 = l\n",
        "        p1_nAuthors = self.g.nodes[p1]['nAuthors'] if self.g.nodes[p1]['nAuthors']>0 else 5      \n",
        "        p1_nRefs = self.g.nodes[p1]['nRefs'] if self.g.nodes[p1]['nRefs']>0 else 100\n",
        "        p2_nAuthors = self.g.nodes[p2]['nAuthors'] if self.g.nodes[p2]['nAuthors']>0 else 100\n",
        "        x = 1 / (p1_nAuthors*p1_nRefs*p2_nAuthors) \n",
        "        a2 = int(l[4])\n",
        "        if weights.get(l[4]) is None:\n",
        "          weights[l[4]] = x\n",
        "        else: \n",
        "          weights[l[4]] += x\n",
        "    return weights\n",
        "  \n",
        "  \n",
        "  # ~~~~~~~~~~ UTILITIES ~~~~~~~~~~\n",
        "  def clone(self):\n",
        "    copy = NetworkxS2AG(self.x_api_key)\n",
        "    copy.added_papers = self.added_papers\n",
        "    copy.g = self.g.copy()\n",
        "    copy.cit_g = self.cit_g.copy()\n",
        "    return copy\n",
        "    \n",
        "  def search_nodes(self, label):\n",
        "    nids = [(nid,attrs) for nid, attrs in self.g.nodes.data() if attrs.get('label')==label]\n",
        "    return nids\n",
        "\n",
        "  def search_edges(self, label):\n",
        "    edges = [(e1,e2,attrs) for e1,e2,attrs in self.g.edges.data() if attrs.get('label')==label]\n",
        "    return edges\n",
        "  \n",
        "  def load_from_pickle(self, file):\n",
        "    with open(file, 'rb') as f:\n",
        "      loaded_copy = pickle.load(f) \n",
        "    self.added_papers = loaded_copy.added_papers\n",
        "    self.g = loaded_copy.g\n",
        "\n",
        "  def save_to_pickle(self, file):\n",
        "    with open(file, 'wb') as f:\n",
        "      pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "  def check_before_adding_edges_from(self, edge_list, label):\n",
        "    checked_edge_list = [(e1,e2) for e1,e2 in edge_list if e1 in self.g.nodes and e2 in self.g.nodes]\n",
        "    if len(checked_edge_list) > 0: \n",
        "      print('adding %d new edges'%(len(checked_edge_list)))\n",
        "      self.g.add_edges_from(checked_edge_list, label=label)\n",
        "      \n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "show_doc(NetworkxS2AG.print_basic_stats)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "show_doc(NetworkxS2AG.search_for_disambiguated_author)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "show_doc(NetworkxS2AG.build_author_citation_graph)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "show_doc(NetworkxS2AG.run_thresholded_centrality_analysis)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}