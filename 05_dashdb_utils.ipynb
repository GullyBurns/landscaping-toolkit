{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAGIC %pip install git+https://github.com/GullyBurns/czLandscapingTk.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAGIC %run ./01_query_translator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev\n",
    "nbdev.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAGIC %run ./02_search_engine_eutils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAGIC %run ./04_airtable_utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dashdbUtils\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Dashboard Database Utilities\n",
    "\n",
    "> Simple query classes that allows contruction of a SQL database tables for science literature dashboard applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "from cryptography.hazmat.primitives.asymmetric import rsa\n",
    "from cryptography.hazmat.primitives.asymmetric import dsa\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "import io\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "\n",
    "class Platform(Enum):\n",
    "  GC = 'Google Cloud'\n",
    "  DB = 'Databricks'\n",
    "\n",
    "class SecretManager():\n",
    "  def __init__(self, platform):\n",
    "    self.platform = platform\n",
    "  \n",
    "  def get_creds(self, key):\n",
    "    # databricks credentials\n",
    "    if self.platform == Platform.DB:\n",
    "      user = dbutils.secrets.get(scope=key, key=\"SNOWFLAKE_SERVICE_USERNAME\")\n",
    "      pem = dbutils.secrets.get(scope=key, key=\"SNOWFLAKE_SERVICE_PRIVATE_KEY\")\n",
    "      pwd = dbutils.secrets.get(scope=key, key=\"SNOWFLAKE_SERVICE_PASSPHRASE\")\n",
    "    else:\n",
    "      raise Exception(\"Platform not set\")\n",
    "    return (user, pem, pwd)\n",
    "\n",
    "class Snowflake():\n",
    "\n",
    "  def __init__(self, user, pem, pwd, warehouse, database, schema, role):\n",
    "    self.user = user\n",
    "    self.pem = pem\n",
    "    self.pwd = pwd\n",
    "    self.warehouse = warehouse\n",
    "    self.database = database\n",
    "    self.schema = schema\n",
    "    self.role = role\n",
    "\n",
    "    #string_private_key = f\"-----BEGIN ENCRYPTED PRIVATE KEY-----\\n{pem.strip()}\\n-----END ENCRYPTED PRIVATE KEY-----\"\n",
    "    string_private_key = f\"{pem.strip()}\"\n",
    "\n",
    "    p_key = serialization.load_pem_private_key(\n",
    "        io.BytesIO(string_private_key.encode()).read(),\n",
    "        password=pwd.strip().encode(),\n",
    "        backend=default_backend())\n",
    "\n",
    "    pkb = p_key.private_bytes(\n",
    "        encoding=serialization.Encoding.DER,\n",
    "        format=serialization.PrivateFormat.PKCS8,\n",
    "        encryption_algorithm=serialization.NoEncryption())\n",
    "\n",
    "    self.ctx = snowflake.connector.connect(\n",
    "        user=user.strip(),\n",
    "        private_key=pkb,\n",
    "        account='lr02922')\n",
    "\n",
    "    cur = self.ctx.cursor()\n",
    "    cur.execute(\"select current_date;\")\n",
    "    print(cur.fetchone()[0])\n",
    "\n",
    "  def cursor(self):\n",
    "    self.cs = self.ctx.cursor()\n",
    "    return self.cs\n",
    "\n",
    "  def get_cursor(self):\n",
    "    cs = self.cursor()\n",
    "    cs.execute('USE WAREHOUSE ' + self.warehouse)\n",
    "    cs.execute('USE SCHEMA '+self.database+'.'+self.schema)\n",
    "    cs.execute('USE ROLE '+self.role)\n",
    "    print('USE SCHEMA '+self.database+'.'+self.schema)\n",
    "    return cs\n",
    "\n",
    "  def execute_query(self, cs, sql, columns):\n",
    "    cs.execute(sql)\n",
    "    df = pd.DataFrame(cs.fetchall(), columns=columns)\n",
    "    df = df.replace('\\n', ' ', regex=True)\n",
    "    return df\n",
    "\n",
    "  def run_query_in_spark(self, user, query):\n",
    "    string_private_key = f\"{self.pem.strip()}\"\n",
    "\n",
    "    p_key = serialization.load_pem_private_key(\n",
    "        io.BytesIO(string_private_key.encode()).read(),\n",
    "        password=self.pwd.strip().encode(),\n",
    "        backend=default_backend())\n",
    "\n",
    "    pkb = p_key.private_bytes(\n",
    "      encoding = serialization.Encoding.PEM,\n",
    "      format = serialization.PrivateFormat.PKCS8,\n",
    "      encryption_algorithm = serialization.NoEncryption()\n",
    "      )\n",
    "\n",
    "    pkb = pkb.decode(\"UTF-8\")\n",
    "    pkb = re.sub(\"-*(BEGIN|END) PRIVATE KEY-*\\n\",\"\",pkb).replace(\"\\n\",\"\")\n",
    "\n",
    "    # snowflake connection options\n",
    "    options = dict(sfUrl=\"https://lr02922.snowflakecomputing.com/\",\n",
    "                   sfUser=self.user.strip(),\n",
    "                   pem_private_key=pkb,\n",
    "                   sfRole=\"ARST_TEAM\",\n",
    "                   sfDatabase=g_database,\n",
    "                   sfSchema=g_schema,\n",
    "                   sfWarehouse=\"DEV_WAREHOUSE\")\n",
    "\n",
    "    sdf = spark.read \\\n",
    "          .format(\"snowflake\") \\\n",
    "          .options(**options) \\\n",
    "          .option(\"query\", query) \\\n",
    "          .load()\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "sec = SecretManager(Platform.DB)\n",
    "(user, pem, pwd) = sec.get_creds('gully-scope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def build_lookup_table(cs, delete_existing=False):\n",
    "  COLS = ['PMID', 'FIRST_AUTHOR', 'YEAR', 'VOLUME', 'PAGE']\n",
    "  SQL = '''\n",
    "    SELECT p.PMID as PMID, REGEXP_SUBSTR(a.NAME, '\\\\\\\\S*$') as FIRST_AUTHOR, \n",
    "        p.YEAR as YEAR, p.VOLUME as VOLUME, REGEXP_SUBSTR(p.PAGINATION, '^\\\\\\\\d+') as PAGE\n",
    "    FROM FIVETRAN.KG_RDS_CORE_DB.PAPER as p\n",
    "      JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_AUTHOR_v2 as pa on (p.ID=pa.ID_PAPER)\n",
    "      JOIN FIVETRAN.KG_RDS_CORE_DB.AUTHOR_v2 as a on (a.ID=pa.ID_AUTHOR)\n",
    "    WHERE pa.AUTHOR_INDEX=0\n",
    "    ORDER BY p.PMID DESC\n",
    "  '''\n",
    "  BUILD_PMID_LOOKUP_SQL = \"create table if not exists PMID_LOOKUP as \" + SQL\n",
    "  if deleted_existing: \n",
    "    cs.execute('drop table if exists PMID_LOOKUP')\n",
    "  cs.execute(BUILD_PMID_LOOKUP_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from pathlib import Path\n",
    "from czLandscapingTk.searchEngineUtils import ESearchQuery, EuroPMCQuery\n",
    "\n",
    "class DashboardDb:\n",
    "\n",
    "  def __init__(self, prefix, secret_scope, warehouse, database, schema, role, loc):\n",
    "    self._secretManager = SecretManager(Platform.DB)\n",
    "    (user, pem, pwd) = self._secretManager.get_creds(secret_scope)\n",
    "    self.sf = Snowflake(user, pem, pwd, warehouse, database, schema, role)\n",
    "    self.database = database\n",
    "    self.schema = schema\n",
    "    self.loc = loc\n",
    "    self.prefix = prefix\n",
    "\n",
    "    if os.path.exists(loc) is False:\n",
    "      os.mkdir(loc)\n",
    "\n",
    "    log_path = '%s/sf_log.txt' % (loc)\n",
    "    if os.path.exists(log_path) is False:\n",
    "      Path(log_path).touch()\n",
    "\n",
    "    self.temp_annotations_path = '%s/TMP_SO.txt' % (loc)\n",
    "    if os.path.exists(self.temp_annotations_path) is False:\n",
    "      Path(self.temp_annotations_path).touch()\n",
    "\n",
    "    self.temp_documents_path = '%s/TMP_DOC.txt' % (loc)\n",
    "    if os.path.exists(self.temp_documents_path) is False:\n",
    "      Path(self.temp_documents_path).touch()\n",
    "      \n",
    "  #export\n",
    "  def get_cursor(self):\n",
    "    cs = self.sf.get_cursor()\n",
    "    return cs\n",
    "\n",
    "  def execute_query(self, sql, columns):\n",
    "    cs = self.get_cursor()\n",
    "    cs.execute(sql)\n",
    "    df = pd.DataFrame(cs.fetchall(), columns=columns)\n",
    "    df = df.replace('\\n', ' ', regex=True)\n",
    "    return df\n",
    "\n",
    "  def upload_wb(self, df2, table_name, cs=None):\n",
    "    if cs is None: \n",
    "      cs = self.get_cursor()\n",
    "    table_name = re.sub('PREFIX_', self.prefix, 'PREFIX_'+table_name)\n",
    "    df = df2.replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)\n",
    "    df.to_csv(self.loc+'/'+table_name+'.tsv', index=False, header=False, sep='\\t')\n",
    "    cs.execute('DROP TABLE IF EXISTS '+table_name+';')\n",
    "    cols = [re.sub(' ','_',c).lower() for c in df.columns if c is not None]\n",
    "    cols = [c+' INT AUTOINCREMENT' if c=='ID' else c+' TEXT' for c in cols]\n",
    "    cs.execute('CREATE TABLE '+table_name+'('+', '.join(cols)+');')\n",
    "    print(self.loc +'/'+table_name+'.tsv')\n",
    "    cs.execute('put file://' + self.loc +'/'+table_name+'.tsv' + ' @%'+table_name+';')\n",
    "    cs.execute(\"copy into \"+table_name+\" from @%\"+table_name+\" FILE_FORMAT=(TYPE=CSV FIELD_DELIMITER=\\'\\\\t\\')\")\n",
    "\n",
    "  def clear_corpus_to_paper_table(self, cs=None):\n",
    "    if cs is None: \n",
    "      cs = self.get_cursor()\n",
    "    table_name = re.sub('PREFIX_', self.prefix, 'PREFIX_CORPUS_TO_PAPER')\n",
    "    cs.execute('DROP TABLE IF EXISTS ' + table_name + ';')\n",
    "\n",
    "  def build_core_tables_from_pmids(self, cs=None):\n",
    "    if cs is None: \n",
    "      cs = self.get_cursor()\n",
    "    print('PAPER_NOTES')\n",
    "    cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"PAPER_NOTES\")\n",
    "    cs.execute(re.sub('PREFIX_', self.prefix, BUILD_DASHBOARD_PAPER_NOTES))\n",
    "    print('PAPER_OPEN_ACCESS')\n",
    "    cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"PAPER_OPEN_ACCESS\")\n",
    "    cs.execute(re.sub('PREFIX_', self.prefix, BUILD_DASHBOARD_PAPER_OPEN_ACCESS))\n",
    "    print('COLLABORATIONS')\n",
    "    cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"COLLABORATIONS\")\n",
    "    cs.execute(re.sub('PREFIX_', self.prefix, BUILD_DASHBOARD_COLLABORATIONS))\n",
    "    print('ALL KNOWN AUTHOR LOCATIONS')\n",
    "    cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"AUTHOR_LOCATION\")\n",
    "    cs.execute(re.sub('PREFIX_', self.prefix, BUILD_DASHBOARD_AUTHOR_LOCATION))\n",
    "    print('CITATION COUNTS')\n",
    "    cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"CITATION_COUNTS\")\n",
    "    cs.execute(re.sub('PREFIX_', self.prefix, BUILD_DASHBOARD_CITATION_COUNTS))\n",
    "\n",
    "  def drop_database(self, cs=None):\n",
    "    if cs is None: \n",
    "      cs = self.get_cursor()\n",
    "    try:\n",
    "      cs.execute(\"BEGIN\")\n",
    "      cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"AUTHOR_LOCATION\")\n",
    "      cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"CITATION_COUNTS\")\n",
    "      cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"COLLABORATIONS\")\n",
    "      cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"PAPER_NOTES\")\n",
    "      cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"CORPUS\")\n",
    "      cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"CORPUS_TO_PAPER\")\n",
    "      cs.execute(\"COMMIT\")\n",
    "    finally:\n",
    "      cs.close()\n",
    "\n",
    "  def build_database_from_queries(self, prefix, id_col, q_col, subq_col, df, delete_db=True, pm_include=True, epmc_inlcude=True):\n",
    "    qt = QueryTranslator(df, q_col)\n",
    "    qt2 = QueryTranslator(subsets_df, subq_col)\n",
    "        \n",
    "    corpus_paper_list = []\n",
    "    \n",
    "    if pm_include:\n",
    "      (corpus_ids, pubmed_queries) = qt.generate_queries(QueryType.pubmed)\n",
    "      (subset_ids, pubmed_subset_queries) = qt2.generate_queries(QueryType.pubmed)\n",
    "      for (i, q) in enumerate(pubmed_queries):\n",
    "        for (j, sq) in zip(subset_ids, ['']):\n",
    "          if len(sq)>0:\n",
    "          q = '(%s) AND (%s)'%(q,sq) \n",
    "        q = re.sub('\\s+','+',q)\n",
    "        esq = ESearchQuery(g_pubmed_api_key)\n",
    "        pubmed_pmids = esq.execute_query(q)\n",
    "        print(len(pubmed_pmids))\n",
    "        for pmid in pubmed_pmids:\n",
    "          corpus_paper_list.append((pmid, i, 'pubmed', j))\n",
    "\n",
    "    if empc_include:\n",
    "      (corpus_ids, epmc_queries) = qt.generate_queries(QueryType.epmc)\n",
    "      (subset_ids, epmc_subset_queries) = qt2.generate_queries(QueryType.closed)\n",
    "      for (i, q) in (corpus_ids, epmc_queries):\n",
    "        for (j, sq) in zip(subset_ids, epmc_subset_queries):\n",
    "          query = q\n",
    "          if len(sq)>0:\n",
    "            query = '(%s) AND (%s)'%(q, sq) \n",
    "          epmcq = EuroPMCQuery()\n",
    "          numFound, epmc_pmids, other_ids  = epmcq.run_empc_query(query)\n",
    "          for id in tqdm(epmc_pmids):\n",
    "            corpus_paper_list.append((id, i, 'epmc', j))\n",
    "    \n",
    "    corpus_paper_df = DataFrame(corpus_paper_list, columns=['ID_PAPER', 'ID_CORPUS', 'SOURCE', 'SUBSET_CODE'])\n",
    "  \n",
    "    cs = self.get_cursor()\n",
    "    cs.execute(\"BEGIN\")\n",
    "    if delete_db:\n",
    "      self.drop_database(cs=cs)\n",
    "    self.upload_wb(df, 'CORPUS', cs=cs)\n",
    "    self.upload_wb(corpus_paper_df, 'CORPUS_TO_PAPER', cs=cs)\n",
    "    self.build_core_tables_from_pmids(cs=cs)\n",
    "    cs.execute('COMMIT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "import os\n",
    "import re\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "from cryptography.hazmat.primitives.asymmetric import rsa\n",
    "from cryptography.hazmat.primitives.asymmetric import dsa\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "import io\n",
    "\n",
    "def port_from_snowflake_to_databricks(database, schema, table_names, table_sql):\n",
    "  \n",
    "  # Use secret manager to get the login name and password for the Snowflake user\n",
    "  user = dbutils.secrets.get(scope=g_scope, key=\"SNOWFLAKE_SERVICE_USERNAME\")\n",
    "  pem = dbutils.secrets.get(scope=g_scope, key=\"SNOWFLAKE_SERVICE_PRIVATE_KEY\")\n",
    "  pwd = dbutils.secrets.get(scope=g_scope, key=\"SNOWFLAKE_SERVICE_PASSPHRASE\")\n",
    "\n",
    "  string_private_key = f\"{pem.strip()}\"\n",
    "\n",
    "  p_key = serialization.load_pem_private_key(\n",
    "      io.BytesIO(string_private_key.encode()).read(),\n",
    "      password=pwd.strip().encode(),\n",
    "      backend=default_backend())\n",
    "\n",
    "  pkb = p_key.private_bytes(\n",
    "    encoding = serialization.Encoding.PEM,\n",
    "    format = serialization.PrivateFormat.PKCS8,\n",
    "    encryption_algorithm = serialization.NoEncryption()\n",
    "    )\n",
    "\n",
    "  pkb = pkb.decode(\"UTF-8\")\n",
    "  pkb = re.sub(\"-*(BEGIN|END) PRIVATE KEY-*\\n\",\"\",pkb).replace(\"\\n\",\"\")\n",
    "\n",
    "  # snowflake connection options\n",
    "  options = dict(sfUrl=\"https://lr02922.snowflakecomputing.com/\",\n",
    "                 sfUser=user.strip(),\n",
    "                 pem_private_key=pkb,\n",
    "                 sfRole=\"ARST_TEAM\",\n",
    "                 sfDatabase=database,\n",
    "                 sfSchema=schema,\n",
    "                 sfWarehouse=\"DEV_WAREHOUSE\")\n",
    "\n",
    "  table_dict = {}\n",
    "  for name, sql in zip(table_names, table_sql):\n",
    "      table_dict[name] = re.sub('PREFIX_', prefix, sql)     \n",
    "\n",
    "  sqlContext.sql('CREATE DATABASE IF NOT EXISTS ' + prefix )\n",
    "  sqlContext.sql('USE '+prefix )\n",
    "  for t in table_names:\n",
    "    sqlContext.sql('DROP TABLE IF EXISTS ' + t)\n",
    "  for t in table_names:\n",
    "    sdf = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**options) \\\n",
    "        .option(\"query\", table_dict[t]) \\\n",
    "        .load()\n",
    "    print(t)\n",
    "    print(table_dict[t])\n",
    "    sdf.write.saveAsTable(t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
