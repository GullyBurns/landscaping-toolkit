{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp dashdbUtils\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cryptography.hazmat.backends import default_backend\n",
    "from cryptography.hazmat.primitives.asymmetric import rsa\n",
    "from cryptography.hazmat.primitives.asymmetric import dsa\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "import io\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "\n",
    "class Platform(Enum):\n",
    "  GC = 'Google Cloud'\n",
    "  DB = 'Databricks'\n",
    "\n",
    "class SecretManager():\n",
    "  def __init__(self, platform):\n",
    "    self.platform = platform\n",
    "  \n",
    "  def get_creds(self, key):\n",
    "    # databricks credentials\n",
    "    if self.platform == Platform.DB:\n",
    "      user = dbutils.secrets.get(scope=key, key=\"SNOWFLAKE_SERVICE_USERNAME\")\n",
    "      pem = dbutils.secrets.get(scope=key, key=\"SNOWFLAKE_SERVICE_PRIVATE_KEY\")\n",
    "      pwd = dbutils.secrets.get(scope=key, key=\"SNOWFLAKE_SERVICE_PASSPHRASE\")\n",
    "    else:\n",
    "      raise Exception(\"Platform not set\")\n",
    "    return (user, pem, pwd)\n",
    "\n",
    "class Snowflake():\n",
    "\n",
    "  def __init__(self, user, pem, pwd, warehouse, database, schema, role):\n",
    "    self.user = user\n",
    "    self.pem = pem\n",
    "    self.pwd = pwd\n",
    "    self.warehouse = warehouse\n",
    "    self.database = database\n",
    "    self.schema = schema\n",
    "    self.role = role\n",
    "\n",
    "    #string_private_key = f\"-----BEGIN ENCRYPTED PRIVATE KEY-----\\n{pem.strip()}\\n-----END ENCRYPTED PRIVATE KEY-----\"\n",
    "    string_private_key = f\"{pem.strip()}\"\n",
    "\n",
    "    p_key = serialization.load_pem_private_key(\n",
    "        io.BytesIO(string_private_key.encode()).read(),\n",
    "        password=pwd.strip().encode(),\n",
    "        backend=default_backend())\n",
    "\n",
    "    pkb = p_key.private_bytes(\n",
    "        encoding=serialization.Encoding.DER,\n",
    "        format=serialization.PrivateFormat.PKCS8,\n",
    "        encryption_algorithm=serialization.NoEncryption())\n",
    "\n",
    "    self.ctx = snowflake.connector.connect(\n",
    "        user=user.strip(),\n",
    "        private_key=pkb,\n",
    "        account='lr02922')\n",
    "\n",
    "    cur = self.ctx.cursor()\n",
    "    cur.execute(\"select current_date;\")\n",
    "    print(cur.fetchone()[0])\n",
    "\n",
    "  def cursor(self):\n",
    "    self.cs = self.ctx.cursor()\n",
    "    return self.cs\n",
    "\n",
    "  def get_cursor(self):\n",
    "    cs = self.cursor()\n",
    "    cs.execute('USE WAREHOUSE ' + self.warehouse)\n",
    "    cs.execute('USE SCHEMA '+self.database+'.'+self.schema)\n",
    "    cs.execute('USE ROLE '+self.role)\n",
    "    print('USE SCHEMA '+self.database+'.'+self.schema)\n",
    "    return cs\n",
    "\n",
    "  def execute_query(self, cs, sql, columns):\n",
    "    cs.execute(sql)\n",
    "    df = pd.DataFrame(cs.fetchall(), columns=columns)\n",
    "    df = df.replace('\\n', ' ', regex=True)\n",
    "    return df\n",
    "\n",
    "  def run_query_in_spark(self, user, query):\n",
    "\n",
    "    string_private_key = f\"{self.pem.strip()}\"\n",
    "\n",
    "    p_key = serialization.load_pem_private_key(\n",
    "        io.BytesIO(string_private_key.encode()).read(),\n",
    "        password=self.pwd.strip().encode(),\n",
    "        backend=default_backend())\n",
    "\n",
    "    pkb = p_key.private_bytes(\n",
    "      encoding = serialization.Encoding.PEM,\n",
    "      format = serialization.PrivateFormat.PKCS8,\n",
    "      encryption_algorithm = serialization.NoEncryption()\n",
    "      )\n",
    "\n",
    "    pkb = pkb.decode(\"UTF-8\")\n",
    "    pkb = re.sub(\"-*(BEGIN|END) PRIVATE KEY-*\\n\",\"\",pkb).replace(\"\\n\",\"\")\n",
    "\n",
    "    # snowflake connection options\n",
    "    options = dict(sfUrl=\"https://lr02922.snowflakecomputing.com/\",\n",
    "                   sfUser=self.user.strip(),\n",
    "                   pem_private_key=pkb,\n",
    "                   sfRole=\"ARST_TEAM\",\n",
    "                   sfDatabase=g_database,\n",
    "                   sfSchema=g_schema,\n",
    "                   sfWarehouse=\"DEV_WAREHOUSE\")\n",
    "\n",
    "    sdf = spark.read \\\n",
    "          .format(\"snowflake\") \\\n",
    "          .options(**options) \\\n",
    "          .option(\"query\", query) \\\n",
    "          .load()\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "sec = SecretManager(Platform.DB)\n",
    "(user, pem, pwd) = sec.get_creds('gully-scope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET UP LOCAL VARIABLES FOR DASHBOARD DATABASE CREATION HERE\n",
    "\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dash_cursor(sf, mcdash):\n",
    "  cs = sf.cursor()\n",
    "  cs.execute('USE WAREHOUSE DEV_WAREHOUSE')\n",
    "  cs.execute('USE SCHEMA '+mcdash.database+'.'+mcdash.schema)\n",
    "  cs.execute('USE ROLE ARST_TEAM')\n",
    "  print('USE SCHEMA '+mcdash.database+'.'+mcdash.schema)\n",
    "  return cs\n",
    "\n",
    "def execute_query(cs, sql, columns):\n",
    "  cs.execute(sql)\n",
    "  df = pd.DataFrame(cs.fetchall(), columns=columns)\n",
    "  df = df.replace('\\n', ' ', regex=True)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lookup_table(cs, delete_existing=False):\n",
    "  COLS = ['PMID', 'FIRST_AUTHOR', 'YEAR', 'VOLUME', 'PAGE']\n",
    "  SQL = '''\n",
    "    SELECT p.PMID as PMID, REGEXP_SUBSTR(a.NAME, '\\\\\\\\S*$') as FIRST_AUTHOR, \n",
    "        p.YEAR as YEAR, p.VOLUME as VOLUME, REGEXP_SUBSTR(p.PAGINATION, '^\\\\\\\\d+') as PAGE\n",
    "    FROM FIVETRAN.KG_RDS_CORE_DB.PAPER as p\n",
    "      JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_AUTHOR_v2 as pa on (p.ID=pa.ID_PAPER)\n",
    "      JOIN FIVETRAN.KG_RDS_CORE_DB.AUTHOR_v2 as a on (a.ID=pa.ID_AUTHOR)\n",
    "    WHERE pa.AUTHOR_INDEX=0\n",
    "    ORDER BY p.PMID DESC\n",
    "  '''\n",
    "  BUILD_PMID_LOOKUP_SQL = \"create table if not exists PMID_LOOKUP as \" + SQL\n",
    "  if deleted_existing: \n",
    "    cs.execute('drop table if exists PMID_LOOKUP')\n",
    "  cs.execute(BUILD_PMID_LOOKUP_SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metaDash.py - GENERATE DASHBOARD TABLES IN SNOWFLAKE\n",
    "\n",
    "import os.path\n",
    "from itertools import islice\n",
    "from pandas import DataFrame\n",
    "from urllib.parse import quote_plus\n",
    "import re\n",
    "\n",
    "DASHBOARD_CORPUS = \"\"\"\n",
    "SELECT d.* \n",
    "FROM PREFIX_CORPUS as d\n",
    "\"\"\"\n",
    "\n",
    "DASHBOARD_CORPUS_TO_PAPER = \"\"\"\n",
    "SELECT dp.* \n",
    "FROM PREFIX_CORPUS as d\n",
    "        INNER JOIN PREFIX_CORPUS_TO_PAPER as dp on (d.ID=dp.ID_CORPUS)\n",
    "\"\"\"\n",
    "\n",
    "DASHBOARD_PAPER = \"\"\"\n",
    "SELECT DISTINCT * FROM (\n",
    "    SELECT DISTINCT p.id AS ID, p.DOI, p.TITLE, p.ABSTRACT, p.YEAR, p.MONTH, \n",
    "        p.DAY, p.VOLUME, p.ISSUE, p.PAGINATION, p.SOURCE, p.ISO as ISO_ABBREVIATION,\n",
    "        p.JOURNAL_NAME_RAW as JOURNAL_TITLE, p.EIGENFACTOR, p.TYPE as ARTICLE_TYPE    \n",
    "    FROM PREFIX_CORPUS as d\n",
    "        INNER JOIN PREFIX_CORPUS_TO_PAPER as dp on (d.ID=dp.ID_CORPUS)\n",
    "        INNER JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p on (p.ID=dp.ID_PAPER)\n",
    "  ) as P2 \n",
    "  LEFT JOIN (SELECT PAPER_ID, F1000, FACEBOOK, MENDELEY, NEWS, REDDIT, TWITTER, WIKIPEDIA\n",
    "      FROM PROD_DB.CORE_RAW.ALTMETRIC_IMPORT) V on P2.ID=V.PAPER_ID\n",
    "\"\"\"\n",
    "\n",
    "DASHBOARD_BASIC_PAPER_COLUMNS = ['ID', 'DOI', 'TITLE', 'ABSTRACT', \n",
    "                                 'YEAR', 'MONTH', 'VOLUME', \n",
    "                                 'ISSUE', 'MESH', 'PAGINATION', \n",
    "                                 'JOURNAL_TITLE', 'ARTICLE_TYPE']\n",
    "DASHBOARD_BASIC_PAPER = '''\n",
    "SELECT DISTINCT p.id AS ID, p.DOI, p.TITLE, p.ABSTRACT, p.YEAR, p.MONTH, \n",
    "        p.VOLUME, p.ISSUE, p.MESH_TERMS_RAW as MESH, p.PAGINATION, \n",
    "        p.JOURNAL_NAME_RAW as JOURNAL_TITLE, p.TYPE as ARTICLE_TYPE  \n",
    "FROM PREFIX_CORPUS as d\n",
    "      JOIN PREFIX_CORPUS_TO_PAPER as dp on (d.ID=dp.ID_CORPUS)\n",
    "      JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p on (p.ID=dp.ID_PAPER)\n",
    "'''\n",
    "\n",
    "DASHBOARD_PAPER_TO_AUTHOR = \"\"\"\n",
    "SELECT DISTINCT pa.*\n",
    "FROM PREFIX_CORPUS_TO_PAPER as dp \n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p ON (p.ID=dp.ID_PAPER)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_AUTHOR_V2 as pa ON (p.ID=pa.ID_PAPER)\n",
    "\"\"\"\n",
    "\n",
    "DASHBOARD_AUTHOR = \"\"\"\n",
    "SELECT DISTINCT a.id as ID, a.id_orcid as ID_ORCID, a.NAME as NAME, a.SOURCE as SOURCE, \n",
    "    zeroifnull(sum(alt.MENDELEY)) as MENDELEY, zeroifnull(SUM(alt.TWITTER)) as TWITTER\n",
    "FROM (SELECT DISTINCT ID_PAPER FROM PREFIX_CORPUS_TO_PAPER) as dp \n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p ON (p.ID=dp.ID_PAPER)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_AUTHOR_V2 as pa ON (p.ID=pa.ID_PAPER)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.AUTHOR_V2 as a ON (a.ID=pa.ID_AUTHOR) \n",
    "    LEFT JOIN (SELECT PAPER_ID, F1000, FACEBOOK, MENDELEY, NEWS, REDDIT, TWITTER, WIKIPEDIA\n",
    "      FROM PROD_DB.CORE_RAW.ALTMETRIC_IMPORT) alt on p.ID=alt.PAPER_ID\n",
    "GROUP BY a.ID, a.id_orcid, a.NAME, a.SOURCE\n",
    "\"\"\"\n",
    "\n",
    "DASHBOARD_PAPER_NOTES = \"\"\"\n",
    "SELECT p.id as PMID,\n",
    "  LISTAGG(a.name, ', ') WITHIN GROUP (order by author_index) as AUTHOR_STRING,\n",
    "  COUNT(a.name) as AUTHOR_COUNT \n",
    "FROM (SELECT DISTINCT ID_PAPER FROM PREFIX_CORPUS_TO_PAPER) as dp \n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p ON (p.ID=dp.ID_PAPER)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_AUTHOR_V2 as pa ON (p.ID=pa.ID_PAPER)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.AUTHOR_V2 as a ON (a.ID=pa.ID_AUTHOR) \n",
    "GROUP BY p.id ;\n",
    "\"\"\"\n",
    "BUILD_DASHBOARD_PAPER_NOTES = \"create table PREFIX_PAPER_NOTES as \" + DASHBOARD_PAPER_NOTES\n",
    "\n",
    "DASHBOARD_PAPER_OPEN_ACCESS = \"\"\"\n",
    "SELECT p.id as PMID, uu.license, uu.open_access \n",
    "FROM (SELECT DISTINCT ID_PAPER FROM PREFIX_CORPUS_TO_PAPER) as dp \n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p ON (p.ID=dp.ID_PAPER)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.UNPAYWALL_URLS as uu on (p.DOI=uu.DOI)\n",
    "GROUP BY p.id, uu.license, uu.open_access;\n",
    "\"\"\"\n",
    "BUILD_DASHBOARD_PAPER_OPEN_ACCESS = \"create table PREFIX_PAPER_OPEN_ACCESS as \" + DASHBOARD_PAPER_OPEN_ACCESS\n",
    "\n",
    "\n",
    "DASHBOARD_AFFILIATION = \"\"\"\n",
    "SELECT aff.* \n",
    "FROM PREFIX_CORPUS_TO_PAPER as dp \n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p ON (p.ID=dp.ID_PAPER)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.AFFILIATION aff ON (p.ID = aff.ID_PAPER)\n",
    "\"\"\"\n",
    "\n",
    "DASHBOARD_INSTITUTION = \"\"\"\n",
    "select i.*, r.PLACE_LATITUDE as latitude, r.PLACE_LONGITUDE as longitude \n",
    "from PREFIX_CORPUS_TO_PAPER as dp \n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p ON (p.ID=dp.ID_PAPER)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.AFFILIATION aff ON (p.ID = aff.ID_PAPER)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.INSTITUTION i ON (i.ID = aff.ID_INSTITUTION)\n",
    "    JOIN DEV_DB.SKE.RINGGOLD_LAT_LONG as r ON (i.id_source=r.ringgold_id )\n",
    "\"\"\"\n",
    "\n",
    "DASHBOARD_PAPER_TO_CONCEPT = \"\"\"\n",
    "SELECT pc.* \n",
    "FROM PREFIX_CORPUS_TO_PAPER as dp \n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p ON (p.ID=dp.ID_PAPER)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_CONCEPT pc ON (p.ID=pc.ID_PAPER)\n",
    "\"\"\"\n",
    "\n",
    "DASHBOARD_CONCEPT = \"\"\"\n",
    "SELECT c.* \n",
    "FROM PREFIX_CORPUS_TO_PAPER as dp \n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p ON (p.ID=dp.ID_PAPER)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_CONCEPT pc ON (p.ID=pc.ID_PAPER)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.CONCEPT c ON (c.ID=pc.ID_CONCEPT)\n",
    "\"\"\"\n",
    "\n",
    "DASHBOARD_CONCEPT_TO_SEMANTIC_TYPE = \"\"\"\n",
    "SELECT cst.* \n",
    "FROM PREFIX_CORPUS_TO_PAPER as dp \n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p ON (p.ID=dp.ID_PAPER)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_CONCEPT pc ON (p.ID=pc.ID_PAPER)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.CONCEPT_TO_SEMANTIC_TYPE cst ON (pc.ID_CONCEPT=cst.ID_CONCEPT)\n",
    "\"\"\"\n",
    "\n",
    "DASHBOARD_SEMANTIC_TYPE = \"\"\"\n",
    "SELECT st.* \n",
    "FROM DEV_DB.SKE.SEMANTIC_TYPES_CATEGORIES as st\n",
    "\"\"\"\n",
    "\n",
    "DASHBOARD_COLLABORATIONS = \"\"\"\n",
    "SELECT DISTINCT id_author_a, id_author_b, id_paper as id_paper, author_count as author_count \n",
    "FROM \n",
    "    (select distinct a.id_author as id_author_a, b.id_author_b, a.id_paper, b.author_count \n",
    "    from RARE_CORPUS_TO_PAPER as cp \n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_AUTHOR_V2 a on (cp.ID_PAPER=a.ID_PAPER)\n",
    "    left join \n",
    "        (select pa1.id_author as id_author_b, pa1.id_paper as id_paper, COUNT(pa2.id_author) as author_count \n",
    "        from RARE_CORPUS_TO_PAPER as cp \n",
    "        JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_AUTHOR_V2 pa1 on (cp.ID_PAPER=pa1.ID_PAPER)\n",
    "        JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_AUTHOR_V2 pa2 on (pa1.ID_PAPER=pa2.ID_PAPER)\n",
    "        GROUP BY pa1.id_author, pa1.id_paper) b on a.id_paper=b.id_paper \n",
    "    where id_author_a > id_author_b \n",
    "    order by a.id_paper, id_author_a) \n",
    "\"\"\"\n",
    "BUILD_DASHBOARD_COLLABORATIONS = \"create table PREFIX_COLLABORATIONS as \" + DASHBOARD_COLLABORATIONS\n",
    "\n",
    "DASHBOARD_AUTHOR_LOCATION = '''\n",
    "SELECT DISTINCT\n",
    "    a.id as author_id, a.name as author_name, ordered_locations.REV_ORDER, ordered_locations.institution, \n",
    "    ordered_locations.city, ordered_locations.country, ordered_locations.lat, ordered_locations.long\n",
    "FROM PREFIX_CORPUS_TO_PAPER as cp \n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_AUTHOR_V2 pa on (cp.ID_PAPER=pa.ID_PAPER)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.AUTHOR_V2 AS a on (pa.ID_AUTHOR=a.ID)\n",
    "        JOIN (\n",
    "          SELECT p2a.ID_AUTHOR as ID_AUTHOR, \n",
    "                p.YEAR as YEAR, \n",
    "                rank() over (partition by p2a.ID_AUTHOR order by p.PMID desc, i.name) as REV_ORDER,\n",
    "                i.name as institution, i.city as city, i.country as country, rll.PLACE_LATITUDE as lat, rll.PLACE_LONGITUDE as long\n",
    "          FROM PREFIX_CORPUS_TO_PAPER as cp \n",
    "            JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_AUTHOR_V2 p2a on (cp.ID_PAPER=p2a.ID_PAPER)\n",
    "            JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER AS p ON (p2a.ID_PAPER = p.ID)\n",
    "            JOIN FIVETRAN.KG_RDS_CORE_DB.AFFILIATION AS aff ON (p2a.id_paper=aff.id_paper AND p2a.author_index=aff.index_author)\n",
    "            JOIN FIVETRAN.KG_RDS_CORE_DB.INSTITUTION AS i ON (aff.id_institution = i.id)\n",
    "            JOIN DEV_DB.SKE.RINGGOLD_LAT_LONG AS rll ON (rll.ringgold_id = i.id_source)\n",
    "         ORDER BY id_author, REV_ORDER\n",
    "        ) AS ordered_locations on (ordered_locations.ID_AUTHOR=a.id)\n",
    "WHERE ordered_locations.REV_ORDER=1\n",
    "GROUP BY author_id, author_name, REV_ORDER, year, institution, lat, long, city, country\n",
    "ORDER BY author_name, REV_ORDER;\n",
    "'''\n",
    "BUILD_DASHBOARD_AUTHOR_LOCATION = \"create table PREFIX_AUTHOR_LOCATION as \" + DASHBOARD_AUTHOR_LOCATION\n",
    "\n",
    "DASHBOARD_CITATION_COUNTS = '''\n",
    "        select p.id as ID, cit.DOI_TO_PAPER as DOI, COUNT(cit.ID) as CITATION_COUNT \n",
    "        from PREFIX_CORPUS_TO_PAPER as cp \n",
    "            JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p ON (cp.ID_PAPER=p.ID)\n",
    "            JOIN FIVETRAN.KG_RDS_CORE_DB.CITATION as cit ON (cit.DOI_TO_PAPER=p.DOI) \n",
    "        where DOI != ''\n",
    "        group by p.id, cit.DOI_TO_PAPER;\n",
    "'''\n",
    "BUILD_DASHBOARD_CITATION_COUNTS = 'create table PREFIX_CITATION_COUNTS as ' + DASHBOARD_CITATION_COUNTS\n",
    "\n",
    "ADD_SEMANTIC_TYPES_TO_FILTER_CONCEPTS = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "DASHBOARD_CURATED_DATA = '''\n",
    "        select p.id as ID_PAPER, cit.DOI_TO_PAPER as DOI, COUNT(cit.ID) as CITATION_COUNT \n",
    "        from PREFIX_CORPUS_TO_PAPER as cp \n",
    "            JOIN PREFIX_FIVETRAN.KG_RDS_CORE_DB.PAPER as p ON (cp.ID_PAPER=p.ID)\n",
    "            JOIN FIVETRAN.KG_RDS_CORE_DB.CITATION as cit ON (cit.DOI_TO_PAPER=p.DOI) \n",
    "        where DOI != ''\n",
    "        group by p.id, cit.DOI_TO_PAPER;\n",
    "'''\n",
    "BUILD_DASHBOARD_CITATION_COUNTS = 'create table PREFIX_CITATION_COUNTS as ' + DASHBOARD_CITATION_COUNTS\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# PRIMARILY NOTES TO ALLOW US TO RECORD HOW QUERIES WERE PERFORMED ON THE DASHBOARD\n",
    "\n",
    "DASH_CORPUS_SQL = '''SELECT CORPUS_NAME \n",
    "FROM PREFIX_CORPUS as d\n",
    "'''\n",
    "\n",
    "DASH_PAPER_COUNT_SQL = '''SELECT DISTINCT COUNT(p.id) AS PAPER_COUNT, dp.SUBSET_CODE AS SUBSET, CORPUS_NAME, d.ID\n",
    "FROM PREFIX_CORPUS as d\n",
    "    JOIN PREFIX_CORPUS_TO_PAPER as dp on (d.ID=dp.ID_CORPUS)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p on (p.ID=dp.ID_PAPER)\n",
    "GROUP BY CORPUS_NAME, SUBSET, d.ID \n",
    "'''\n",
    "CREATE_DASH_PAPER_COUNT = 'create table PREFIX_DASH_PAPER_COUNT as select * from \\n(%s)'%(DASH_PAPER_COUNT_SQL)\n",
    "\n",
    "DASH_OA_PAPER_COUNT_SQL = '''SELECT DISTINCT COUNT(DISTINCT p.id) AS OA_PAPER_COUNT, dp.SUBSET_CODE AS SUBSET, CORPUS_NAME, d.ID\n",
    "FROM PREFIX_CORPUS as d\n",
    "    JOIN PREFIX_CORPUS_TO_PAPER as dp on (d.ID=dp.ID_CORPUS)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p on (p.ID=dp.ID_PAPER)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.UNPAYWALL_URLS as uu on (p.DOI=uu.DOI)\n",
    "WHERE uu.open_access is not null\n",
    "GROUP BY CORPUS_NAME, SUBSET, d.ID  \n",
    "'''\n",
    "CREATE_DASH_OA_PAPER_COUNT = 'create table PREFIX_DASH_OA_PAPER_COUNT as select * from \\n(%s)'%(DASH_OA_PAPER_COUNT_SQL)\n",
    "\n",
    "DASH_AUTHOR_COUNT_SQL = '''SELECT count(DISTINCT a.id) AS AUTHOR_COUNT, dp.SUBSET_CODE AS SUBSET, CORPUS_NAME, d.ID\n",
    "FROM PREFIX_CORPUS as d\n",
    "    INNER JOIN PREFIX_CORPUS_TO_PAPER as dp on (d.ID=dp.ID_CORPUS)  \n",
    "    INNER JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p on (p.ID=dp.ID_PAPER)\n",
    "    INNER JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_AUTHOR_V2 as pa on (p.ID=pa.ID_PAPER)  \n",
    "    INNER JOIN FIVETRAN.KG_RDS_CORE_DB.AUTHOR_V2 as a on (a.ID=pa.ID_AUTHOR)\n",
    "GROUP BY CORPUS_NAME, SUBSET, d.ID \n",
    "'''\n",
    "CREATE_DASH_AUTHOR_COUNT_SQL = 'create table PREFIX_DASH_AUTHOR_COUNT as select * from \\n(%s)'%(DASH_AUTHOR_COUNT_SQL)\n",
    "\n",
    "DASH_AUTHOR_SQL = '''select distinct a.id, \n",
    "        a.id_orcid, \n",
    "        a.name as author_name, \n",
    "        sum(ZEROIFNULL(cc.citation_count)/(2021-p.year)) as normalized_citation_count,  \n",
    "        zeroifnull(sum(alt.MENDELEY)) as Mendeley, \n",
    "        zeroifnull(SUM(alt.TWITTER)) as Twitter,\n",
    "        loc.institution as Institution,\n",
    "        loc.city as City,\n",
    "        loc.country as Country,\n",
    "        DENSE_RANK() OVER (PARTITION BY CORPUS_NAME ORDER BY normalized_citation_count DESC, a.id, a.id_orcid, a.name, loc.institution DESC) as RANK,\n",
    "        CORPUS_NAME,\n",
    "        dop.SUBSET_CODE as SUBSET\n",
    "    from PREFIX_CORPUS_TO_PAPER as dop \n",
    "        JOIN PREFIX_CORPUS as do on (dop.id_corpus=do.id)\n",
    "        JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_AUTHOR_V2 as pa on (dop.id_paper=pa.id_paper)\n",
    "        JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p on (pa.id_paper=p.id)\n",
    "        JOIN PROD_DB.CORE_RAW.ALTMETRIC_IMPORT AS alt on (p.id=alt.paper_id)\n",
    "        JOIN FIVETRAN.KG_RDS_CORE_DB.AUTHOR_V2 as a on (pa.id_author=a.id)\n",
    "        JOIN PREFIX_CITATION_COUNTS as cc on (dop.ID_PAPER=cc.id)\n",
    "        LEFT JOIN PREFIX_AUTHOR_LOCATION as loc on (a.id=loc.author_id)    \n",
    "    group by a.id, a.id_orcid, a.name, alt.Mendeley, alt.Twitter, loc.Institution, loc.City, loc.Country, CORPUS_NAME, SUBSET\n",
    "    order by Twitter desc\n",
    "'''\n",
    "CREATE_DASH_AUTHOR_SQL = 'create table PREFIX_DASH_AUTHOR as select * from \\n(%s)'%(DASH_AUTHOR_SQL)\n",
    "\n",
    "DASH_PAPERS_SQL = '''    \n",
    "    SELECT DISTINCT p.id as PMID, \n",
    "        ZEROIFNULL(cc.CITATION_COUNT) as N_CITATIONS,\n",
    "        alt.MENDELEY,\n",
    "        alt.TWITTER,\n",
    "        pas.author_string as AUTHORS,\n",
    "        pas.open_access as open_access,\n",
    "        YEAR AS YEAR,\n",
    "        TITLE, \n",
    "        ABSTRACT,\n",
    "        CONCAT(JOURNAL_NAME_RAW, ' ', p.VOLUME ,':',p.PAGINATION) as Journal_Ref,\n",
    "        REPLACE(REGEXP_REPLACE( p.TYPE, 'Research Support,.*?($|\\|)', '\\\\1' ), '|', '; ') AS Type,  \n",
    "        DENSE_RANK() OVER (PARTITION BY CORPUS_NAME ORDER BY N_CITATIONS DESC) AS RANK,\n",
    "        CORPUS_NAME,\n",
    "        dp.SUBSET_CODE as SUBSET\n",
    "    FROM PREFIX_CORPUS as d \n",
    "        JOIN PREFIX_CORPUS_TO_PAPER as dp on (d.ID=dp.ID_CORPUS) \n",
    "        JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p on (p.ID=dp.ID_PAPER)\n",
    "        LEFT JOIN PROD_DB.CORE_RAW.ALTMETRIC_IMPORT AS alt on (p.id=alt.paper_id)\n",
    "        LEFT JOIN PREFIX_CITATION_COUNTS as cc on (p.ID=cc.id)\n",
    "        JOIN PREFIX_PAPER_NOTES as pas on (p.ID=pas.PMID)\n",
    "        JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_AUTHOR_V2 as pa on p.ID=pa.ID_PAPER\n",
    "        JOIN FIVETRAN.KG_RDS_CORE_DB.AUTHOR_V2 as a on (a.ID=pa.ID_AUTHOR)\n",
    "    ORDER BY CORPUS_NAME, SUBSET, N_CITATIONS DESC\n",
    "'''\n",
    "CREATE_DASH_PAPERS_SQL = 'create table PREFIX_DASH_PAPERS as select * from \\n(%s)'%(DASH_PAPERS_SQL)\n",
    "\n",
    "DASH_CONCEPTS_COLUMNS = ['CONCEPT', 'PAPER_COUNT', 'SEMTYPES', 'CORPUS_NAME']\n",
    "DASH_CONCEPTS_SQL = '''SELECT e.NAME AS CONCEPT, \n",
    "    count(distinct c.ID) AS PAPER_COUNT, \n",
    "    f.semtypes AS SEMTYPES,\n",
    "    CORPUS_NAME,\n",
    "    b.SUBSET_CODE as SUBSET\n",
    "FROM PREFIX_CORPUS as a\n",
    "    JOIN PREFIX_CORPUS_TO_PAPER as b on (a.ID = b.ID_CORPUS)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as c on (b.ID_PAPER = c.ID)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_CONCEPT as d on (c.ID = d.ID_PAPER) \n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.CONCEPT as e on (d.ID_CONCEPT = e.ID)  \n",
    "    JOIN (\n",
    "      SELECT concept_id, LISTAGG(semtype, '; ') as semtypes \n",
    "      FROM (\n",
    "        SELECT DISTINCT x.ID_CONCEPT as concept_id, y.NAME as semtype \n",
    "          FROM FIVETRAN.KG_RDS_CORE_DB.CONCEPT_TO_SEMANTIC_TYPE as x\n",
    "            JOIN FIVETRAN.KG_RDS_CORE_DB.SEMANTIC_TYPE as y on (x.ID_SEMANTIC_TYPE = y.ID)\n",
    "      )\n",
    "      GROUP BY concept_id) as f on (e.ID = f.concept_id)\n",
    "GROUP BY e.NAME, f.semtypes, CORPUS_NAME, SUBSET\n",
    "ORDER BY count(distinct c.ID) DESC\n",
    "'''\n",
    "CREATE_DASH_CONCEPTS_SQL = 'create table PREFIX_DASH_CONCEPTS as select * from \\n(%s)'%(DASH_CONCEPTS_SQL)\n",
    "\n",
    "DASH_GEO_MAP_SQL = '''select distinct \n",
    "        institute.NAME as NAME,\n",
    "        institute.CITY as CITY,        \n",
    "        institute.COUNTRY as COUNTRY,\n",
    "        institute.LATITUDE as LATITUDE,\n",
    "        institute.LONGITUDE as LONGITUDE,\n",
    "        sum(institute.C_COUNT) as C_COUNT, \n",
    "        CONCAT('Count of Citations: ', sum(institute.C_COUNT)) as CITATION_COUNT, \n",
    "        LISTAGG(institute.author_list, '; ') as LIST, \n",
    "        CORPUS_NAME,\n",
    "        dop.SUBSET_CODE as SUBSET,\n",
    "        DENSE_RANK() OVER (PARTITION BY CORPUS_NAME ORDER BY \"C_COUNT\" DESC) AS \"Rank\"\n",
    "    from \n",
    "        (SELECT loc.institution as NAME,\n",
    "              loc.city as CITY,        \n",
    "              loc.country as COUNTRY,\n",
    "              loc.lat as LATITUDE,\n",
    "              loc.long as LONGITUDE,\n",
    "              ZEROIFNULL( cc.citation_count) as C_COUNT,\n",
    "              p.id as id_paper,\n",
    "              LISTAGG(DISTINCT a.name, '; ') as author_list\n",
    "          FROM PREFIX_AUTHOR_LOCATION as loc \n",
    "              JOIN FIVETRAN.KG_RDS_CORE_DB.AUTHOR_V2 as a on (a.id=loc.author_id) \n",
    "              JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_AUTHOR_V2 as pa on (pa.id_author=a.id) \n",
    "              JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p on (pa.id_paper=p.id)\n",
    "              JOIN PREFIX_CITATION_COUNTS as cc on (p.id=cc.id)\n",
    "          group by loc.institution, loc.CITY, loc.COUNTRY, LATITUDE, LONGITUDE, p.id, C_COUNT\n",
    "        ) as institute \n",
    "        JOIN PREFIX_CORPUS_TO_PAPER as dop on (dop.id_paper=institute.id_paper)\n",
    "        JOIN PREFIX_CORPUS as do on (dop.id_corpus=do.id)\n",
    "    GROUP BY institute.NAME, institute.CITY, institute.COUNTRY, institute.LATITUDE, institute.LONGITUDE, C_COUNT, CORPUS_NAME, SUBSET\n",
    "    ORDER BY \"Rank\"\n",
    "'''\n",
    "CREATE_DASH_GEO_MAP_SQL = 'create table PREFIX_DASH_GEO_MAP as select * from \\n(%s)'%(DASH_GEO_MAP_SQL)\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "CONTENT_CORPUS_SQL = '''SELECT DISTINCT p.id as PMID, \n",
    "        YEAR AS YEAR,\n",
    "        TITLE, \n",
    "        ABSTRACT, \n",
    "        JOURNAL_NAME_RAW as Journal,\n",
    "        CORPUS_NAME\n",
    "    FROM PREFIX_CORPUS as d \n",
    "        JOIN PREFIX_CORPUS_TO_PAPER as dp on (d.ID=dp.ID_CORPUS) \n",
    "        JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p on (p.ID=dp.ID_PAPER)\n",
    "    WHERE dp.SUBSET_CODE=NULL\n",
    "    ORDER BY N_CITATIONS DESC\n",
    "'''\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "MONDO_SEARCH_TERMS = '''\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_FULL_TEXT_DOCUMENT_TABLE = \"\"\"\n",
    "    CREATE TABLE FULLTEXT_DOCUMENT (ID INT IDENTITY, \n",
    "        ID_FTD TEXT, \n",
    "        PLAIN_TEXT TEXT,\n",
    "        ENCODING TEXT, \n",
    "        PROVENANCE TEXT );\n",
    "\"\"\"\n",
    "\n",
    "CREATE_FULL_TEXT_ANNOTATION_TABLE = \"\"\"\n",
    "    CREATE TABLE FULLTEXT_ANNOTATION (\n",
    "        ID INT IDENTITY, \n",
    "        LOCAL_ID TEXT,\n",
    "        ID_FTD TEXT, \n",
    "        START_SPAN INT, \n",
    "        END_SPAN INT,\n",
    "        TYPE TEXT, \n",
    "        TAG TEXT, \n",
    "        ATTR TEXT,\n",
    "        SANITY_CHECK TEXT);\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DASHBOARD_PAPER_NOTES = \"\"\"\n",
    "SELECT p.id as PMID,\n",
    "  LISTAGG(a.name, ', ') WITHIN GROUP (order by author_index) as AUTHOR_STRING,\n",
    "  COUNT(a.name) as AUTHOR_COUNT \n",
    "FROM (SELECT DISTINCT ID_PAPER FROM PREFIX_CORPUS_TO_PAPER) as dp \n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p ON (p.ID=dp.ID_PAPER)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_AUTHOR_V2 as pa ON (p.ID=pa.ID_PAPER)\n",
    "    JOIN FIVETRAN.KG_RDS_CORE_DB.AUTHOR_V2 as a ON (a.ID=pa.ID_AUTHOR) \n",
    "GROUP BY p.id ;\n",
    "\"\"\"\n",
    "BUILD_DASHBOARD_PAPER_NOTES = \"create table PREFIX_PAPER_NOTES as \" + DASHBOARD_PAPER_NOTES\n",
    "\n",
    "class RstContentDashboard:\n",
    "\n",
    "    def __init__(self, database, schema, loc, prefix='PREFIX_'):\n",
    "        self.database = database\n",
    "        self.schema = schema\n",
    "        self.loc = loc\n",
    "        self.prefix = prefix\n",
    "        \n",
    "        if os.path.exists(loc) is False:\n",
    "            os.mkdir(loc)\n",
    "\n",
    "        log_path = '%s/sf_log.txt' % (loc)\n",
    "        if os.path.exists(log_path) is False:\n",
    "            Path(log_path).touch()\n",
    "\n",
    "        self.temp_annotations_path = '%s/TMP_SO.txt' % (loc)\n",
    "        if os.path.exists(self.temp_annotations_path) is False:\n",
    "            Path(self.temp_annotations_path).touch()\n",
    "\n",
    "        self.temp_documents_path = '%s/TMP_DOC.txt' % (loc)\n",
    "        if os.path.exists(self.temp_documents_path) is False:\n",
    "            Path(self.temp_documents_path).touch()\n",
    "\n",
    "    def upload_wb(self, cs, df2, table_name):\n",
    "        table_name = re.sub('PREFIX_', self.prefix, 'PREFIX_'+table_name)\n",
    "        df = df2.replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)\n",
    "        df.to_csv(self.loc+'/'+table_name+'.tsv', index=False, header=False, sep='\\t')\n",
    "        cs.execute('DROP TABLE IF EXISTS '+table_name+';')\n",
    "        cols = [re.sub(' ','_',c).lower() for c in df.columns if c is not None]\n",
    "        cols = [c+' INT AUTOINCREMENT' if c=='ID' else c+' TEXT' for c in cols]\n",
    "        cs.execute('CREATE TABLE '+table_name+'('+', '.join(cols)+');')\n",
    "        print(self.loc +'/'+table_name+'.tsv')\n",
    "        cs.execute('put file://' + self.loc +'/'+table_name+'.tsv' + ' @%'+table_name+';')\n",
    "        cs.execute(\"copy into \"+table_name+\" from @%\"+table_name+\" FILE_FORMAT=(TYPE=CSV FIELD_DELIMITER=\\'\\\\t\\')\")\n",
    "\n",
    "    def clear_corpus_to_paper_table(self, cs):\n",
    "        table_name = re.sub('PREFIX_', self.prefix, 'PREFIX_CORPUS_TO_PAPER')\n",
    "        cs.execute('DROP TABLE IF EXISTS ' + table_name + ';')\n",
    "        \n",
    "    def build_core_tables_from_pmids(self, cs):\n",
    "        print('PAPER_NOTES')\n",
    "        cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"PAPER_NOTES\")\n",
    "        cs.execute(re.sub('PREFIX_', self.prefix, BUILD_DASHBOARD_PAPER_NOTES))\n",
    "        print('PAPER_OPEN_ACCESS')\n",
    "        cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"PAPER_OPEN_ACCESS\")\n",
    "        cs.execute(re.sub('PREFIX_', self.prefix, BUILD_DASHBOARD_PAPER_OPEN_ACCESS))\n",
    "        print('COLLABORATIONS')\n",
    "        cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"COLLABORATIONS\")\n",
    "        cs.execute(re.sub('PREFIX_', self.prefix, BUILD_DASHBOARD_COLLABORATIONS))\n",
    "        print('ALL KNOWN AUTHOR LOCATIONS')\n",
    "        cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"AUTHOR_LOCATION\")\n",
    "        cs.execute(re.sub('PREFIX_', self.prefix, BUILD_DASHBOARD_AUTHOR_LOCATION))\n",
    "        print('CITATION COUNTS')\n",
    "        cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"CITATION_COUNTS\")\n",
    "        cs.execute(re.sub('PREFIX_', self.prefix, BUILD_DASHBOARD_CITATION_COUNTS))\n",
    "\n",
    "    def run_build_pipeline(self, cs, df, api_key):\n",
    "        cs.execute(\"BEGIN\")\n",
    "        self.upload_wb(cs, df)\n",
    "        self.execute_pubmed_queries(cs, df, api_key)\n",
    "        self.build_core_tables_from_pmids(cs)\n",
    "        cs.execute(\"COMMIT\")\n",
    "              \n",
    "    def drop_database(self, cs):\n",
    "        try:\n",
    "            cs.execute(\"BEGIN\")\n",
    "            cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"AUTHOR_LOCATION\")\n",
    "            cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"CITATION_COUNTS\")\n",
    "            cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"COLLABORATIONS\")\n",
    "            cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"PAPER_NOTES\")\n",
    "            cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"CORPUS\")\n",
    "            cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"CORPUS_TO_PAPER\")\n",
    "            cs.execute(\"COMMIT\")\n",
    "\n",
    "        finally:\n",
    "            cs.close()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "from cryptography.hazmat.primitives.asymmetric import rsa\n",
    "from cryptography.hazmat.primitives.asymmetric import dsa\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "import io\n",
    "\n",
    "def port_from_snowflake_to_databricks(database, schema, table_names, table_sql):\n",
    "  \n",
    "  # Use secret manager to get the login name and password for the Snowflake user\n",
    "  user = dbutils.secrets.get(scope=g_scope, key=\"SNOWFLAKE_SERVICE_USERNAME\")\n",
    "  pem = dbutils.secrets.get(scope=g_scope, key=\"SNOWFLAKE_SERVICE_PRIVATE_KEY\")\n",
    "  pwd = dbutils.secrets.get(scope=g_scope, key=\"SNOWFLAKE_SERVICE_PASSPHRASE\")\n",
    "\n",
    "  string_private_key = f\"{pem.strip()}\"\n",
    "\n",
    "  p_key = serialization.load_pem_private_key(\n",
    "      io.BytesIO(string_private_key.encode()).read(),\n",
    "      password=pwd.strip().encode(),\n",
    "      backend=default_backend())\n",
    "\n",
    "  pkb = p_key.private_bytes(\n",
    "    encoding = serialization.Encoding.PEM,\n",
    "    format = serialization.PrivateFormat.PKCS8,\n",
    "    encryption_algorithm = serialization.NoEncryption()\n",
    "    )\n",
    "\n",
    "  pkb = pkb.decode(\"UTF-8\")\n",
    "  pkb = re.sub(\"-*(BEGIN|END) PRIVATE KEY-*\\n\",\"\",pkb).replace(\"\\n\",\"\")\n",
    "\n",
    "  # snowflake connection options\n",
    "  options = dict(sfUrl=\"https://lr02922.snowflakecomputing.com/\",\n",
    "                 sfUser=user.strip(),\n",
    "                 pem_private_key=pkb,\n",
    "                 sfRole=\"ARST_TEAM\",\n",
    "                 sfDatabase=database,\n",
    "                 sfSchema=schema,\n",
    "                 sfWarehouse=\"DEV_WAREHOUSE\")\n",
    "\n",
    "  table_dict = {}\n",
    "  for name, sql in zip(table_names, table_sql):\n",
    "      table_dict[name] = re.sub('PREFIX_', prefix, sql)     \n",
    "\n",
    "  sqlContext.sql('CREATE DATABASE IF NOT EXISTS ' + prefix )\n",
    "  sqlContext.sql('USE '+prefix )\n",
    "  for t in table_names:\n",
    "    sqlContext.sql('DROP TABLE IF EXISTS ' + t)\n",
    "  for t in table_names:\n",
    "    sdf = spark.read \\\n",
    "        .format(\"snowflake\") \\\n",
    "        .options(**options) \\\n",
    "        .option(\"query\", table_dict[t]) \\\n",
    "        .load()\n",
    "    print(t)\n",
    "    print(table_dict[t])\n",
    "    sdf.write.saveAsTable(t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
