{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "803a8d3b-ce1e-4ddd-90d2-ff5b56954d4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# default_exp drsm\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6303966a-7080-43c4-89a7-fac16ea362ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Disease Research State Model  \n",
    "\n",
    "> Classes and functions to execute functionality for generating and analyzing the state of research into one or many identified diseases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b15dd14d-53d0-4dfe-8faa-95cebf4626e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import activesoup\n",
    "from bs4 import BeautifulSoup,Tag,Comment,NavigableString\n",
    "import datetime\n",
    "from enum import Enum\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from owlready2 import *\n",
    "import pandas as pd\n",
    "from prophet.serialize import model_to_json, model_from_json\n",
    "import re\n",
    "import requests\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "from time import time,sleep\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "from urllib.error import URLError\n",
    "\n",
    "class DRSMCollection():\n",
    "  \"\"\"This class generates and supports analysis the research landscape over a collection of diseases. \n",
    "  \"\"\"\n",
    "  def __init__(self, study_name, corpora_df, name_col='CORPUS_NAME', mondo_col='MONDO_CURI', query_col='QUERY'):\n",
    "    '''\n",
    "    Initializes the DRSM Collection object.\n",
    "    '''\n",
    "    self.study_name = name\n",
    "    self.corpora_df = corpora_df\n",
    "    self.name_col = name_col\n",
    "    self.query_col = query_col\n",
    "    self.mondo_col = mondo_col\n",
    "    \n",
    "  def check_query_phrase(self, phrase):\n",
    "    \"\"\"\n",
    "    Checks whether a single phrase would work on Pubmed or would be expanded (which can lead to unpredictable errors). \n",
    "    Use this as a check for synonyms.   \n",
    "    \"\"\"\n",
    "    idPrefix = ''\n",
    "    phrase = re.sub('\"','',phrase)\n",
    "    m1 = re.match('^[a-zA-Z0-9]{1,5}$', phrase)\n",
    "    if m1 is not None:\n",
    "      return False, phrase + ': Abbreviation', 0\n",
    "\n",
    "    m2 = re.search('[(\\)]', phrase)\n",
    "    if m2 is not None:\n",
    "      return False, phrase+': Brackets', 0\n",
    "\n",
    "    m3 = re.search('[\\,\\;]', phrase)\n",
    "    if m3 is not None:\n",
    "      phrase = '(\"' + '\" AND \"'.join(re.split('[\\,\\;]', phrase.strip()))+'\")'\n",
    "\n",
    "    if self.api_key is not None: \n",
    "      esearch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+self.api_key+'&db=' + self.db + '&term='\n",
    "    else:\n",
    "      esearch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db='+self.db + '&term='\n",
    "    url =  esearch_stem + quote('\"'+phrase+'\"')\n",
    "\n",
    "    #print(url)\n",
    "    esearch_response = urlopen(url)\n",
    "    esearch_data = esearch_response.read().decode('utf-8')\n",
    "    esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "    count = int(esearch_soup.find('Count').string)\n",
    "    #n_translations = len(esearch_soup.find('TranslationStack').findAll('TermSet'))\n",
    "    phrase_not_found = esearch_soup.find('PhraseNotFound')\n",
    "    quoted_phrase_not_found = esearch_soup.find('QuotedPhraseNotFound')\n",
    "    if phrase_not_found is not None or quoted_phrase_not_found is not None:\n",
    "      return False, '\"'+phrase+'\" not found', 0\n",
    "    if count == 0:\n",
    "      return False, phrase, count\n",
    "    return True, phrase, count        \n",
    "\n",
    "  def build_query_tuples(self, df, check_threshold, name_col, terms_col, sep):\n",
    "    '''\n",
    "    '''\n",
    "    query_tuples = []\n",
    "    phrase_counts = []\n",
    "    for ind in df.index:\n",
    "      search_l = []\n",
    "      terms_to_check = [df[name_col][ind]]\n",
    "      terms_to_check.extend(df[terms_col][ind].split('|'))\n",
    "      for s in terms_to_check: \n",
    "        go_no_go, phrase, count = check_query_phrase(esq, s.strip())\n",
    "        print(go_no_go, phrase, count)\n",
    "        if go_no_go:\n",
    "          search_l.append(phrase)\n",
    "          sleep(0.10)\n",
    "          if count>check_threshold:\n",
    "            phrase_counts.append((phrase, count))\n",
    "      query_tuples.append( (ind, df[name_col][ind], ' OR '.join(search_l)) )\n",
    "    return query_tuples, phrase_counts    \n",
    "\n",
    "\n",
    "class DRSM():\n",
    "  \"\"\"This class provides a model of the state of research into a particular disease ('Disease Research State Model') based on broad subtypes of research article present in the literature. It makes use of functionality within the CZ Landscaping Toolkit to search online sources, classify the data it finds, and run analyses over that data. \n",
    "  \n",
    "  This version is based on some assumptions: (1) data pertaining to a single disease is linked to an entry in a PREFIX_CORPUS table; (2) papers for that disease/corpus are indexed in the PREFIX_CORPUS_PAPERS table; (3) Codes denoting the type of each paper are stored in the PREFIX_DRSM table.  \n",
    "  \n",
    "  Note that the time series computation is also simply the difference between matched curves over the publishing timeframe of the analysis. \n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, dashdb, corpus_id, name, mondo_id, event_lines=[]):\n",
    "    '''\n",
    "    Initializes the DRSM object.\n",
    "    '''\n",
    "    self.name = name\n",
    "    self.dashdb = dashdb\n",
    "    self.corpus_id = corpus_id\n",
    "    self.mondo_id = mondo_id\n",
    "    self.event_lines = event_lines\n",
    "    \n",
    "  def build_trend_dataset(self):\n",
    "    '''\n",
    "    Computes trend data from existing an underlying corpus of papers annotated for study categories \n",
    "    '''\n",
    "    sql = '''SELECT DISTINCT count(DISTINCT p.id) AS paper_count, d.ID, p.YEAR, p.MONTH, drsm.DRSM_LABEL\n",
    "          FROM PREFIX_CORPUS as d\n",
    "              JOIN PREFIX_CORPUS_TO_PAPER as dp on (d.ID=dp.ID_CORPUS)\n",
    "              JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p on (p.ID=dp.ID_PAPER)\n",
    "              JOIN PREFIX_DRSM as drsm on (p.ID=drsm.ID_PAPER)\n",
    "          WHERE d.ID='''+str(self.corpus_id)+'''\n",
    "          GROUP BY drsm.DRSM_LABEL, YEAR, MONTH, d.ID\n",
    "          ORDER BY drsm.DRSM_LABEL, YEAR, MONTH'''\n",
    "    cols = ['paper_count', 'CORPUS_ID', 'YEAR', 'MONTH', 'DRSM_LABEL']\n",
    "    sql = re.sub('PREFIX_', self.dashdb.prefix, sql)\n",
    "    df = self.dashdb.execute_query(sql, cols)\n",
    "    df = df.fillna(1).loc[df.YEAR>0]\n",
    "    df['date'] = [datetime.date(int(row.YEAR), int(row.MONTH), 1).isoformat() for row in df.itertuples()]\n",
    "    df = df.drop(columns=['YEAR', 'MONTH'])\n",
    "    df = df.replace('irrelevant','reviews')\n",
    "    df['date'] = df['date'].astype({'date': 'datetime64[ns]'})\n",
    "    \n",
    "    l = []\n",
    "    for cat in df.DRSM_LABEL.unique():\n",
    "      for d in pd.date_range(min(df['date']), max(df['date']), freq='MS'):\n",
    "        idx = (df.DRSM_LABEL==cat) & (df.date==d)\n",
    "        if any(idx):\n",
    "          l.append((cat, d, df[idx].paper_count.values[0]))\n",
    "        else:\n",
    "          l.append((cat, d, 0))\n",
    "    ts_df = pd.DataFrame(l, columns=['drsm','date','paper_count']) \n",
    "    self.raw_df = ts_df\n",
    "    \n",
    "    ts_piv_df = ts_df.pivot(index='date',columns='drsm', values='paper_count')\n",
    "    for c in ['clinical characteristics or disease pathology', 'therapeutics in the clinic']:\n",
    "      if c not in ts_piv_df.columns:\n",
    "        ts_piv_df[c] = 0      \n",
    "    ts_piv_df['clinical'] = ts_piv_df['clinical characteristics or disease pathology'] + ts_piv_df['therapeutics in the clinic']\n",
    "    #ts_piv_df = ts_piv_df.set_index(pd.DatetimeIndex(ts_piv_df['date']))\n",
    "    self.cols=['clinical', 'disease mechanism', 'patient-based therapeutics']\n",
    "    ts_piv_df = ts_piv_df.drop(columns=[c for c in ts_piv_df.columns if c not in self.cols])\n",
    "    \n",
    "    prophet_models = []\n",
    "    threshold = 0.01\n",
    "    trends = {}\n",
    "    changepoints = {}\n",
    "    for i,c in enumerate(self.cols):\n",
    "      if c not in ts_piv_df.columns:\n",
    "        continue\n",
    "      df1 = ts_piv_df.reset_index().rename(columns={'date':'ds', c:'y'}).drop(columns=[cc for cc in self.cols if cc!=c and cc in ts_piv_df.columns])  \n",
    "      model = Prophet(seasonality_mode='additive', changepoint_range=0.99)\n",
    "      model.fit(df1)\n",
    "      future = model.make_future_dataframe(periods=12 * 3, freq='MS')\n",
    "      forecast = model.predict(future)\n",
    "      if trends.get('ds') is None: \n",
    "        trends['ds'] = forecast['ds']\n",
    "      trends[c] = forecast['trend']\n",
    "      cps = model.changepoints[ # Note - derived from how changepoints are computed in Prophet\n",
    "            np.abs(np.nanmean(model.params['delta'], axis=0)) >= threshold\n",
    "        ] if len(model.changepoints) > 0 else [] \n",
    "      changepoints[c] = [c for c in cps]\n",
    "      prophet_models.append(json.loads(model_to_json(model)))\n",
    "    \n",
    "    self.prophet_models = prophet_models\n",
    "    self.trends_df = pd.DataFrame(trends)\n",
    "    self.changepoints = changepoints\n",
    "\n",
    "  def plot_raw(self, w=10, h=5):\n",
    "    '''\n",
    "    Plots a line graph of raw monthly publication counts within the corpus for each study category \n",
    "    '''\n",
    "    ig, ax = plt.subplots()\n",
    "    plt.rcParams[\"figure.figsize\"] = [w, 5]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    ax = sns.lineplot (x = \"date\", y = \"paper_count\", data = self.raw_df, hue='drsm')\n",
    "    ax.tick_params (rotation = 60)\n",
    "    plt.show()\n",
    "    \n",
    "  def plot_prophet_models(self):\n",
    "    '''\n",
    "    Shows full plots of the Prophet models for each study category\n",
    "    '''\n",
    "    for i,c in enumerate(self.cols):\n",
    "      model = model_from_json(json.dumps(self.prophet_models[i]))\n",
    "      future = model.make_future_dataframe(periods=12 * 3, freq='MS')\n",
    "      forecast = model.predict(future)\n",
    "      fig = model.plot(forecast)\n",
    "      add_changepoints_to_plot(fig.gca(), model, forecast)\n",
    "      plt.title(c)\n",
    "      plt.figure(i)\n",
    "      plt.show()\n",
    "      \n",
    "  def plot_trends(self):\n",
    "    long_df = self.trends_df.melt(id_vars=['ds'], value_vars=self.cols, var_name='drsm', value_name='monthly_publication_count')\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "    g = sns.lineplot(data=long_df, x=\"ds\", y=\"monthly_publication_count\", hue=\"drsm\")\n",
    "    g.set(title=self.name)\n",
    "    if len(self.event_lines) > 0:\n",
    "      for rl in self.event_lines:\n",
    "        g.axvline(rl, color=\"red\")\n",
    "\n",
    "  def compute_history_euclidean_distance(self, that):\n",
    "    if isinstance(that, DRSM) is False:\n",
    "      raise Exception(\"Can only complare DRSM instances, not \"+type(that))\n",
    "    d = []\n",
    "    for i,c in enumerate(self.cols):\n",
    "      s_y1 = self.trends_df[c].to_numpy()\n",
    "      cp1 = self.get_index_of_first_changepoint(c)\n",
    "      l1 = s_y1.shape[0]\n",
    "      s_y2 = that.trends_df[c].to_numpy()\n",
    "      cp2 = that.get_index_of_first_changepoint(c)\n",
    "      l2 = s_y2.shape[0]\n",
    "      if l2 > l1:\n",
    "        s_y1 = numpy.insert(s_y1, 0, [s_y1[0]] * (l2-l1))\n",
    "        cp1 += l2-l1\n",
    "      if l1 > l2:\n",
    "        s_y2 = numpy.insert(s_y2, 0, [s_y2[0]] * (l1-l2))  \n",
    "        cp2 += l1-l2\n",
    "      denominator = s_y1.shape[0] - min(cp1, cp2)\n",
    "      d.append(np.sqrt(sum(s_y1*s_y1 + s_y2*s_y2)) / denominator)  \n",
    "    return d\n",
    "  \n",
    "  def get_index_of_first_changepoint(self, c):\n",
    "    date = self.changepoints[c][0]\n",
    "    index = self.trends_df[self.trends_df.ds == date].index[0]\n",
    "    return index\n",
    "  \n",
    "  def scrape_fda_website_for_drug_approvals(self):\n",
    "    d = activesoup.Driver()\n",
    "    page = d.get(\"https://www.accessdata.fda.gov/scripts/opdlisting/oopd/\")\n",
    "    form1, form2 = page.find_all('form')\n",
    "    r = form2.submit({\"Designation\": '%'+self.name+'%',\n",
    "                    \"Designation_Start_Date\": \"01/01/1983\",\n",
    "                    \"Designation_End_Date\": \"07/25/2022\", \n",
    "                    \"Search_param\": \"DESDATE\",\n",
    "                    \"Output_Format\": \"Short\",\n",
    "                    \"Sort_order\": \"GENERIC_NAME\",\n",
    "                    \"RecordsPerPage\": 25})\n",
    "    bsoup = BeautifulSoup(r._raw_response.text, \"html.parser\")\n",
    "    if bsoup.find('th') is None:\n",
    "      return None, None\n",
    "    df = pd.read_html(str(bsoup.find('th').parent.parent), extract_links=\"all\")[0]\n",
    "    designations_df = pd.read_html(str(bsoup.find('th').parent.parent))[0]\n",
    "    approvals_dflist = []\n",
    "    for row in df.itertuples():\n",
    "      name, url = row[2]\n",
    "      designation, nuttin = row[5]\n",
    "      if 'Approved' in designation:\n",
    "        print(name)\n",
    "        m = re.match('.*\\((\\d+)\\).*', url)\n",
    "        if(m):\n",
    "          id = m.group(1)\n",
    "          print()\n",
    "          d = activesoup.Driver()\n",
    "          r2 = d.get('https://www.accessdata.fda.gov/scripts/opdlisting/oopd/detailedIndex.cfm?cfgridkey='+id)\n",
    "          bsoup2 = BeautifulSoup(r2._raw_response.text, \"html.parser\")\n",
    "          table_list = bsoup2.find_all(\"table\", {\"class\": \"resultstable\"})\n",
    "          for j,t in enumerate(table_list):\n",
    "            if j==0:\n",
    "              continue\n",
    "            df2 = pd.read_html(str(t))[0]\n",
    "            # important data is generic name (r0c2) / trade name (r1c2) / approval date (r2c2) / Approved Labeled Indication (r3c2)\n",
    "            generic_name = df2.iloc[0,2]\n",
    "            trade_name = df2.iloc[1,2]\n",
    "            approval_date = df2.iloc[2,2]\n",
    "            approved_labeled_indication = df2.iloc[3,2]\n",
    "            approvals_dflist.append((generic_name, trade_name, approval_date, approved_labeled_indication))\n",
    "    approvals_df =  pd.DataFrame(approvals_dflist, columns=['generic_name', 'trade_name', 'approval_date', 'approved_labeled_indication'])\n",
    "    return designations_df, approvals_df\n",
    "  \n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "11_disease_research_state_model",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
