{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# default_exp dashdbUtils\n",
        "from nbdev import *"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " # Dashboard Database Utilities\n",
        "\n",
        "> Simple query classes that allows contruction of a SQL database in Snowflake for science literature dashboard applications. Note that this implementation is intended primarily for internal CZI use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ![Schema for Dashboard Database](https://lucid.app/publicSegments/view/0388e058-e5f8-4914-9536-f718edf21d47/image.jpeg)\n",
        "\n",
        "Image source on LucidDraw: [Link](https://lucid.app/lucidchart/29f3e2c7-cd56-46fa-a6ce-0dda18d819e1/edit?viewport_loc=-2670%2C-1547%2C3099%2C1648%2CoxaLRZ4JBiatT&invitationId=inv_64fde248-ce31-40b5-85d5-2f3317b5f876#)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Use this class to run queries from a spreadsheet across various online academic graph systems and generate a database based on the data from those queries. \n",
        "\n",
        "If we had a dataframe `query_df` where one of the columns described a literature query expressed in Boolean Logic:\n",
        "\n",
        "| ID | DISEASE NAME | MONDO_ID | QUERY  | \n",
        "|----|--------------|----------|--------|\n",
        "| 1 | Adult Polyglucosan Body Disease | MONDO:0009897 | adult polyglucosan body disease \\| adult polyglucosan body neuropathy\n",
        "| 2 | Creatine transporter deficiency | MONDO:0010305 |creatine transporter deficiency \\| guanidinoacetate methyltransferase deficiency \\| AGAT deficiency \\| cerebral creatine deficiency syndrome 1 \\| X-linked creatine deficiency syndrome \\| Cerebral Creatine Deficiency Syndromes \\| creatine transporter defect \\| SLC6A8 deficiency \\| X-linked creatine transporter deficiency \\| X-linked creatine deficiency \\| X-linked creatine deficiency syndrome \\| guanidinoacetate methyltransferase deficiency \\| guanidinoacetate N-methyltransferase activity disease \\| GAMT deficiency \\| glycine amidinotransferase activity disease \\| arginine:glycine amidinotransferase deficiency \\| AGAT deficiency \\| GATM deficiency              \n",
        "| 3 | AGAT deficiency | MONDO:0012996 |  \"GATM deficiency\" \\| \"AGAT deficiency\" \\| \"arginine:glycine amidinotransferase deficiency\" \\| \"L-arginine:glycine amidinotransferase deficiency\"\n",
        "| 4 | Guanidinoacetate methyltransferase deficiency | MONDO:0012999 |  \"guanidinoacetate methyltransferase deficiency\" \\| \"GAMT deficiency\"\n",
        "| 5 | CLOVES Syndrome | MONDO:0013038 | \"CLOVES syndrome \\| (congenital lipomatous overgrowth) & (vascular malformation epidermal) & (nevi-spinal) & syndrome \\| (congenital lipomatous overgrowth) & (vascular malformations) & (Epidermal nevi) & ((skeletal\\|spinal) & abnormalities) \\| CLOVE syndrome \\| (congenital lipomatous overgrowth) & (vascular malformation) & (epidermal nevi)\n",
        "\n",
        "\n",
        "It is straightforward to build a database of all corpora listed in the spreadsheet from the search queries expressed in the `QUERY` column:\n",
        "\n",
        "```\n",
        "from czLandscapingTk.airtableUtils import AirtableUtils\n",
        "from czLandscapingTk.dashdbUtils import DashboardDb\n",
        "from czLandscapingTk.generalUtils import dump_data_to_disk\n",
        "import re\n",
        "\n",
        "at_ID_column = 'ID'\n",
        "at_query_column = 'QUERY'\n",
        "\n",
        "# this will be substituted into the tables above instead of 'PREFIX_'\n",
        "prefix = 'MY_AMAZING_DATABASE_' \n",
        "\n",
        "# Databricks secret management\n",
        "secret_scope = 'secret-scope' \n",
        "\n",
        "# Location of data in SNOWFLAKE\n",
        "warehouse = 'DEV_WAREHOUSE'\n",
        "database = 'DEV_DB' \n",
        "schema = 'SKE'\n",
        "\n",
        "# SNOWFLAKE role for permissions\n",
        "role = 'ARST_TEAM'\n",
        "\n",
        "# Location of temp files in Databricks file storage\n",
        "loc = '/dbfs/FileStore/user/gully/'\n",
        "\n",
        "# See https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/\n",
        "pubmed_api_key = 'blahblahblahblah' \n",
        "\n",
        "# SNOWFLAKE Login credentials to be stored in secrets\n",
        "user = dbutils.secrets.get(scope=secret_scope, key=\"SNOWFLAKE_SERVICE_USERNAME\")\n",
        "pem = dbutils.secrets.get(scope=secret_scope, key=\"SNOWFLAKE_SERVICE_PRIVATE_KEY\")\n",
        "pwd = dbutils.secrets.get(scope=secret_scope, key=\"SNOWFLAKE_SERVICE_PASSPHRASE\")\n",
        "\n",
        "# Execution of the query and generation of the dashboard database\n",
        "dashdb = DashboardDb(prefix, user, pem, pwd, warehouse, database, schema, role, loc)\n",
        "corpus_paper_df = dashdb.run_remote_paper_queries(pubmed_api_key, queries_df, at_ID_column, at_query_column, \n",
        "    sf_include=False, pm_include=True, epmc_include=False)\n",
        "```\n",
        "\n",
        "The parameters `sf_include`, `pm_include`, and `empc_include` denote whether the Boolean queries listed will be run on (A) our own internal SNOWFLAKE database; (B) Pubmed; and (C) European PMC. Records for each of these databases are differentiated based on the `CORPUS` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#export\n",
        "\n",
        "from cryptography.hazmat.backends import default_backend\n",
        "from cryptography.hazmat.primitives.asymmetric import rsa\n",
        "from cryptography.hazmat.primitives.asymmetric import dsa\n",
        "from cryptography.hazmat.primitives import serialization\n",
        "import io\n",
        "import os\n",
        "import snowflake.connector\n",
        "import pandas as pd\n",
        "from enum import Enum\n",
        "import re\n",
        "\n",
        "class Snowflake():\n",
        "  '''\n",
        "  Class to provide simple access to Snowflake from within CZI\n",
        "\n",
        "  Attributes (note - store `user`, `pem`, and `pwd` as `dbutils.secret` data ):\n",
        "  * user: Snowflake username \n",
        "  * pem: SSH key \n",
        "  * pwd: Password for SSH key \n",
        "  * warehouse: name of the SNOWFLAKE warehouse\n",
        "  * database: name of the SNOWFLAKE database\n",
        "  * schema: name of the SNOWFLAKE schema\n",
        "  * role: name of the SNOWFLAKE role with correct permissions to execute database editing\n",
        "  '''\n",
        "\n",
        "  def __init__(self, user, pem, pwd, warehouse, database, schema, role):\n",
        "    self.user = user\n",
        "    self.pem = pem\n",
        "    self.pwd = pwd\n",
        "    self.warehouse = warehouse\n",
        "    self.database = database\n",
        "    self.schema = schema\n",
        "    self.role = role\n",
        "\n",
        "    #string_private_key = f\"-----BEGIN ENCRYPTED PRIVATE KEY-----\\n{pem.strip()}\\n-----END ENCRYPTED PRIVATE KEY-----\"\n",
        "    string_private_key = f\"{pem.strip()}\"\n",
        "\n",
        "    p_key = serialization.load_pem_private_key(\n",
        "        io.BytesIO(string_private_key.encode()).read(),\n",
        "        password=pwd.strip().encode(),\n",
        "        backend=default_backend())\n",
        "\n",
        "    pkb = p_key.private_bytes(\n",
        "        encoding=serialization.Encoding.DER,\n",
        "        format=serialization.PrivateFormat.PKCS8,\n",
        "        encryption_algorithm=serialization.NoEncryption())\n",
        "\n",
        "    self.ctx = snowflake.connector.connect(\n",
        "        user=user.strip(),\n",
        "        private_key=pkb,\n",
        "        account='lr02922')\n",
        "\n",
        "    cur = self.ctx.cursor()\n",
        "    cur.execute(\"select current_date;\")\n",
        "    print(cur.fetchone()[0])\n",
        "\n",
        "  def cursor(self):\n",
        "    self.cs = self.ctx.cursor()\n",
        "    return self.cs\n",
        "\n",
        "  def get_cursor(self):\n",
        "    '''\n",
        "    Gets an active cursor for use within the database \n",
        "    '''\n",
        "    cs = self.cursor()\n",
        "    cs.execute('USE ROLE '+self.role)\n",
        "    cs.execute('USE WAREHOUSE ' + self.warehouse)\n",
        "    print('USE SCHEMA '+self.database+'.'+self.schema)\n",
        "    cs.execute('USE SCHEMA '+self.database+'.'+self.schema)\n",
        "\n",
        "    return cs\n",
        "\n",
        "  def execute_query(self, cs, sql, columns):\n",
        "    '''\n",
        "    Executes an SQL query with a list of column names and returns a Pandas DataFrame\n",
        "    '''    \n",
        "    cs.execute(sql)\n",
        "    df = pd.DataFrame(cs.fetchall(), columns=columns)\n",
        "    df = df.replace('\\n', ' ', regex=True)\n",
        "    return df\n",
        "\n",
        "  def run_query_in_spark(self, query):\n",
        "    string_private_key = f\"{self.pem.strip()}\"\n",
        "\n",
        "    p_key = serialization.load_pem_private_key(\n",
        "        io.BytesIO(string_private_key.encode()).read(),\n",
        "        password=self.pwd.strip().encode(),\n",
        "        backend=default_backend())\n",
        "\n",
        "    pkb = p_key.private_bytes(\n",
        "      encoding = serialization.Encoding.PEM,\n",
        "      format = serialization.PrivateFormat.PKCS8,\n",
        "      encryption_algorithm = serialization.NoEncryption()\n",
        "      )\n",
        "\n",
        "    pkb = pkb.decode(\"UTF-8\")\n",
        "    pkb = re.sub(\"-*(BEGIN|END) PRIVATE KEY-*\\n\",\"\",pkb).replace(\"\\n\",\"\")\n",
        "\n",
        "    # snowflake connection options\n",
        "    options = dict(sfUrl=\"https://lr02922.snowflakecomputing.com/\",\n",
        "                   sfUser=self.user.strip(),\n",
        "                   pem_private_key=pkb,\n",
        "                   sfRole=\"ARST_TEAM\",\n",
        "                   sfDatabase=self.database,\n",
        "                   sfSchema=self.schema,\n",
        "                   sfWarehouse=\"DEV_WAREHOUSE\")\n",
        "\n",
        "    sdf = spark.read \\\n",
        "          .format(\"snowflake\") \\\n",
        "          .options(**options) \\\n",
        "          .option(\"query\", query) \\\n",
        "          .load()\n",
        "\n",
        "    return sdf"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "show_doc(Snowflake.get_cursor)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "show_doc(Snowflake.execute_query)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DBTITLE 1,Dashboard Creation Tools\n",
        "#export\n",
        "\n",
        "from pathlib import Path\n",
        "from czLandscapingTk.searchEngineUtils import ESearchQuery, EuroPMCQuery\n",
        "from czLandscapingTk.queryTranslator import QueryTranslator, QueryType\n",
        "import czLandscapingTk.dashdbQueries\n",
        "\n",
        "from datetime import datetime\n",
        "from time import time,sleep\n",
        "\n",
        "import requests\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "class DashboardDb:\n",
        "  \"\"\"This class permits the construction of a database of resources generated from combining a list of queries with a list of subqueries on multiple online repositories.<BR>\n",
        "  Functionality includes:\n",
        "    * Define a spreadsheet with a column of queries expressed in boolean logic\n",
        "    * Optional: Define a secondary spreadsheet with a column of subqueries expressed in boolean logic\n",
        "    * Iterate over different sources (Pubmed + European Pubmed) to execute all combinations of queries and subqueries\n",
        "    * Store extended records for all papers - including full text where available from CZI's internal data repo. \n",
        "    \n",
        "  Attributes (note - store `user`, `pem`, and `pwd` as `dbutils.secret` data ):\n",
        "    * prefix: a string that will be used as the prefix for each table in the database \n",
        "    * user: Snowflake username \n",
        "    * pem: SSH key \n",
        "    * pwd: Password for SSH key \n",
        "    * warehouse: name of the SNOWFLAKE warehouse\n",
        "    * database: name of the SNOWFLAKE database\n",
        "    * schema: name of the SNOWFLAKE schema\n",
        "    * role: name of the SNOWFLAKE role with correct permissions to execute database editing\n",
        "    * loc: local disk location for files \n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, prefix, user, pem, pwd, warehouse, database, schema, role, loc):\n",
        "    self.sf = Snowflake(user, pem, pwd, warehouse, database, schema, role)\n",
        "    self.database = database\n",
        "    self.schema = schema\n",
        "    self.loc = loc\n",
        "    self.prefix = prefix\n",
        "\n",
        "    if os.path.exists(loc) is False:\n",
        "      os.mkdir(loc)\n",
        "\n",
        "    log_path = '%s/sf_log.txt' % (loc)\n",
        "    if os.path.exists(log_path) is False:\n",
        "      Path(log_path).touch()\n",
        "\n",
        "    self.temp_annotations_path = '%s/TMP_SO.txt' % (loc)\n",
        "    if os.path.exists(self.temp_annotations_path) is False:\n",
        "      Path(self.temp_annotations_path).touch()\n",
        "\n",
        "    self.temp_documents_path = '%s/TMP_DOC.txt' % (loc)\n",
        "    if os.path.exists(self.temp_documents_path) is False:\n",
        "      Path(self.temp_documents_path).touch()\n",
        "      \n",
        "  def get_cursor(self):\n",
        "    cs = self.sf.get_cursor()\n",
        "    return cs\n",
        "\n",
        "  def execute_query(self, sql, columns, cs=None):\n",
        "    if cs is None: \n",
        "      cs = self.get_cursor()\n",
        "    cs.execute(sql)\n",
        "    df = pd.DataFrame(cs.fetchall(), columns=columns)\n",
        "    df = df.replace('\\n', ' ', regex=True)\n",
        "    return df\n",
        "\n",
        "  def upload_wb(self, df2, table_name, cs=None):\n",
        "    if cs is None: \n",
        "      cs = self.get_cursor()\n",
        "    table_name = re.sub('PREFIX_', self.prefix, 'PREFIX_'+table_name)\n",
        "    df = df2.replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True)\n",
        "    df.to_csv(self.loc+'/'+table_name+'.tsv', index=False, header=False, sep='\\t')\n",
        "    cs.execute('DROP TABLE IF EXISTS '+table_name+';')\n",
        "    cols = [re.sub(' ','_',c).lower() for c in df.columns if c is not None]\n",
        "    cols = [c+' INT AUTOINCREMENT' if c=='ID' else c+' TEXT' for c in cols]\n",
        "    cs.execute('CREATE TABLE '+table_name+'('+', '.join(cols)+');')\n",
        "    print(self.loc +'/'+table_name+'.tsv')\n",
        "    cs.execute('put file://' + self.loc +'/'+table_name+'.tsv' + ' @%'+table_name+';')\n",
        "    cs.execute(\"copy into \"+table_name+\" from @%\"+table_name+\" FILE_FORMAT=(TYPE=CSV FIELD_DELIMITER=\\'\\\\t\\')\")\n",
        "\n",
        "  def clear_corpus_to_paper_table(self, cs=None):\n",
        "    if cs is None: \n",
        "      cs = self.get_cursor()\n",
        "    table_name = re.sub('PREFIX_', self.prefix, 'PREFIX_CORPUS_TO_PAPER')\n",
        "    cs.execute('DROP TABLE IF EXISTS ' + table_name + ';')\n",
        "\n",
        "\n",
        "  def build_lookup_table(self, cs=None, delete_existing=False):\n",
        "    if cs is None: \n",
        "      cs = self.get_cursor()\n",
        "    COLS = ['PMID', 'FIRST_AUTHOR', 'YEAR', 'VOLUME', 'PAGE']\n",
        "    SQL = '''\n",
        "      SELECT p.PMID as PMID, REGEXP_SUBSTR(a.NAME, '\\\\\\\\S*$') as FIRST_AUTHOR, \n",
        "          p.YEAR as YEAR, p.VOLUME as VOLUME, REGEXP_SUBSTR(p.PAGINATION, '^\\\\\\\\d+') as PAGE\n",
        "      FROM FIVETRAN.KG_RDS_CORE_DB.PAPER as p\n",
        "        JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER_TO_AUTHOR_v2 as pa on (p.ID=pa.ID_PAPER)\n",
        "        JOIN FIVETRAN.KG_RDS_CORE_DB.AUTHOR_v2 as a on (a.ID=pa.ID_AUTHOR)\n",
        "      WHERE pa.AUTHOR_INDEX=0\n",
        "      ORDER BY p.PMID DESC\n",
        "    '''\n",
        "    BUILD_PMID_LOOKUP_SQL = \"create table if not exists PMID_LOOKUP as \" + SQL\n",
        "    if delete_existing:\n",
        "      cs.execute('drop table if exists PMID_LOOKUP')\n",
        "    cs.execute(BUILD_PMID_LOOKUP_SQL)    \n",
        "    \n",
        "  def build_core_tables_from_pmids(self, cs=None):\n",
        "    if cs is None: \n",
        "      cs = self.get_cursor()\n",
        "    print('PAPER_NOTES')\n",
        "    cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"PAPER_NOTES\")\n",
        "    cs.execute(re.sub('PREFIX_', self.prefix, czLandscapingTk.dashdbQueries.BUILD_DASHBOARD_PAPER_NOTES))\n",
        "    print('PAPER_OPEN_ACCESS')\n",
        "    cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"PAPER_OPEN_ACCESS\")\n",
        "    cs.execute(re.sub('PREFIX_', self.prefix, czLandscapingTk.dashdbQueries.BUILD_DASHBOARD_PAPER_OPEN_ACCESS))\n",
        "    print('COLLABORATIONS')\n",
        "    cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"COLLABORATIONS\")\n",
        "    cs.execute(re.sub('PREFIX_', self.prefix, czLandscapingTk.dashdbQueries.BUILD_DASHBOARD_COLLABORATIONS))\n",
        "    print('ALL KNOWN AUTHOR LOCATIONS')\n",
        "    cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"AUTHOR_LOCATION\")\n",
        "    cs.execute(re.sub('PREFIX_', self.prefix, czLandscapingTk.dashdbQueries.BUILD_DASHBOARD_AUTHOR_LOCATION))\n",
        "    print('CITATION COUNTS')\n",
        "    cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"CITATION_COUNTS\")\n",
        "    cs.execute(re.sub('PREFIX_', self.prefix, czLandscapingTk.dashdbQueries.BUILD_DASHBOARD_CITATION_COUNTS))\n",
        "\n",
        "  def drop_database(self, cs=None):\n",
        "    if cs is None: \n",
        "      cs = self.get_cursor()\n",
        "    cs.execute(\"BEGIN\")\n",
        "    cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"AUTHOR_LOCATION\")\n",
        "    cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"CITATION_COUNTS\")\n",
        "    cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"COLLABORATIONS\")\n",
        "    cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"PAPER_NOTES\")\n",
        "    cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"CORPUS\")\n",
        "    cs.execute(\"DROP TABLE IF EXISTS \" + self.prefix + \"CORPUS_TO_PAPER\")\n",
        "    cs.execute(\"COMMIT\")\n",
        "\n",
        "  def run_remote_paper_queries(self, pubmed_api_key, query_df, id_col, q_col, \n",
        "                                  subquery_df=None, subq_col=None, \n",
        "                                  delete_db=True, pm_include=True, \n",
        "                                  epmc_include=True, sf_include=True):\n",
        "    '''\n",
        "    Function to generate a snowflake database of scientific papers based on a list of queries listed in a dataframe \n",
        "    (and possibly faceted by a second set of queries in a second dataframe). This system will (optionally) \n",
        "    execute queries on the REST services of Pubmed and European PMC to build the database.    \n",
        "\n",
        "    Attributes:\n",
        "    * pubmed_api_key: see https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/\n",
        "    * query_df: a Pandas Dataframe of corpora where one column specifies the query\n",
        "    * id_col: ID column used to identify the corpus  \n",
        "    * q_col: column for the query (expressed using '&' for AND and '|' for OR)\n",
        "    * subquery_df: an optional Pandas Dataframe of subqueries\n",
        "    * subq_col: an optional column for the subquery (expressed using '&' for AND and '|' for OR)\n",
        "    * delete_db (default=True): delete the existing database?\n",
        "    * pm_include (default=True): Run corpus construction queries on Pubmed \n",
        "    * epmc_include (default=True): Run corpus construction queries on European PMC\n",
        "    '''\n",
        "    qt = QueryTranslator(query_df, id_col, q_col)\n",
        "    if subquery_df is not None:\n",
        "      qt2 = QueryTranslator(subquery_df, id_col, subq_col)\n",
        "    else:\n",
        "      qt2 = None\n",
        "\n",
        "    corpus_paper_list = []\n",
        "    if pm_include:\n",
        "      pubmed_corpus_paper_list, pubmed_errors = self.execute_pubmed_queries(qt, qt2)\n",
        "      corpus_paper_list.extend(pubmed_corpus_paper_list)\n",
        "\n",
        "    epmc_errors = []        \n",
        "    if epmc_include:\n",
        "      epmc_corpus_paper_list, epmc_errors = self.execute_epmc_queries(qt, qt2)\n",
        "      corpus_paper_list.extend(epmc_corpus_paper_list) \n",
        "      \n",
        "    sf_errors = []\n",
        "    if sf_include: \n",
        "      sf_corpus_paper_list, sf_errors_errors = self.execute_epmc_queries(qt, qt2)\n",
        "      corpus_paper_list.extend(sf_corpus_paper_list) \n",
        "\n",
        "    corpus_paper_df = pd.DataFrame(corpus_paper_list, columns=['ID_PAPER', 'ID_CORPUS', 'SOURCE', 'SUBSET_CODE'])\n",
        "\n",
        "    return corpus_paper_df   \n",
        "    \n",
        "  def execute_pubmed_queries(self, qt, qt2): \n",
        "    corpus_paper_list = []\n",
        "    pubmed_errors = []\n",
        "    (corpus_ids, pubmed_queries) = qt.generate_queries(QueryType.pubmed, skipErrors=False)\n",
        "    if qt2:\n",
        "      (subset_ids, pubmed_subset_queries) = qt2.generate_queries(QueryType.pubmed)\n",
        "    else: \n",
        "      (subset_ids, pubmed_subset_queries) = ([0],[''])\n",
        "    for (i, q) in zip(corpus_ids, pubmed_queries):\n",
        "      for (j, sq) in zip(subset_ids, pubmed_subset_queries):\n",
        "        query = q\n",
        "        print(query)\n",
        "        if query=='nan' or len(query)==0: \n",
        "          pubmed_errors.append((i, j, query))\n",
        "          continue\n",
        "        if len(sq) > 0:\n",
        "          query = '(%s) AND (%s)'%(q,sq) \n",
        "        print(query)\n",
        "        esq = ESearchQuery(pubmed_api_key)\n",
        "        try: \n",
        "          pubmed_pmids = esq.execute_query(query)\n",
        "        except:\n",
        "          pubmed_errors.append((i, j, query))\n",
        "          pummed_pmids = []\n",
        "        print(len(pubmed_pmids))\n",
        "        for pmid in pubmed_pmids:\n",
        "          corpus_paper_list.append((pmid, i, 'pubmed', j))\n",
        "    return corpus_paper_list, pubmed_errors\n",
        "  \n",
        "  def execute_epmc_queries(self, qt, qt2):\n",
        "    corpus_paper_list = []\n",
        "    epmc_errors = []\n",
        "    (corpus_ids, epmc_queries) = qt.generate_queries(QueryType.closed)\n",
        "    if qt2:\n",
        "      (subset_ids, epmc_subset_queries) = qt2.generate_queries(QueryType.closed)\n",
        "    else: \n",
        "      (subset_ids, epmc_subset_queries) = ([0],[''])\n",
        "    for (i, q) in zip(corpus_ids, epmc_queries):\n",
        "      for (j, sq) in zip(subset_ids, epmc_subset_queries):\n",
        "        query = q\n",
        "        if query=='nan' or len(query)==0: \n",
        "          epmc_errors.append((i, j, query))\n",
        "          continue\n",
        "        if len(sq) > 0:\n",
        "          query = '(%s) AND (%s)'%(q, sq) \n",
        "        epmcq = EuroPMCQuery()\n",
        "        try: \n",
        "          numFound, epmc_pmids = epmcq.run_empc_query(query)\n",
        "          for id, doi in tqdm(epmc_pmids):\n",
        "            corpus_paper_list.append((id, i, 'epmc', j, doi))\n",
        "        except:\n",
        "          epmc_errors.append((id, i, j, query))\n",
        "    return corpus_paper_list, epmc_errors\n",
        "  \n",
        "  def execute_pubmed_queries_on_sections(self, qt, qt2, api_key='', sections=['tiab']):\n",
        "    corpus_paper_list = []\n",
        "    errors = []\n",
        "    (corpus_ids, pubmed_queries) = qt.generate_queries(QueryType.pubmed, sections=sections)\n",
        "    if qt2:\n",
        "      (subset_ids, pubmed_subset_queries) = qt2.generate_queries(QueryType.pubmed, sections=sections)\n",
        "    else: \n",
        "      (subset_ids, pubmed_subset_queries) = ([0],[''])\n",
        "    for (i, q) in zip(corpus_ids, pubmed_queries):\n",
        "      #if i != 851:\n",
        "      #  continue\n",
        "      for (j, sq) in zip(subset_ids, pubmed_subset_queries):\n",
        "        query = q\n",
        "        if query=='nan' or len(query)==0: \n",
        "          errors.append((i, j, query))\n",
        "          continue\n",
        "        if len(sq) > 0:\n",
        "          query = '(%s) AND (%s)'%(q, sq) \n",
        "        pmq = ESearchQuery(api_key=api_key)\n",
        "        num_found = pmq.execute_count_query(query)\n",
        "        print(num_found)\n",
        "        if num_found>0:\n",
        "          pmids = pmq.execute_query(query)\n",
        "          sleep(0.5) # Sleep for half a second\n",
        "          for id in tqdm(pmids):\n",
        "            corpus_paper_list.append((id, i, 'pubmed', j))\n",
        "    return corpus_paper_list\n",
        "\n",
        "  def execute_epmc_queries_on_sections(self, qt, qt2, sections=['paper_title', 'ABSTRACT']):\n",
        "    corpus_paper_list = []\n",
        "    epmc_errors = []\n",
        "    (corpus_ids, epmc_queries) = qt.generate_queries(QueryType.epmc, sections=sections)\n",
        "    if qt2:\n",
        "      (subset_ids, epmc_subset_queries) = qt2.generate_queries(QueryType.epmc, sections=sections)\n",
        "    else: \n",
        "      (subset_ids, epmc_subset_queries) = ([0],[''])\n",
        "    for (i, q) in zip(corpus_ids, epmc_queries):\n",
        "      for (j, sq) in zip(subset_ids, epmc_subset_queries):\n",
        "        query = q\n",
        "        if query=='nan' or len(query)==0: \n",
        "          continue\n",
        "        if len(sq) > 0:\n",
        "          query = '(%s) AND (%s)'%(q, sq) \n",
        "        epmcq = EuroPMCQuery()\n",
        "        try:\n",
        "          numFound, epmc_pmids = epmcq.run_empc_query(query)\n",
        "          for id, doi in tqdm(epmc_pmids):\n",
        "            corpus_paper_list.append((id, i, 'epmc', j, doi))\n",
        "        except:\n",
        "          epmc_errors.append((id, i, j, query))\n",
        "    return corpus_paper_list, epmc_errors\n",
        "\n",
        "  def execute_sf_queries(self, qt, qt2):\n",
        "    corpus_paper_list = []\n",
        "    sf_errors = []\n",
        "    (corpus_ids, sf_queries) = qt.generate_queries(QueryType.snowflake)\n",
        "    if qt2:\n",
        "      (subset_ids, sf_subset_queries) = qt2.generate_queries(QueryType.snowflake)\n",
        "    else: \n",
        "      (subset_ids, sf_subset_queries) = ([0],[''])\n",
        "    cs = self.get_cursor()\n",
        "    for (i, q) in zip(corpus_ids, sf_queries):\n",
        "      for (j, sq) in zip(subset_ids, sf_subset_queries):\n",
        "        stem = 'SELECT p.ID FROM FIVETRAN.KG_RDS_CORE_DB.PAPER as p WHERE '\n",
        "        query = stem + q\n",
        "        if query=='nan' or len(query)==0: \n",
        "          sf_errors.append((i, j, query))\n",
        "          continue\n",
        "        if len(sq) > 0:\n",
        "          query = stem + '(%s) AND (%s)'%(q, sq) \n",
        "        print(query)\n",
        "        try: \n",
        "          df = self.execute_query(query, ['ID'], cs)\n",
        "          numFound = len(df)\n",
        "          print(i, q, numFound)\n",
        "          sf_ids = df.ID.to_list()\n",
        "          for id in tqdm(sf_ids):\n",
        "            corpus_paper_list.append((id, i, 'czkg', j))\n",
        "        except:\n",
        "          sf_errors.append((i, j, query))\n",
        "    return corpus_paper_list, sf_errors\n",
        "  \n",
        "  \n",
        "  def build_db(self, query_df, corpus_paper_df, subquery_df=None, delete_db=True):\n",
        "    cs = self.get_cursor()\n",
        "    cs.execute(\"BEGIN\")\n",
        "    if delete_db:\n",
        "      self.drop_database(cs=cs)\n",
        "    self.upload_wb(query_df, 'CORPUS', cs=cs)\n",
        "    self.upload_wb(corpus_paper_df, 'CORPUS_TO_PAPER', cs=cs)\n",
        "    if subquery_df is not None:\n",
        "      self.upload_wb(subquery_df, 'SUB_CORPUS', cs=cs)\n",
        "    self.build_core_tables_from_pmids(cs=cs)\n",
        "    cs.execute('COMMIT')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "show_doc(DashboardDb.run_remote_paper_queries)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}