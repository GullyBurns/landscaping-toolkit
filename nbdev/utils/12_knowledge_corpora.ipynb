{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f4babdd-991f-4b4d-9694-0429cf938e7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# default_exp knowlege_corpora\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b17fbe1-5578-4d9b-9010-a5ae2459156e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Knowledge Corpora  \n",
    "# MAGIC\n",
    "> Classes and functions to execute knowledge landscaping analysis over a collection of documents ('expressions') of scientific knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e11a724-50d9-450c-b91f-9c30c705f99c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "import activesoup\n",
    "from bs4 import BeautifulSoup,Tag,Comment,NavigableString\n",
    "import datetime\n",
    "from enum import Enum\n",
    "import json \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from owlready2 import *\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from prophet.plot import add_changepoints_to_plot\n",
    "from prophet.serialize import model_to_json, model_from_json\n",
    "import re\n",
    "import requests\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "from time import time,sleep\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus, quote, unquote\n",
    "from urllib.error import URLError\n",
    "\n",
    "class KnowledgeCorpusCollection():\n",
    "  \"\"\"This class generates and supports analysis the research landscape over a collection of knowledge corpora. \n",
    "  \"\"\"\n",
    "  def __init__(self, study_name, corpora_df, name_col='CORPUS_NAME', mondo_col='MONDO_CURI', query_col='QUERY'):\n",
    "    '''\n",
    "    Initializes the DRSM Collection object.\n",
    "    '''\n",
    "    self.study_name = name\n",
    "    self.corpora_df = corpora_df\n",
    "    self.name_col = name_col\n",
    "    self.query_col = query_col\n",
    "    self.mondo_col = mondo_col\n",
    "    \n",
    "  def check_query_phrase(self, phrase):\n",
    "    \"\"\"\n",
    "    Checks whether a single phrase would work on Pubmed or would be expanded (which can lead to unpredictable errors). \n",
    "    Use this as a check for synonyms.   \n",
    "    \"\"\"\n",
    "    idPrefix = ''\n",
    "    phrase = re.sub('\"','',phrase)\n",
    "    m1 = re.match('^[a-zA-Z0-9]{1,5}$', phrase)\n",
    "    if m1 is not None:\n",
    "      return False, phrase + ': Abbreviation', 0\n",
    "\n",
    "    m2 = re.search('[(\\)]', phrase)\n",
    "    if m2 is not None:\n",
    "      return False, phrase+': Brackets', 0\n",
    "\n",
    "    m3 = re.search('[\\,\\;]', phrase)\n",
    "    if m3 is not None:\n",
    "      phrase = '(\"' + '\" AND \"'.join(re.split('[\\,\\;]', phrase.strip()))+'\")'\n",
    "\n",
    "    if self.api_key is not None: \n",
    "      esearch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?api_key='+self.api_key+'&db=' + self.db + '&term='\n",
    "    else:\n",
    "      esearch_stem = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db='+self.db + '&term='\n",
    "    url =  esearch_stem + quote('\"'+phrase+'\"')\n",
    "\n",
    "    #print(url)\n",
    "    esearch_response = urlopen(url)\n",
    "    esearch_data = esearch_response.read().decode('utf-8')\n",
    "    esearch_soup = BeautifulSoup(esearch_data, \"lxml-xml\")\n",
    "    count = int(esearch_soup.find('Count').string)\n",
    "    #n_translations = len(esearch_soup.find('TranslationStack').findAll('TermSet'))\n",
    "    phrase_not_found = esearch_soup.find('PhraseNotFound')\n",
    "    quoted_phrase_not_found = esearch_soup.find('QuotedPhraseNotFound')\n",
    "    if phrase_not_found is not None or quoted_phrase_not_found is not None:\n",
    "      return False, '\"'+phrase+'\" not found', 0\n",
    "    if count == 0:\n",
    "      return False, phrase, count\n",
    "    return True, phrase, count        \n",
    "\n",
    "  def build_query_tuples(self, df, check_threshold, name_col, terms_col, sep):\n",
    "    '''\n",
    "    '''\n",
    "    query_tuples = []\n",
    "    phrase_counts = []\n",
    "    for ind in df.index:\n",
    "      search_l = []\n",
    "      terms_to_check = [df[name_col][ind]]\n",
    "      terms_to_check.extend(df[terms_col][ind].split('|'))\n",
    "      for s in terms_to_check: \n",
    "        go_no_go, phrase, count = check_query_phrase(esq, s.strip())\n",
    "        print(go_no_go, phrase, count)\n",
    "        if go_no_go:\n",
    "          search_l.append(phrase)\n",
    "          sleep(0.10)\n",
    "          if count>check_threshold:\n",
    "            phrase_counts.append((phrase, count))\n",
    "      query_tuples.append( (ind, df[name_col][ind], ' OR '.join(search_l)) )\n",
    "    return query_tuples, phrase_counts    \n",
    "\n",
    "\n",
    "class KnowledgeCorpus():\n",
    "  \"\"\"This class provides a model of the state of research over a particular knowledge corpus. It makes use of functionality within the CZ Landscaping Toolkit to search online sources, classify the data it finds, and run analyses over that data. \n",
    "  \n",
    "  This version is based on some assumptions: (1) data pertaining to a single disease is linked to an entry in a PREFIX_CORPUS table; (2) papers for that disease/corpus are indexed in the PREFIX_CORPUS_PAPERS table; (3) Codes denoting the type of each paper are stored in the PREFIX_DRSM table.  \n",
    "  \n",
    "  Note that the time series computation is also simply the difference between matched curves over the publishing timeframe of the analysis. \n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, dashdb, corpus_id, name, mondo_id, event_lines=[]):\n",
    "    '''\n",
    "    Initializes the DRSM object.\n",
    "    '''\n",
    "    self.name = name\n",
    "    self.dashdb = dashdb\n",
    "    self.corpus_id = corpus_id\n",
    "    self.event_lines = event_lines\n",
    "    \n",
    "  def build_trend_dataset(self):\n",
    "    '''\n",
    "    Computes trend data from existing an underlying corpus of papers annotated for study categories \n",
    "    '''\n",
    "    sql = '''SELECT DISTINCT count(DISTINCT p.id) AS paper_count, c.ID, p.YEAR, p.MONTH, c.CORPUS_NAME\n",
    "          FROM PREFIX_CORPUS as c\n",
    "              JOIN PREFIX_CORPUS_TO_PAPER as cp on (c.ID=cp.ID_CORPUS)\n",
    "              JOIN FIVETRAN.KG_RDS_CORE_DB.PAPER as p on (p.ID=cp.ID_PAPER)\n",
    "          WHERE c.ID='''+str(self.corpus_id)+'''\n",
    "          GROUP BY c.ID, c.CORPUS_NAME, YEAR, MONTH\n",
    "          ORDER BY c.ID, YEAR, MONTH'''\n",
    "    cols = ['paper_count', 'CORPUS_ID', 'YEAR', 'MONTH', 'CORPUS_NAME']\n",
    "    sql = re.sub('PREFIX_', self.dashdb.prefix, sql)\n",
    "    df = self.dashdb.execute_query(sql, cols)\n",
    "    df = df.fillna(1).loc[df.YEAR>0]\n",
    "    df['date'] = [datetime.date(int(row.YEAR), int(row.MONTH), 1).isoformat() for row in df.itertuples()]\n",
    "    df = df.drop(columns=['YEAR', 'MONTH'])\n",
    "    df = df.replace('irrelevant','reviews')\n",
    "    df['date'] = df['date'].astype({'date': 'datetime64[ns]'})\n",
    "    \n",
    "    l = []\n",
    "    for cat in df.CORPUS_NAME.unique():\n",
    "      for d in pd.date_range(min(df['date']), max(df['date']), freq='MS'):\n",
    "        idx = (df.CORPUS_NAME==cat) & (df.date==d)\n",
    "        if any(idx):\n",
    "          l.append((cat, d, df[idx].paper_count.values[0]))\n",
    "        else:\n",
    "          l.append((cat, d, 0))\n",
    "    ts_df = pd.DataFrame(l, columns=['corpus','date','paper_count']) \n",
    "    self.raw_df = ts_df\n",
    "    \n",
    "    ts_piv_df = ts_df.pivot(index='date',columns='corpus', values='paper_count')\n",
    "    #for c in ['clinical characteristics or disease pathology', 'therapeutics in the clinic']:\n",
    "    #  if c not in ts_piv_df.columns:\n",
    "    #    ts_piv_df[c] = 0      \n",
    "    #ts_piv_df['clinical'] = ts_piv_df['clinical characteristics or disease pathology'] + ts_piv_df['therapeutics in the clinic']\n",
    "    ##ts_piv_df = ts_piv_df.set_index(pd.DatetimeIndex(ts_piv_df['date']))\n",
    "    #self.cols=['clinical', 'disease mechanism', 'patient-based therapeutics']\n",
    "    #ts_piv_df = ts_piv_df.drop(columns=[c for c in ts_piv_df.columns if c not in self.cols])\n",
    "    \n",
    "    prophet_models = []\n",
    "    threshold = 0.01\n",
    "    trends = {}\n",
    "    changepoints = {}\n",
    "    for i,c in enumerate(self.cols):\n",
    "      if c not in ts_piv_df.columns:\n",
    "        continue\n",
    "      df1 = ts_piv_df.reset_index().rename(columns={'date':'ds', c:'y'}).drop(columns=[cc for cc in self.cols if cc!=c and cc in ts_piv_df.columns])  \n",
    "      model = Prophet(seasonality_mode='additive', changepoint_range=0.99)\n",
    "      model.fit(df1)\n",
    "      future = model.make_future_dataframe(periods=12 * 3, freq='MS')\n",
    "      forecast = model.predict(future)\n",
    "      if trends.get('ds') is None: \n",
    "        trends['ds'] = forecast['ds']\n",
    "      trends[c] = forecast['trend']\n",
    "      cps = model.changepoints[ # Note - derived from how changepoints are computed in Prophet\n",
    "            np.abs(np.nanmean(model.params['delta'], axis=0)) >= threshold\n",
    "        ] if len(model.changepoints) > 0 else [] \n",
    "      changepoints[c] = [c for c in cps]\n",
    "      prophet_models.append(json.loads(model_to_json(model)))\n",
    "    \n",
    "    self.prophet_models = prophet_models\n",
    "    self.trends_df = pd.DataFrame(trends)\n",
    "    self.changepoints = changepoints\n",
    "\n",
    "  def plot_raw(self, w=10, h=5):\n",
    "    '''\n",
    "    Plots a line graph of raw monthly publication counts within the corpus for each study category \n",
    "    '''\n",
    "    ig, ax = plt.subplots()\n",
    "    plt.rcParams[\"figure.figsize\"] = [w, 5]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    ax = sns.lineplot (x = \"date\", y = \"paper_count\", data = self.raw_df, hue='drsm')\n",
    "    ax.tick_params (rotation = 60)\n",
    "    plt.show()\n",
    "    \n",
    "  def plot_prophet_models(self):\n",
    "    '''\n",
    "    Shows full plots of the Prophet models for each study category\n",
    "    '''\n",
    "    for i,c in enumerate(self.cols):\n",
    "      model = model_from_json(json.dumps(self.prophet_models[i]))\n",
    "      future = model.make_future_dataframe(periods=12 * 3, freq='MS')\n",
    "      forecast = model.predict(future)\n",
    "      fig = model.plot(forecast)\n",
    "      add_changepoints_to_plot(fig.gca(), model, forecast)\n",
    "      plt.title(c)\n",
    "      plt.figure(i)\n",
    "      plt.figure(i)\n",
    "      plt.show()\n",
    "      \n",
    "  def plot_trends(self):\n",
    "    long_df = self.trends_df.melt(id_vars=['ds'], value_vars=self.cols, var_name='drsm', value_name='monthly_publication_count')\n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "    g = sns.lineplot(data=long_df, x=\"ds\", y=\"monthly_publication_count\", hue=\"drsm\")\n",
    "    g.set(title=self.name)\n",
    "    if len(self.event_lines) > 0:\n",
    "      for rl in self.event_lines:\n",
    "        g.axvline(rl, color=\"red\")\n",
    "\n",
    "  def compute_history_euclidean_distance(self, that):\n",
    "    if isinstance(that, KnowledgeCorpus) is False:\n",
    "      raise Exception(\"Can only complare DRSM instances, not \"+type(that))\n",
    "    d = []\n",
    "    for i,c in enumerate(self.cols):\n",
    "      s_y1 = self.trends_df[c].to_numpy()\n",
    "      cp1 = self.get_index_of_first_changepoint(c)\n",
    "      l1 = s_y1.shape[0]\n",
    "      s_y2 = that.trends_df[c].to_numpy()\n",
    "      cp2 = that.get_index_of_first_changepoint(c)\n",
    "      l2 = s_y2.shape[0]\n",
    "      if l2 > l1:\n",
    "        s_y1 = numpy.insert(s_y1, 0, [s_y1[0]] * (l2-l1))\n",
    "        cp1 += l2-l1\n",
    "      if l1 > l2:\n",
    "        s_y2 = numpy.insert(s_y2, 0, [s_y2[0]] * (l1-l2))  \n",
    "        cp2 += l1-l2\n",
    "      denominator = s_y1.shape[0] - min(cp1, cp2)\n",
    "      d.append(np.sqrt(sum(s_y1*s_y1 + s_y2*s_y2)) / denominator)  \n",
    "    return d\n",
    "  \n",
    "  def get_index_of_first_changepoint(self, c):\n",
    "    date = self.changepoints[c][0]\n",
    "    index = self.trends_df[self.trends_df.ds == date].index[0]\n",
    "    return index\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "12_knowledge_corpora",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
