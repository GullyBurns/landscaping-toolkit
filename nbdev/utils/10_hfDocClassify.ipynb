{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2267698-48d8-4ba9-abab-2ad730682f8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# default_exp docClassify\n",
    "#from nbdev import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f017d08-530b-4e6c-a858-bea38fd7f087",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# HuggingFace Document Classification Utils \n",
    "\n",
    "> Classes and functions to train and run document classification pipelines using baseline HuggingFace functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a40aa826-9a07-4dc8-b8c4-bb4937e86318",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This library contains a single utility class and several functions to make it easy to run a simple document classifier for scientific papers. \n",
    "\n",
    "An example code run through is as follows: \n",
    "\n",
    "\n",
    "```\n",
    "# DRSM BASIC TRAINING ANALYSIS  \n",
    "import datasets\n",
    "\n",
    "column_names =['ID_PAPER', 'Labeling_State', 'Comments', 'Explanation', 'Correct_Label', 'Agreement', 'TRIMMED_TEXT']\n",
    "text_columns = ['TRIMMED_TEXT']\n",
    "label_column = 'Correct_Label'\n",
    "drsm_categories = ['clinical characteristics or disease pathology',\n",
    "              'therapeutics in the clinic', \n",
    "              'disease mechanism', \n",
    "              'patient-based therapeutics', \n",
    "              'other',\n",
    "              'irrelevant']\n",
    "\n",
    "ds_temp = datasets.load_dataset('csv', delimiter=\"\\t\", data_files='/dbfs/FileStore/user/gully/drsm_curated_data/labeled_data_2022_01_03.tsv')\n",
    "train_test_valid = ds_temp['train'].train_test_split(0.1)\n",
    "test_valid = train_test_valid['test'].train_test_split(0.5)\n",
    "drsm_ds = datasets.DatasetDict({\n",
    "    'train': train_test_valid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0c29a4e-7e3c-42af-a421-cdf255d647e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "from functools import partial \n",
    "from tqdm import tqdm \n",
    "tqdm = partial(tqdm, position=0, leave=True)\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datasets import list_datasets, load_dataset, load_metric\n",
    "from transformers import (AutoTokenizer, \n",
    "                          AutoModelForSequenceClassification, AutoConfig, \n",
    "                          TrainingArguments, Trainer)\n",
    "from sklearn.metrics import (f1_score, precision_score, recall_score, accuracy_score, \n",
    "                             classification_report, confusion_matrix, multilabel_confusion_matrix)\n",
    "\n",
    "import pickle\n",
    "\n",
    "print(f\"Running on transformers v{transformers.__version__} and datasets v{datasets.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dbe9bd3-8a33-444d-bd8a-0ff36a0d855d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class HF_trainer_wrapper():\n",
    "  '''\n",
    "  Class to provide support training and experimenting with simple document classification tools under either a multi-label or multi-class classification paradigm.\n",
    "\n",
    "  Attributes:\n",
    "  * run_name:  \n",
    "  * model_ckpt:  \n",
    "  * output_dir:  \n",
    "  * logging_dir: \n",
    "  * epochs: \n",
    "  * max_length: \n",
    "  * problem_type: \n",
    "  '''\n",
    "  tokenizer = None\n",
    "  name = ''\n",
    "  text_columns = []\n",
    "  ds = None\n",
    "  \n",
    "  def __init__(self, run_name, model_ckpt, output_dir, logging_dir, epochs, max_length=512, problem_type=\"multi_label_classification\"):\n",
    "    self.run_name = run_name\n",
    "    self.model_ckpt = model_ckpt\n",
    "    self.output_dir = output_dir\n",
    "    self.logging_dir = logging_dir\n",
    "    self.epochs = epochs\n",
    "    self.max_length = max_length\n",
    "    self.problem_type = problem_type\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(model_ckpt, problem_type=self.problem_type)\n",
    "    \n",
    "  def prepare_dataset(self, ds, text_columns, label_column, categories, problem_type=\"multi_label_classification\"):\n",
    "    self.ds = ds \n",
    "    self.text_columns = text_columns\n",
    "    self.label_column = label_column\n",
    "    self.categories = categories\n",
    "    \n",
    "    def concat_text_fields(row):\n",
    "      t = ''\n",
    "      for f in self.text_columns:\n",
    "        if row[f] is not None:\n",
    "          t += row[f]\n",
    "          if t[-1:] != '.':\n",
    "            t += '.\\n'\n",
    "          else: \n",
    "            t += '\\n'\n",
    "      return {\"text\": '%s'%(t)}     \n",
    "\n",
    "    def tokenize_and_encode(row):\n",
    "      return self.tokenizer(row[\"text\"], \n",
    "                     padding='max_length', \n",
    "                     truncation=True, \n",
    "                     max_length=self.max_length)\n",
    "    \n",
    "    print('Stripping Null Data from datasets')\n",
    "    if problem_type==\"multi_label_classification\": \n",
    "      self.ds = self.ds.map(lambda x : {\"labels\": [1 if x[self.label_column] is not None and c in x[self.label_column] else 0 for c in self.categories] })\n",
    "    else: \n",
    "      self.ds = self.ds.map(lambda x : {\"labels\": [self.categories.index(x[self.label_column])]})\n",
    "    \n",
    "    # NOTE THIS USES THE CONTEXTUALLY DEFINED 'field_list' variable \n",
    "    # implicitly in the concat_text_fields function above \n",
    "    # (not all that great, but not sure how better to do this)\n",
    "    print('Concatonating text fields')\n",
    "    self.ds = self.ds.map(concat_text_fields)\n",
    "    \n",
    "    cols = self.ds[\"train\"].column_names\n",
    "    cols.remove(\"labels\")\n",
    "    print('Tokenizing and encoding')\n",
    "    self.ds_enc = self.ds.map(tokenize_and_encode, \n",
    "                    batched=True, \n",
    "                    remove_columns=cols)\n",
    "\n",
    "    # cast label IDs to floats\n",
    "    self.ds_enc.set_format(\"torch\")\n",
    "    \n",
    "    if problem_type==\"multi_label_classification\": \n",
    "      print('Converting label ints to floats')\n",
    "      self.ds_enc = (self.ds_enc.map(lambda x : \n",
    "                           {\"float_labels\": x[\"labels\"].to(torch.float)},\n",
    "                           remove_columns=[\"labels\"])\n",
    "                .rename_column(\"float_labels\", \"labels\"))\n",
    "    print('Done')\n",
    "  \n",
    "  def build_model(self, loc=None):\n",
    "    if loc is None:\n",
    "      loc = self.model_ckpt\n",
    "     \n",
    "    if self.problem_type == 'multi_label_classification':\n",
    "      num_labels = len( self.ds_enc['train']['labels'][0] )\n",
    "    else:\n",
    "      num_labels = len(set([i[0] for i in self.ds['train']['labels']]))\n",
    "\n",
    "    if os.path.exists(loc):\n",
    "      self.model = AutoModelForSequenceClassification.from_pretrained(loc, \n",
    "                                                                      num_labels=num_labels, \n",
    "                                                                      ignore_mismatched_sizes=True, \n",
    "                                                                      problem_type=self.problem_type).to('cuda')\n",
    "    else: \n",
    "      self.model = AutoModelForSequenceClassification.from_pretrained(loc, \n",
    "                                                                      num_labels=num_labels, \n",
    "                                                                      problem_type=self.problem_type).to('cuda')\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    self.model = self.model.to(device)\n",
    "\n",
    "  def build_trainer(self, warmup_prop = 0.1, batch_size = 8, gradient_accumulation_steps = 2):\n",
    "    \n",
    "    def compute_metrics(pred):\n",
    "      if self.problem_type==\"multi_label_classification\": \n",
    "        labels = pred.label_ids\n",
    "        preds = torch.sigmoid(torch.FloatTensor(pred.predictions)).round().long().cpu().detach().numpy()\n",
    "        #preds = [pl>0 for pl in pred.predictions] \n",
    "        #preds = pred.predictions.argmax(-1)\n",
    "      else: \n",
    "        preds = np.argmax(pred.predictions, axis=1)\n",
    "        labels = pred.label_ids\n",
    "      \n",
    "      precision = precision_score(labels, preds, average='weighted')\n",
    "      recall = recall_score(labels, preds, average='weighted')\n",
    "      f1 = f1_score(labels, preds, average='weighted')\n",
    "      acc = accuracy_score(labels, preds)\n",
    "      \n",
    "      return {\n",
    "          'accuracy': acc,\n",
    "          'f1': f1,\n",
    "          'precision': precision,\n",
    "          'recall': recall\n",
    "      }\n",
    "    \n",
    "    num_train_optimization_steps = int(len(self.ds['train']) / batch_size / gradient_accumulation_steps) * self.epochs\n",
    "    warmup_steps = int(warmup_prop * num_train_optimization_steps)\n",
    "    \n",
    "    self.args = TrainingArguments(learning_rate=2e-5, \n",
    "                                  output_dir=self.output_dir, \n",
    "                                  num_train_epochs=self.epochs,\n",
    "                                  per_device_train_batch_size=batch_size, \n",
    "                                  gradient_accumulation_steps=2,\n",
    "                                  per_device_eval_batch_size=batch_size, \n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False, \n",
    "                                  warmup_steps=warmup_steps, \n",
    "                                  logging_dir=self.logging_dir,\n",
    "                                  run_name=self.run_name)\n",
    "    \n",
    "    self.trainer = Trainer(model = self.model, \n",
    "                           args = self.args,\n",
    "                           train_dataset = self.ds_enc[\"train\"], \n",
    "                           eval_dataset = self.ds_enc[\"valid\"], \n",
    "                           tokenizer = self.tokenizer, \n",
    "                           compute_metrics = compute_metrics)\n",
    "\n",
    "  def train(self, checkpoint=None):\n",
    "    if checkpoint:\n",
    "      self.trainer.train(checkpoint)\n",
    "    else: \n",
    "      self.trainer.train()\n",
    "    tmp_model_path = self.output_dir+'/final_model/'\n",
    "    self.trainer.save_model(tmp_model_path)\n",
    "\n",
    "  def evaluate(self):\n",
    "    self.trainer.evaluate()\n",
    "   \n",
    "  def test(self):\n",
    "    self.last_prediction = self.trainer.predict(self.ds_enc['test'])\n",
    "    return self.last_prediction\n",
    "  \n",
    "  def print_report(self):\n",
    "    if self.last_prediction is not None:\n",
    "      preds = np.argmax(self.last_prediction.predictions, axis=1)\n",
    "      labels = self.last_prediction.label_ids\n",
    "      print(classification_report(labels, preds, target_names=self.categories)) \n",
    "\n",
    "  def save(self):\n",
    "    with open(output_dir+'/hft.pickle', 'wb') as f:\n",
    "      pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "412cee0f-664b-406e-aa72-776abc33dae1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "import mlflow\n",
    "import pickle\n",
    "\n",
    "def run_HF_trainer_expt(ds, text_columns, label_column, categories, run_name, \n",
    "                                   model_input, model_path, log_path, epochs, \n",
    "                                   batch_size=8,\n",
    "                                   transfer_model=None,\n",
    "                                   problem_type=\"multi_label_classification\",\n",
    "                                   run_training=True,\n",
    "                                   freeze_layers=False):\n",
    "  \n",
    "  hft = HF_trainer_wrapper(run_name, model_input, model_path, log_path, epochs, problem_type=problem_type)\n",
    "  hft.prepare_dataset(ds, text_columns, label_column, categories, problem_type=problem_type)\n",
    "  if transfer_model is None:\n",
    "    hft.build_model()\n",
    "  else:\n",
    "    hft.build_model(loc=transfer_model)\n",
    "  \n",
    "  if freeze_layers:\n",
    "    for param in hft.model.bert.parameters():\n",
    "      param.requires_grad = False\n",
    "  \n",
    "  hft.build_trainer(batch_size=batch_size)\n",
    "  if run_training:\n",
    "    hft.train()\n",
    "  pdat = hft.test()\n",
    "  with open(log_path+'/pdat.pkl', 'wb') as f:\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(pdat, f, pickle.HIGHEST_PROTOCOL)\n",
    "  \n",
    "  mlflow.log_param('model_input', model_input)\n",
    "  mlflow.log_param('model_path', model_path)\n",
    "  mlflow.log_param('log_path', log_path)\n",
    "  if transfer_model: \n",
    "    mlflow.log_param('transfer_model', transfer_model )\n",
    "  mlflow.log_param('epochs', model_input)\n",
    "  mlflow.log_metric('test_accuracy', pdat.metrics['test_accuracy'])\n",
    "  mlflow.log_metric('test_f1', pdat.metrics['test_f1'])\n",
    "  mlflow.log_metric('test_precision', pdat.metrics['test_precision'])\n",
    "  mlflow.log_metric('test_recall', pdat.metrics['test_recall'])\n",
    "  mlflow.end_run()\n",
    "\n",
    "  return hft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd275f07-323e-4bbe-82af-00dd8820e9f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "def get_folds_from_dataframe(df, id_col, category_col, n_splits):\n",
    "  folded_ds = []\n",
    "  X = np.array(df[id_col].to_list())\n",
    "  y = np.array(df[category_col].to_list())\n",
    "  skf = StratifiedKFold(n_splits=n_splits, random_state=42, shuffle=True)\n",
    "  skf.get_n_splits(X, y)\n",
    "  for train_index, test_index in skf.split(X, y):\n",
    "    train_valid_df = df.iloc[train_index]\n",
    "    train_df, valid_df = train_test_split(train_valid_df, train_size=.9)\n",
    "    test_df = df.iloc[test_index]\n",
    "    \n",
    "    #checks\n",
    "    for i in test_index:\n",
    "      if i in train_index:\n",
    "        raise ValueError('TEST DATA FOUND IN TRAINING DATA.')\n",
    "    \n",
    "    train = Dataset.from_pandas(train_df)\n",
    "    valid = Dataset.from_pandas(valid_df)\n",
    "    test = Dataset.from_pandas(test_df)\n",
    "    drsm_ds = datasets.DatasetDict({'train': train, 'test': test, 'valid': valid})\n",
    "    folded_ds.append(drsm_ds)\n",
    "  \n",
    "  return folded_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc2c68d3-1f66-41f2-a39e-5a0ba271d060",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "import mlflow\n",
    "import os\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "def run_HF_trainer_kfold_crossvalidation(folds, text_columns, label_column, categories, run_name, \n",
    "                                                    model_input, model_path, log_path, epochs, \n",
    "                                                    batch_size=8, \n",
    "                                                    problem_type=\"multi_label_classification\",\n",
    "                                                    transfer_model=None, \n",
    "                                                    run_training=True,\n",
    "                                                    freeze_layers=False):\n",
    "  metrics_list = []\n",
    "  for i, fold_ds in enumerate(folds):\n",
    "    \n",
    "    hft = run_HF_trainer_expt(fold_ds, text_columns, label_column, categories, run_name, \n",
    "                                         model_input, model_path+'/fold'+str(i), log_path+'/fold'+str(i), \n",
    "                                         epochs, \n",
    "                                         batch_size=batch_size, \n",
    "                                         problem_type=problem_type, \n",
    "                                         transfer_model=transfer_model, \n",
    "                                         run_training=run_training,\n",
    "                                         freeze_layers=freeze_layers)\n",
    "    \n",
    "    if os.path(log_path).exists() is False:\n",
    "      os.makedirs(log_path)\n",
    "    with open(log_path+'/fold'+str(i)+'/pdat.pkl', 'rb') as f:\n",
    "      pdat = pickle.load(f)\n",
    "    tuple = (pdat.metrics['test_accuracy'], pdat.metrics['test_f1'], pdat.metrics['test_precision'], pdat.metrics['test_recall'])\n",
    "    metrics_list.append(tuple)\n",
    "  \n",
    "  df = pd.DataFrame(metrics_list, columns=['test_accuracy','test_f1','test_precision','test_recall'])\n",
    "\n",
    "  return df\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "10_hfDocClassify",
   "notebookOrigID": 1250693293797103,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
