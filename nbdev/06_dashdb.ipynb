{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building databases of published works  \n",
    "\n",
    "> Pragmatic tools for constructing databases of scientific works based on queries defined with Boolean Logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| default_exp dashDatabricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabulate queries in a spreadsheet and generate a database based on the data from those queries. \n",
    "\n",
    "**Example**:  Define a dataframe with an `id` column and a `query` column (expressing a search query in Boolean Logic):\n",
    "\n",
    "| ID | DISEASE NAME | MONDO_ID | QUERY  | \n",
    "|----|--------------|----------|--------|\n",
    "| 1 | Adult Polyglucosan Body Disease | MONDO:0009897 | adult polyglucosan body disease \\| adult polyglucosan body neuropathy\n",
    "| 2 | AGAT deficiency | MONDO:0012996 |  \"GATM deficiency\" \\| \"AGAT deficiency\" \\| \"arginine:glycine amidinotransferase deficiency\" \\| \"L-arginine:glycine amidinotransferase deficiency\"\n",
    "| 3 | Guanidinoacetate methyltransferase deficiency | MONDO:0012999 |  \"guanidinoacetate methyltransferase deficiency\" \\| \"GAMT deficiency\"\n",
    "| 4 | CLOVES Syndrome | MONDO:0013038 | \"CLOVES syndrome \\| (congenital lipomatous overgrowth) & (vascular malformation epidermal) & (nevi-spinal) & syndrome \\| (congenital lipomatous overgrowth) & (vascular malformations) & (Epidermal nevi) & ((skeletal\\|spinal) & abnormalities) \\| CLOVE syndrome \\| (congenital lipomatous overgrowth) & (vascular malformation) & (epidermal nevi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from pathlib import Path\n",
    "from czLandscapingTk.searchEngineUtils import ESearchQuery, EuroPMCQuery\n",
    "from czLandscapingTk.queryTranslator import QueryTranslator, QueryType\n",
    "import czLandscapingTk.dashdbQueries\n",
    "\n",
    "from datetime import datetime\n",
    "from time import time,sleep\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class DashboardDb:\n",
    "\n",
    "  \"\"\"This class permits the construction of a database of resources generated from combining a list of queries with a list of subqueries on multiple online repositories.\n",
    "\n",
    "  Functionality includes:\n",
    "\n",
    "    * Define a spreadsheet with a column of queries expressed in boolean logic\n",
    "    * Optional: Define a secondary spreadsheet with a column of subqueries expressed in boolean logic\n",
    "    * Iterate over different sources (Pubmed + European Pubmed) to execute all combinations of queries and subqueries\n",
    "    * Store extended records for all papers - including full text where available from CZI's internal data repo. \n",
    "    \n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, catalog, database, loc):\n",
    "    self.catalog = catalog\n",
    "    self.database = database\n",
    "    self.loc = loc\n",
    "\n",
    "    if os.path.exists(loc) is False:\n",
    "      os.mkdir(loc)\n",
    "\n",
    "    log_path = '%s/db_log.txt' % (loc)\n",
    "    if os.path.exists(log_path) is False:\n",
    "      Path(log_path).touch()\n",
    "      \n",
    "  def execute_query(self, sql):\n",
    "    sdf = spark.sql(sql)\n",
    "    return sdf.toPandas()\n",
    "\n",
    "  def execute_pubmed_queries_on_sections(self, qt, qt2, api_key='', sections=['tiab']):\n",
    "    corpus_paper_list = []\n",
    "    errors = []\n",
    "    (corpus_ids, pubmed_queries) = qt.generate_queries(QueryType.pubmed, sections=sections)\n",
    "    if qt2:\n",
    "      (subset_ids, pubmed_subset_queries) = qt2.generate_queries(QueryType.pubmed, sections=sections)\n",
    "    else: \n",
    "      (subset_ids, pubmed_subset_queries) = ([0],[''])\n",
    "    for (i, q) in zip(corpus_ids, pubmed_queries):\n",
    "      #if i != 851:\n",
    "      #  continue\n",
    "      for (j, sq) in zip(subset_ids, pubmed_subset_queries):\n",
    "        query = q\n",
    "        if query=='nan' or len(query)==0: \n",
    "          errors.append((i, j, query))\n",
    "          continue\n",
    "        if len(sq) > 0:\n",
    "          query = '(%s) AND (%s)'%(q, sq) \n",
    "        pmq = ESearchQuery(api_key=api_key)\n",
    "        num_found = pmq.execute_count_query(query)\n",
    "        print(num_found)\n",
    "        if num_found>0:\n",
    "          pmids = pmq.execute_query(query)\n",
    "          sleep(0.5) # Sleep for half a second\n",
    "          for id in tqdm(pmids):\n",
    "            corpus_paper_list.append((id, i, 'pubmed', j))\n",
    "    return corpus_paper_list\n",
    "\n",
    "  def execute_epmc_queries_on_sections(self, qt, qt2, sections=['paper_title', 'ABSTRACT'], extra_columns=[]):\n",
    "    corpus_paper_list = []\n",
    "    epmc_errors = []\n",
    "    (corpus_ids, epmc_queries) = qt.generate_queries(QueryType.epmc, sections=sections)\n",
    "    if qt2:\n",
    "      (subset_ids, epmc_subset_queries) = qt2.generate_queries(QueryType.epmc, sections=sections)\n",
    "    else: \n",
    "      (subset_ids, epmc_subset_queries) = ([0],[''])\n",
    "    for (i, q) in zip(corpus_ids, epmc_queries):\n",
    "      for (j, sq) in zip(subset_ids, epmc_subset_queries):\n",
    "        query = q\n",
    "        if query is None or query=='nan' or len(query)==0: \n",
    "          continue\n",
    "        if len(sq) > 0:\n",
    "          query = '(%s) AND (%s)'%(q, sq) \n",
    "        epmcq = EuroPMCQuery()\n",
    "        #try:\n",
    "        numFound, epmc_pmids = epmcq.run_empc_query(query, extra_columns=extra_columns)\n",
    "        for row in tqdm(epmc_pmids):\n",
    "            tup = [row[0], i, 'epmc', j, row[1]]\n",
    "            if len(row)>2:\n",
    "                tup.extend(row[2:])\n",
    "            corpus_paper_list.append(tup)\n",
    "        #except Exception as e:\n",
    "        #  epmc_errors.append((i, j, query, e))\n",
    "    return corpus_paper_list, epmc_errors\n",
    "\n",
    "  def check_query_terms(self, qt, qt2=None, pubmed_api_key=''):\n",
    "    pmq = ESearchQuery(api_key=pubmed_api_key)\n",
    "    terms = set()\n",
    "    for t in qt.terms2id.keys():\n",
    "        terms.add(t)\n",
    "    if qt2 is not None:\n",
    "        for t2 in qt2.terms2id.keys():\n",
    "            terms.add(t2)\n",
    "    check_table = {} \n",
    "    for t in tqdm(terms):\n",
    "        (is_ok, t2, c) = pmq._check_query_phrase(t)\n",
    "        check_table[t] = (is_ok, c)\n",
    "    return check_table\n",
    "\n",
    "  def run_empc_query(self, q, page_size=1000, timeout=60, extra_columns=[]):\n",
    "    EMPC_API_URL = 'https://www.ebi.ac.uk/europepmc/webservices/rest/search?format=JSON&pageSize='+str(page_size)+'&synonym=TRUE'\n",
    "    if len(extra_columns)>0:\n",
    "        EMPC_API_URL += '&resultType=core'\n",
    "    url = EMPC_API_URL + '&query=' + q\n",
    "    r = requests.get(url, timeout=timeout)\n",
    "    data = json.loads(r.text)\n",
    "    numFound = data['hitCount']\n",
    "    print(url + ', ' + str(numFound) + ' European PMC PAPERS FOUND')\n",
    "    results = []\n",
    "    cursorMark = '*'\n",
    "    for i in tqdm(range(0, numFound, page_size)):\n",
    "        url = EMPC_API_URL + '&cursorMark=' + cursorMark + '&query=' + q\n",
    "        r = requests.get(url, timeout=timeout)\n",
    "        data = json.loads(r.text)\n",
    "        #print(data)\n",
    "        if data.get('nextCursorMark'):\n",
    "            cursorMark = data['nextCursorMark']\n",
    "            for d in data['resultList']['result']:\n",
    "                results.append(d)\n",
    "    results = sorted(list(results), key = lambda x: x['id'])\n",
    "    print(' Returning '+str(len(results)))\n",
    "    return (numFound, results)\n",
    "\n",
    "  def execute_epmc_queries(self, qt, qt2, sections=['paper_title', 'ABSTRACT']):\n",
    "    corpus_paper_list = []\n",
    "    epmc_errors = []\n",
    "    (corpus_ids, epmc_queries) = qt.generate_queries(QueryType.epmc, sections=sections)\n",
    "    if qt2:\n",
    "        (subset_ids, epmc_subset_queries) = qt2.generate_queries(QueryType.epmc, sections=sections)\n",
    "    else: \n",
    "        (subset_ids, epmc_subset_queries) = ([0],[''])\n",
    "    for (i, q) in zip(corpus_ids, epmc_queries):\n",
    "        for (j, sq) in zip(subset_ids, epmc_subset_queries):\n",
    "            query = q\n",
    "            if query is None or query=='nan' or len(query)==0: \n",
    "                continue\n",
    "            if len(sq) > 0:\n",
    "                query = '(%s) AND (%s)'%(q, sq) \n",
    "            #try:\n",
    "            numFound, tuples = run_empc_query(query)\n",
    "            for tup in tqdm(tuples):\n",
    "                corpus_paper_list.append(tup)\n",
    "            #except Exception as e:\n",
    "            #  epmc_errors.append((i, j, query, e))\n",
    "    return corpus_paper_list, epmc_errors\n",
    "    \n",
    "  def airtable_to_corpus_dataframes(self, at_key, at_sheets):\n",
    "    atu = AirtableUtils(at_key)\n",
    "    df1 = pd.DataFrame()\n",
    "    for sn, id_col, query_col, col_map, sections in at_sheets: \n",
    "        cdf = atu.read_airtable(at_file, sn)\n",
    "        cdf = cdf.rename(columns={id_col:'ID', query_col:'QUERY'})\n",
    "        cdf = cdf.rename(columns=col_map)\n",
    "        cdf = cdf.fillna('').rename(\n",
    "            columns={c:re.sub('[\\s\\(\\)]','_', c.upper()) for c in cdf.columns}\n",
    "            )\n",
    "        cdf.QUERY = [re.sub('^http[s]*://', '', r.QUERY) if r.QUERY[:4]=='http' else r.QUERY \n",
    "                     for i,r in cdf.iterrows()] \n",
    "        cdf.QUERY = [re.sub('/$', '', r.QUERY.strip())  \n",
    "                        for i,r in cdf.iterrows()] \n",
    "        df1 = pd.concat([df1, cdf])\n",
    "        qt = QueryTranslator(cdf, 'ID', 'QUERY')\n",
    "        paper_list, errors = self.execute_epmc_queries(qt, None, sections=sections)\n",
    "        l = [tup if re.match('\\d',tup[0]) else (-1, tup[1], tup[2], tup[3], tup[4]) for tup in paper_list]\n",
    "        cols = ['ID_PAPER', 'ID_CORPUS', 'SOURCE', 'SUBSET_CODE', 'DOI']\n",
    "        cols.extend(extra_columns)\n",
    "        temp = pd.DataFrame(paper_list, columns=cols)\n",
    "        df2 = pd.concat([df2, temp.drop(columns=extra_columns).drop_duplicates()])\n",
    "        if len(extra_columns)>0:\n",
    "            df3 = pd.concat([df3, temp.drop(columns=['ID_CORPUS','SOURCE', 'SUBSET_CODE']).drop_duplicates()])\n",
    "\n",
    "    df1.fillna('', inplace=True)\n",
    "    df1 = df1.reset_index(drop=True)\n",
    "    df2.fillna('', inplace=True)\n",
    "    df2 = df2.reset_index(drop=True)\n",
    "    df3.fillna('', inplace=True)\n",
    "    df3 = df3.reset_index(drop=True).drop_duplicates()\n",
    "    \n",
    "    return df1, df2, df3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
