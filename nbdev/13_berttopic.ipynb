{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Topic Utilities\n",
    "\n",
    " > A simple API to the BertTopic library (https://maartengr.github.io/BERTopic/index.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp berttopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gburns/Documents/Coding/czLandscapingTk/.venv/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/gburns/Documents/Coding/czLandscapingTk/.venv/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/gburns/Documents/Coding/czLandscapingTk/.venv/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/gburns/Documents/Coding/czLandscapingTk/.venv/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from enum import Enum\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic import BERTopic\n",
    "import json\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def save_to_file(obj, fpath):\n",
    "  if os.path(fpath).exists():\n",
    "    os.unlink(fpath)\n",
    "  with open(fpath, 'wb') as f:\n",
    "    pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "def load_from_file(fpath):\n",
    "  with open(fpath, 'rb') as f:\n",
    "    obj = pickle.load(f, fix_imports=False, encoding=\"UTF-8\")\n",
    "  return obj\n",
    "\n",
    "class DiscourseType(Enum):\n",
    "    BACKGROUND = 0\n",
    "    OBJECTIVE = 1\n",
    "    METHODS = 2\n",
    "    RESULTS = 2\n",
    "    CONCLUSIONS = 2\n",
    "    \n",
    "class SentenceClusterAnalysis: \n",
    "  \"\"\"\n",
    "  Analysis functions for a corpus made up of sentences, each with high-dimensional embeddings \n",
    "      (expressed in JSON) and assigned discourse types for each sentence.  \n",
    "  \"\"\"\n",
    "  sent_df = None # the pandas dataframe of sentences to be analyzed\n",
    "  embeddings = []# the high-dimensional embeddings associated with the sentences\n",
    "  red_vec = {}\n",
    "  bertopic_model = None\n",
    "  id_to_order = {}\n",
    "  order_to_id = {}\n",
    "    \n",
    "  #def __init__(self, *args, **kwargs): \n",
    "\n",
    "  def load_sent_df(self, sent_df:pd.DataFrame, \n",
    "                   id_paper_col='ID_PAPER', \n",
    "                   sentence_id_col='SENTENCE_ID', \n",
    "                   plain_text_col='text', \n",
    "                   json_embeddings_col='json_embeddings',\n",
    "                   id_col='id'):\n",
    "    self.sent_df = sent_df.rename(columns={id_paper_col: 'ID_PAPER', \n",
    "                                           sentence_id_col: 'SENTENCE_ID', \n",
    "                                           plain_text_col: 'text', \n",
    "                                           json_embeddings_col: 'json_embeddings',\n",
    "                                           id_col: 'id'}).sort_values('id')\n",
    "    self.embeddings = np.array([json.loads(row.json_embeddings) for row in self.sent_df.itertuples()])\n",
    "    self.red_vec = {}    \n",
    "    \n",
    "  def generate_bertopic_model(self, n_dim=5, embedding_model=\"allenai-specter\", \n",
    "                              top_n_words=30, min_cluster_size=15, cluster_metric='euclidean',cluster_selection_method='eom'):\n",
    "\n",
    "    umap_model = umap.UMAP(n_components=n_dim)\n",
    "    hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, metric=cluster_metric, cluster_selection_method=cluster_selection_method)\n",
    "    self.bertopic_model = BERTopic(embedding_model=embedding_model, top_n_words=top_n_words, umap_model=umap_model, hdbscan_model=hdbscan_model)\n",
    "    \n",
    "    docs = self.sent_df.text.to_list()\n",
    "    years = self.sent_df.YEAR.to_list()\n",
    "    topics, probs = self.bertopic_model.fit_transform(docs, self.embeddings)\n",
    "    \n",
    "    self.id_to_order = {}\n",
    "    self.order_to_id = {}\n",
    "    for i in range(len(topics)):\n",
    "      self.id_to_order[self.bertopic_model.hdbscan_model.labels_[i]] = topics[i]\n",
    "      self.order_to_id[topics[i]] = self.bertopic_model.hdbscan_model.labels_[i] \n",
    "      \n",
    "    self.sent_df['cluster_assignments'] = self.bertopic_model.hdbscan_model.labels_\n",
    "    self.sent_df['cluster_probabilities'] = self.bertopic_model.hdbscan_model.probabilities_    \n",
    "    \n",
    "    xy_embed = umap.UMAP(n_components=2).fit_transform(self.embeddings)\n",
    "    self.sent_df['x'] = [xy[0] for xy in xy_embed] \n",
    "    self.sent_df['y'] = [xy[1] for xy in xy_embed] \n",
    "        \n",
    "  def plot_xy(self, red_vec):\n",
    "    x = [row.x for row in self.sent_df.intertuples()]\n",
    "    y = [row.y for row in self.sent_df.intertuples()]\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figheight(20)\n",
    "    fig.set_figwidth(20)\n",
    "    ax.plot(x, y, 'ro',  markersize=0.2)\n",
    "    display(fig)\n",
    "    \n",
    "  def visualize_clusters(self, figsize=(20,20), pointsize=0.01):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    outliers = self.sent_df.loc[self.sent_df.cluster_assignments == -1, :]\n",
    "    clustered = self.sent_df.loc[self.sent_df.cluster_assignments != -1, :]\n",
    "    plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=pointsize)\n",
    "    plt.scatter(clustered.x, clustered.y, c=clustered.cluster_assignments, s=pointsize, cmap='hsv_r')\n",
    "    cent_x = self.sent_df.groupby(['cluster_assignments']).x.mean().reset_index()\n",
    "    cent_y = self.sent_df.groupby(['cluster_assignments']).y.mean().reset_index()\n",
    "    centroids = cent_x.merge(cent_y)\n",
    "    for c in centroids.itertuples():\n",
    "      plt.annotate(c.cluster_assignments if c.cluster_assignments!=-1 else '', # this is the text\n",
    "                 (c.x,c.y), # these are the coordinates to position the label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,-3), # distance from text to points (x,y)\n",
    "                 ha='center') # horizonal alignment\n",
    "    plt.colorbar()\n",
    "    \n",
    "  def generate_berttopic_labels(self):\n",
    "    \n",
    "    def invert_hex(hex_number):\n",
    "      inverse = hex(abs(int(hex_number, 16) - 255))[2:]\n",
    "      # If the number is a single digit add a preceding zero\n",
    "      if len(inverse) == 1:\n",
    "          inverse = '0' + inverse\n",
    "      return inverse\n",
    "\n",
    "    def float_to_greyscale(f):\n",
    "      val = '%x' % int(f * 255)\n",
    "      val = invert_hex(val)\n",
    "      return '#%s%s%s' % (val, val, val)\n",
    "\n",
    "    bertopic_model = self.bertopic_model\n",
    "    ts = bertopic_model.get_topics()\n",
    "    self.html_labels = {}\n",
    "    self.labels = {}\n",
    "    self.label_data = {}\n",
    "    for t in ts:\n",
    "      if t==-1:\n",
    "        \n",
    "        continue\n",
    "      txt = '%d: '%(self.order_to_id[t])\n",
    "      max_weight = ts[t][0][1]\n",
    "      label_data = {'id':int(self.order_to_id[t]), 'max':float(max_weight)}\n",
    "      for i, tup in enumerate(ts[t][:5]):\n",
    "        word = tup[0]\n",
    "        weight = float(tup[1])/max_weight\n",
    "        txt += '<span style=\"color:%s\">%s</span> ' % (float_to_greyscale(weight), word.replace(' ', '&nbsp;'))\n",
    "        label_data[i] = {'word':str(word), 'weight':float(weight)} \n",
    "      self.html_labels[self.order_to_id[t]] = txt\n",
    "      self.labels[self.order_to_id[t]] = '%d: '%(self.order_to_id[t])+'_'.join([tup[0] for tup in ts[t][:5]])\n",
    "      self.label_data[self.order_to_id[t]] = label_data\n",
    "      \n",
    "  def get_cluster_time_series_data(self):\n",
    "    spy_df = pd.pivot_table( self.sent_df.loc[self.sent_df.cluster_assignments>-1], values='id', \n",
    "                          index=['cluster_assignments'], columns=['YEAR'], margins=True, \n",
    "                          aggfunc='nunique', fill_value=0).sort_values('All',ascending=False)\n",
    "    l = []\n",
    "    l2 = []\n",
    "    for r in spy_df.index.values:\n",
    "      #if r==\"All\":\n",
    "      #  continue \n",
    "      m2 = {c:(spy_df.at[r,c]/spy_df.at['All',c]) for c in spy_df.columns if c!='All'}\n",
    "      m = {c:(spy_df.at[r,c]) for c in spy_df.columns if c!='All'}\n",
    "      m['id'] = r\n",
    "      m2['id'] = r\n",
    "      l.append(m)\n",
    "      l2.append(m2)\n",
    "    topics_time_df = pd.DataFrame(l).set_index('id')\n",
    "    topics_time_df2 = pd.DataFrame(l2).set_index('id')\n",
    "    return topics_time_df, topics_time_df2\n",
    "  \n",
    "  def plot_cluster_time_series(self, n=-1, width=5):\n",
    "    if n==-1:\n",
    "      n=len(sent_claims_df)-1\n",
    "    (topics_time_df, topics_time_df2) = self.get_cluster_time_series_data()\n",
    "    topics_time_df = topics_time_df[:n].transpose()\n",
    "    topics_time_df2 = topics_time_df2[:n].transpose()\n",
    "    hgt = math.ceil(n*(2.0/width))\n",
    "    nrows = math.ceil(n/width)\n",
    "    ax = topics_time_df2.plot(subplots=True, layout=(nrows,width), figsize=(5*width,hgt), title=[self.labels[i][:42] for i in topics_time_df.columns[:n]] ) \n",
    "    i = 0\n",
    "    for arow in ax.tolist():\n",
    "      for a in arow:\n",
    "        a2 = a.twinx()\n",
    "        col_name = topics_time_df.columns[i]\n",
    "        a2.set_ylim([0, max(topics_time_df[col_name])*1.1])\n",
    "        a2.plot(topics_time_df[col_name], linestyle=':')\n",
    "        i += 1\n",
    "    return topics_time_df, topics_time_df2\n",
    "\n",
    "  def strip_models(self):\n",
    "    new_self = copy.copy(self)\n",
    "    new_self.bertopic_model = None\n",
    "    new_self.hdbscan_model = None\n",
    "    return new_self\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
